{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from train import *\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 10  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 356318.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = build_vocab(DATA_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4451"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Dataset split: train\n",
      "Unique images: 6000\n",
      "Total size: 30000\n",
      "Dataset split: val\n",
      "Unique images: 1000\n",
      "Total size: 5000\n",
      "__________________________________________________\n",
      "-------------------- Fitting --------------------\n",
      "Epoch: [0][0/469]\tBatch Time 4.229 (4.229)\tData Load Time 1.813 (1.813)\tLoss 9.3459 (9.3459)\tTop-5 Accuracy 0.000 (0.000)\n",
      "Epoch: [0][100/469]\tBatch Time 0.935 (0.986)\tData Load Time 0.000 (0.018)\tLoss 5.7318 (6.3204)\tTop-5 Accuracy 39.776 (33.845)\n",
      "Epoch: [0][200/469]\tBatch Time 0.943 (0.970)\tData Load Time 0.000 (0.009)\tLoss 5.2457 (5.8528)\tTop-5 Accuracy 47.798 (39.861)\n",
      "Epoch: [0][300/469]\tBatch Time 1.038 (0.966)\tData Load Time 0.000 (0.006)\tLoss 5.0713 (5.5781)\tTop-5 Accuracy 51.399 (43.790)\n",
      "Epoch: [0][400/469]\tBatch Time 1.108 (0.975)\tData Load Time 0.000 (0.005)\tLoss 4.8134 (5.3795)\tTop-5 Accuracy 52.443 (46.636)\n",
      "Validation: [0/79]\tBatch Time 4.246 (4.246)\tLoss 5.5030 (5.5030)\tTop-5 Accuracy 50.759 (50.759)\t\n",
      "\n",
      " * LOSS - 5.426, TOP-5 ACCURACY - 51.889, BLEU-4 - 0.005850551358965143\n",
      "\n",
      "Epoch: [1][0/469]\tBatch Time 1.849 (1.849)\tData Load Time 0.842 (0.842)\tLoss 4.6922 (4.6922)\tTop-5 Accuracy 56.013 (56.013)\n",
      "Epoch: [1][100/469]\tBatch Time 0.939 (1.011)\tData Load Time 0.000 (0.008)\tLoss 4.4988 (4.5310)\tTop-5 Accuracy 58.059 (58.034)\n",
      "Epoch: [1][200/469]\tBatch Time 1.040 (1.000)\tData Load Time 0.000 (0.004)\tLoss 4.3933 (4.4894)\tTop-5 Accuracy 58.556 (58.454)\n",
      "Epoch: [1][300/469]\tBatch Time 0.970 (1.003)\tData Load Time 0.000 (0.003)\tLoss 4.3028 (4.4593)\tTop-5 Accuracy 59.937 (58.835)\n",
      "Epoch: [1][400/469]\tBatch Time 0.979 (0.999)\tData Load Time 0.000 (0.002)\tLoss 3.9817 (4.4251)\tTop-5 Accuracy 65.244 (59.355)\n",
      "Validation: [0/79]\tBatch Time 1.890 (1.890)\tLoss 5.0920 (5.0920)\tTop-5 Accuracy 56.402 (56.402)\t\n",
      "\n",
      " * LOSS - 5.247, TOP-5 ACCURACY - 55.467, BLEU-4 - 0.006597182670651274\n",
      "\n",
      "Epoch: [2][0/469]\tBatch Time 1.907 (1.907)\tData Load Time 0.891 (0.891)\tLoss 4.1865 (4.1865)\tTop-5 Accuracy 63.755 (63.755)\n",
      "Epoch: [2][100/469]\tBatch Time 0.972 (0.981)\tData Load Time 0.000 (0.009)\tLoss 4.1294 (4.1351)\tTop-5 Accuracy 62.303 (62.978)\n",
      "Epoch: [2][200/469]\tBatch Time 0.980 (0.978)\tData Load Time 0.000 (0.005)\tLoss 3.9766 (4.1399)\tTop-5 Accuracy 64.823 (62.898)\n",
      "Epoch: [2][300/469]\tBatch Time 0.967 (0.973)\tData Load Time 0.000 (0.003)\tLoss 4.1121 (4.1307)\tTop-5 Accuracy 63.117 (63.024)\n",
      "Epoch: [2][400/469]\tBatch Time 0.978 (0.971)\tData Load Time 0.000 (0.002)\tLoss 4.1279 (4.1240)\tTop-5 Accuracy 62.773 (63.135)\n",
      "Validation: [0/79]\tBatch Time 1.682 (1.682)\tLoss 5.3405 (5.3405)\tTop-5 Accuracy 54.167 (54.167)\t\n",
      "\n",
      " * LOSS - 5.174, TOP-5 ACCURACY - 56.728, BLEU-4 - 0.006940493464956179\n",
      "\n",
      "Epoch: [3][0/469]\tBatch Time 1.922 (1.922)\tData Load Time 0.924 (0.924)\tLoss 4.0285 (4.0285)\tTop-5 Accuracy 63.426 (63.426)\n",
      "Epoch: [3][100/469]\tBatch Time 0.961 (0.980)\tData Load Time 0.000 (0.009)\tLoss 4.0015 (3.9595)\tTop-5 Accuracy 64.013 (65.093)\n",
      "Epoch: [3][200/469]\tBatch Time 0.956 (0.973)\tData Load Time 0.000 (0.005)\tLoss 4.2161 (3.9514)\tTop-5 Accuracy 62.925 (65.287)\n",
      "Epoch: [3][300/469]\tBatch Time 0.960 (0.979)\tData Load Time 0.000 (0.003)\tLoss 4.0004 (3.9381)\tTop-5 Accuracy 64.923 (65.521)\n",
      "Epoch: [3][400/469]\tBatch Time 0.962 (0.978)\tData Load Time 0.000 (0.002)\tLoss 3.8512 (3.9322)\tTop-5 Accuracy 67.381 (65.568)\n",
      "Validation: [0/79]\tBatch Time 1.630 (1.630)\tLoss 5.0068 (5.0068)\tTop-5 Accuracy 59.322 (59.322)\t\n",
      "\n",
      " * LOSS - 5.142, TOP-5 ACCURACY - 58.072, BLEU-4 - 0.007165415321110453\n",
      "\n",
      "Epoch: [4][0/469]\tBatch Time 1.829 (1.829)\tData Load Time 0.837 (0.837)\tLoss 3.7428 (3.7428)\tTop-5 Accuracy 67.915 (67.915)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a8cb27ea12c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/kelwa/DEV/GP/Image-Captioning/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(t_params, checkpoint, m_params)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             epoch=epoch)\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# one epoch of validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kelwa/DEV/GP/Image-Captioning/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_params = {\n",
    "    'data_name': DATA_NAME,\n",
    "    'imgs_path': IMGS_PATH,\n",
    "    'df_path': DATA_JSON_PATH,\n",
    "    'vocab': vocab,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'workers': workers,\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder\n",
    "}\n",
    "\n",
    "m_params = {\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim,\n",
    "    'dropout': dropout\n",
    "}\n",
    "\n",
    "epochs=100\n",
    "fit(t_params=t_params, m_params=m_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
