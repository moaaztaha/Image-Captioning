{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4366e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4034c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from train import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os import path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b5207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2, max_len=100):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_len = max_len\n",
    "        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize_en(text):\n",
    "        return [tok.lower() for tok in araby.tokenize(text)]\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize_en(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    def numericalize(self, tokens, cap_len):\n",
    "        return [self.stoi['<sos>']] + [self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "                                       for token in tokens] + [self.stoi['<eos>']] + [self.stoi['<pad>']] * (self.max_len - cap_len)\n",
    "\n",
    "    def indextostring(self, idx):\n",
    "        sent_tokens = []\n",
    "        for sent in idx:\n",
    "            sent_tokens.append([self.itos[i] for i in sent if i not in {self.stoi['<sos>'], self.stoi['<pad>'], self.stoi['<eos>']}])\n",
    "        return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ce76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 30  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 256\n",
    "workers = 2\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "pretrained_embeddings = False\n",
    "fine_tune_embeddings = False\n",
    "checkpoint = None  # path to checkpoint, None if none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a302e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'flickr8k_ar'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'ar_data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4b1a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [00:00<00:00, 435411.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5788"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq = 30\n",
    "vocab = build_vocab(DATA_JSON_PATH, max_seq=max_seq)\n",
    "vocab_len = len(vocab); vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff6df2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " ['<pad>',\n",
       "  '<sos>',\n",
       "  '<eos>',\n",
       "  '<unk>',\n",
       "  'طفلة',\n",
       "  'صغيرة',\n",
       "  'تتسلق',\n",
       "  'إلى',\n",
       "  'كلب',\n",
       "  'أسود'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.itos.keys())[:10], list(vocab.itos.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fb11e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_params = {\n",
    "    'data_name': DATA_NAME,\n",
    "    'imgs_path': IMGS_PATH,\n",
    "    'df_path': DATA_JSON_PATH,\n",
    "    'vocab': vocab,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'workers': workers,\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder,\n",
    "    'pretrained_embeddings': pretrained_embeddings,\n",
    "}\n",
    "\n",
    "m_params = {\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim,\n",
    "    'dropout': dropout\n",
    "}\n",
    "\n",
    "logger_dic = {\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder,\n",
    "    'pretrained_embeddings': pretrained_embeddings,\n",
    "    'max_seq_length': max_seq,\n",
    "    'vocab_size': vocab_len,\n",
    "    'enocder': 'resnet101',\n",
    "    'dropout': dropout,\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97209564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'flickr8k_ar',\n",
       " 'imgs_path': 'flickr/Images/',\n",
       " 'df_path': 'ar_data.json',\n",
       " 'vocab': <dataset.Vocabulary at 0x7efeedbbe890>,\n",
       " 'epochs': 30,\n",
       " 'batch_size': 256,\n",
       " 'workers': 2,\n",
       " 'decoder_lr': 0.0004,\n",
       " 'encoder_lr': 0.0001,\n",
       " 'fine_tune_encoder': False,\n",
       " 'pretrained_embeddings': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e8ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
