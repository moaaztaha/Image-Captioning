{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Ar pretrained with Arabert preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DCmeibRBTIsx",
        "EMMO_K-2LaYR",
        "WE_gfxdIHTR4"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a823475f6afe4e2f8a6895f7fefcaa0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a92e7eddbbda46298396270c55f39a4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b8187474591446c3b6cd9cce810c474a",
              "IPY_MODEL_d4d348c96ace470e8844221c280c1892"
            ]
          }
        },
        "a92e7eddbbda46298396270c55f39a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8187474591446c3b6cd9cce810c474a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e89f66f2a5b4d03bcf4010a8c93326a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178793939,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178793939,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0674ff2ef24428d9ac7b639db4da7cd"
          }
        },
        "d4d348c96ace470e8844221c280c1892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7b3dafebaf544f8a3b8bac747640c63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 171M/171M [00:15&lt;00:00, 11.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_891eac0414404f48a8b86aca2a5f0b6a"
          }
        },
        "5e89f66f2a5b4d03bcf4010a8c93326a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0674ff2ef24428d9ac7b639db4da7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7b3dafebaf544f8a3b8bac747640c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "891eac0414404f48a8b86aca2a5f0b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AmLihjzYLns",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:08:40.784744Z",
          "iopub.execute_input": "2021-07-02T08:08:40.785208Z",
          "iopub.status.idle": "2021-07-02T08:08:41.472198Z",
          "shell.execute_reply.started": "2021-07-02T08:08:40.785118Z",
          "shell.execute_reply": "2021-07-02T08:08:41.471085Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6b221e-68e1-4b16-c920-5b14ccc59e09"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul 17 13:20:34 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V6nYIdcluzJ",
        "outputId": "6c372981-c91f-4ef6-c22d-ecdecd66ea9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpqIAlbHmLZV",
        "outputId": "aeb9f4cc-76f5-4419-db84-0f59253e2959"
      },
      "source": [
        "# setting up kaggle json\n",
        "!mkdir /root/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# downloading dataset from kaggle\n",
        "!pip install kaggle -q\n",
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions\n",
        "!unzip -q flickr8kimagescaptions.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            " 99% 1.02G/1.04G [00:08<00:00, 155MB/s]\n",
            "100% 1.04G/1.04G [00:08<00:00, 125MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCxFJ5wQYehR",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:08:42.111492Z",
          "iopub.execute_input": "2021-07-02T08:08:42.111882Z",
          "iopub.status.idle": "2021-07-02T08:08:47.713294Z",
          "shell.execute_reply.started": "2021-07-02T08:08:42.111845Z",
          "shell.execute_reply": "2021-07-02T08:08:47.712289Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcea6e72-b0e0-4844-a83c-6cdc46ac5a20"
      },
      "source": [
        "# get the code form github\n",
        "!git clone https://github.com/moaaztaha/Image-Captioning\n",
        "py_files_path = 'Image-Captioning/'\n",
        "import sys\n",
        "sys.path.append(py_files_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning'...\n",
            "remote: Enumerating objects: 679, done.\u001b[K\n",
            "remote: Counting objects: 100% (679/679), done.\u001b[K\n",
            "remote: Compressing objects: 100% (336/336), done.\u001b[K\n",
            "remote: Total 679 (delta 412), reused 597 (delta 330), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (679/679), 43.13 MiB | 24.04 MiB/s, done.\n",
            "Resolving deltas: 100% (412/412), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4366e4d",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:08:47.714804Z",
          "iopub.execute_input": "2021-07-02T08:08:47.715119Z",
          "iopub.status.idle": "2021-07-02T08:08:47.759010Z",
          "shell.execute_reply.started": "2021-07-02T08:08:47.715085Z",
          "shell.execute_reply": "2021-07-02T08:08:47.758232Z"
        },
        "trusted": true
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4034c69e",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:08:47.760609Z",
          "iopub.execute_input": "2021-07-02T08:08:47.760937Z",
          "iopub.status.idle": "2021-07-02T08:08:50.957077Z",
          "shell.execute_reply.started": "2021-07-02T08:08:47.760902Z",
          "shell.execute_reply": "2021-07-02T08:08:50.956238Z"
        },
        "trusted": true
      },
      "source": [
        "import time \n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from models import Encoder, DecoderWithAttention\n",
        "from dataset import *\n",
        "from utils import *\n",
        "from train import *\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from os import path as osp"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ce76f9",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:32.509089Z",
          "iopub.execute_input": "2021-07-02T08:15:32.509464Z",
          "iopub.status.idle": "2021-07-02T08:15:32.575815Z",
          "shell.execute_reply.started": "2021-07-02T08:15:32.509427Z",
          "shell.execute_reply": "2021-07-02T08:15:32.574892Z"
        },
        "trusted": true
      },
      "source": [
        "# Model parameters\n",
        "encoder_dim = 2048 # resnet101\n",
        "emb_dim = 300  # dimension of word embeddings\n",
        "attention_dim = 300  # dimension of attention linear layers\n",
        "decoder_dim = 300  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# training parameters\n",
        "epochs = 30  # number of epochs to train for (if early stopping is not triggered)\n",
        "batch_size = 256\n",
        "workers = 2\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "fine_tune_encoder = False  # fine-tune encoder?\n",
        "pretrained_embeddings = True\n",
        "fine_tune_embeddings = True\n",
        "checkpoint = None  # path to checkpoint, None if none\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7a302e3",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:35.353793Z",
          "iopub.execute_input": "2021-07-02T08:15:35.354151Z",
          "iopub.status.idle": "2021-07-02T08:15:35.418158Z",
          "shell.execute_reply.started": "2021-07-02T08:15:35.354120Z",
          "shell.execute_reply": "2021-07-02T08:15:35.417368Z"
        },
        "trusted": true
      },
      "source": [
        "DATA_NAME = 'flickr8k_ar_arabert_pretrained'\n",
        "\n",
        "# local\n",
        "# DATA_JSON_PATH = 'ar_data.json'\n",
        "# IMGS_PATH = 'flickr/Images/'\n",
        "# kaggle paths\n",
        "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
        "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
        "#colab\n",
        "DATA_JSON_PATH = 'Image-Captioning/ar_data.json'\n",
        "IMGS_PATH = 'flickr8k/images/'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4b1a677",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:35.713846Z",
          "iopub.execute_input": "2021-07-02T08:15:35.714173Z",
          "iopub.status.idle": "2021-07-02T08:15:36.283325Z",
          "shell.execute_reply.started": "2021-07-02T08:15:35.714136Z",
          "shell.execute_reply": "2021-07-02T08:15:36.282393Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132be5f8-9db4-40d5-96e4-f0bd3019fce9"
      },
      "source": [
        "max_seq = 65\n",
        "vocab = build_vocab(DATA_JSON_PATH, max_seq=max_seq)\n",
        "vocab_len = len(vocab); vocab_len"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 24000/24000 [00:00<00:00, 227449.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3309"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6df2d0",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:08:54.943953Z",
          "iopub.execute_input": "2021-07-02T08:08:54.944397Z",
          "iopub.status.idle": "2021-07-02T08:08:54.991849Z",
          "shell.execute_reply.started": "2021-07-02T08:08:54.944344Z",
          "shell.execute_reply": "2021-07-02T08:08:54.990814Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c74e79-608f-4ef2-a83a-d47672a74275"
      },
      "source": [
        "list(vocab.itos.keys())[:10], list(vocab.itos.values())[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
              " ['<pad>', '<sos>', '<eos>', '<unk>', '+', 'ة', 'طفل', 'صغير', 'تتسلق', 'إلى'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCmeibRBTIsx"
      },
      "source": [
        "### Pre-trained Arabic Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:09:06.578331Z",
          "iopub.execute_input": "2021-07-02T08:09:06.578734Z",
          "iopub.status.idle": "2021-07-02T08:09:58.217824Z",
          "shell.execute_reply.started": "2021-07-02T08:09:06.578703Z",
          "shell.execute_reply": "2021-07-02T08:09:58.216636Z"
        },
        "trusted": true,
        "id": "Chu2ZhbITIsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd9f26f-b19b-41fd-f97f-232a18c1d696"
      },
      "source": [
        "# downloading arabic cbow pretrained word embedings\n",
        "! wget https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip\n",
        "! unzip -q full_grams_cbow_300_wiki.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 13:21:33--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1491895880 (1.4G) [application/zip]\n",
            "Saving to: ‘full_grams_cbow_300_wiki.zip’\n",
            "\n",
            "full_grams_cbow_300 100%[===================>]   1.39G   186MB/s    in 26s     \n",
            "\n",
            "2021-07-17 13:22:00 (54.4 MB/s) - ‘full_grams_cbow_300_wiki.zip’ saved [1491895880/1491895880]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:09:58.219669Z",
          "iopub.execute_input": "2021-07-02T08:09:58.220009Z",
          "iopub.status.idle": "2021-07-02T08:10:21.606181Z",
          "shell.execute_reply.started": "2021-07-02T08:09:58.219970Z",
          "shell.execute_reply": "2021-07-02T08:10:21.605224Z"
        },
        "trusted": true,
        "id": "qqgnuOdjTIsy"
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec.load(\"./full_grams_cbow_300_wiki.mdl\")\n",
        "model.wv.save_word2vec_format(\"aravec.txt\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:12:59.760345Z",
          "iopub.execute_input": "2021-07-02T08:12:59.760849Z",
          "iopub.status.idle": "2021-07-02T08:12:59.829858Z",
          "shell.execute_reply.started": "2021-07-02T08:12:59.760810Z",
          "shell.execute_reply": "2021-07-02T08:12:59.828999Z"
        },
        "trusted": true,
        "id": "v4s40HCRTIsz"
      },
      "source": [
        "import numpy as np\n",
        "def get_weights(embedding_path):\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_path) as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "    print(\"Found %s word vectors.\" % len(dict(embeddings_index)))\n",
        "    \n",
        "    num_tokens = len(vocab)\n",
        "    embedding_dim = 300\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "    for word, index in tqdm(vocab.stoi.items()):\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[index] = embeddings_index[word]\n",
        "            hits+=1\n",
        "        else:\n",
        "            misses+=1\n",
        "            embedding_matrix[index] = np.random.uniform(-.1, .1, size=embedding_dim)\n",
        "    print(\"Hist:\", hits, \" | Misses:\", misses)\n",
        "    return embedding_matrix"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:12:59.831186Z",
          "iopub.execute_input": "2021-07-02T08:12:59.831555Z",
          "iopub.status.idle": "2021-07-02T08:14:06.350237Z",
          "shell.execute_reply.started": "2021-07-02T08:12:59.831520Z",
          "shell.execute_reply": "2021-07-02T08:14:06.349371Z"
        },
        "trusted": true,
        "id": "dfybi4X2TIsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98585cc-5490-47aa-f990-19d5e3a03d29"
      },
      "source": [
        "embedding_matrix = get_weights(\"./aravec.txt\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3309/3309 [00:00<00:00, 191618.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 662110 word vectors.\n",
            "Hist: 2723  | Misses: 586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:14:06.351704Z",
          "iopub.execute_input": "2021-07-02T08:14:06.352355Z",
          "iopub.status.idle": "2021-07-02T08:14:06.394518Z",
          "shell.execute_reply.started": "2021-07-02T08:14:06.352315Z",
          "shell.execute_reply": "2021-07-02T08:14:06.393269Z"
        },
        "trusted": true,
        "id": "DiYr4Kk_TIs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c782b80b-0837-4cef-ade5-c1380c729379"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3309, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T08:14:06.397087Z",
          "iopub.execute_input": "2021-07-02T08:14:06.397532Z",
          "iopub.status.idle": "2021-07-02T08:14:06.440650Z",
          "shell.execute_reply.started": "2021-07-02T08:14:06.397490Z",
          "shell.execute_reply": "2021-07-02T08:14:06.439760Z"
        },
        "trusted": true,
        "id": "Cu0DsLMpTIs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f60c3a9-49a7-4935-f35a-e2e4d58b3548"
      },
      "source": [
        "len(vocab.itos)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3309"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fb11e8e",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:39.426587Z",
          "iopub.execute_input": "2021-07-02T08:15:39.426932Z",
          "iopub.status.idle": "2021-07-02T08:15:39.505800Z",
          "shell.execute_reply.started": "2021-07-02T08:15:39.426904Z",
          "shell.execute_reply": "2021-07-02T08:15:39.504856Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706ef22a-0243-4abe-c4f3-f51dea528fcd"
      },
      "source": [
        "t_params = {\n",
        "    'data_name': DATA_NAME,\n",
        "    'imgs_path': IMGS_PATH,\n",
        "    'df_path': DATA_JSON_PATH,\n",
        "    'vocab': vocab,\n",
        "    'epochs': epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'workers': workers,\n",
        "    'decoder_lr': decoder_lr,\n",
        "    'encoder_lr': encoder_lr,\n",
        "    'fine_tune_encoder': fine_tune_encoder,\n",
        "    'pretrained_embeddings': pretrained_embeddings,\n",
        "    'fine_tune_embeddings': fine_tune_embeddings,\n",
        "}\n",
        "\n",
        "m_params = {\n",
        "    'attention_dim': attention_dim,\n",
        "    'embed_dim': emb_dim,\n",
        "    'decoder_dim': decoder_dim,\n",
        "    'encoder_dim': encoder_dim,\n",
        "    'dropout': dropout,\n",
        "    'embeddings_matrix': embedding_matrix\n",
        "}\n",
        "\n",
        "logger_dic = {\n",
        "    'decoder_lr': decoder_lr,\n",
        "    'encoder_lr': encoder_lr,\n",
        "    'fine_tune_encoder': fine_tune_encoder,\n",
        "    'pretrained_embeddings': pretrained_embeddings,\n",
        "    'max_seq_length': max_seq,\n",
        "    'vocab_size': vocab_len,\n",
        "    'enocder': 'resnet101',\n",
        "    'dropout': dropout,\n",
        "    'attention_dim': attention_dim,\n",
        "    'embed_dim': emb_dim,\n",
        "    'decoder_dim': decoder_dim,\n",
        "    'encoder_dim': encoder_dim \n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "t_params"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 256,\n",
              " 'data_name': 'flickr8k_ar_arabert_pretrained',\n",
              " 'decoder_lr': 0.0004,\n",
              " 'df_path': 'Image-Captioning/ar_data.json',\n",
              " 'encoder_lr': 0.0001,\n",
              " 'epochs': 30,\n",
              " 'fine_tune_embeddings': True,\n",
              " 'fine_tune_encoder': False,\n",
              " 'imgs_path': 'flickr8k/images/',\n",
              " 'pretrained_embeddings': True,\n",
              " 'vocab': <dataset.Vocabulary at 0x7f1e787bea10>,\n",
              " 'workers': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d8e8ebf",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:40.200341Z",
          "iopub.execute_input": "2021-07-02T08:15:40.200687Z",
          "iopub.status.idle": "2021-07-02T08:15:40.266679Z",
          "shell.execute_reply.started": "2021-07-02T08:15:40.200657Z",
          "shell.execute_reply": "2021-07-02T08:15:40.265764Z"
        },
        "trusted": true
      },
      "source": [
        "# experiment name\n",
        "name = DATA_NAME + \"pretrained\"\n",
        "# path\n",
        "log_dir = 'experiments'\n",
        "\n",
        "logger = SummaryWriter(log_dir=osp.join(log_dir, name))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIOlt7gxZAbd",
        "execution": {
          "iopub.status.busy": "2021-07-02T08:15:41.100495Z",
          "iopub.execute_input": "2021-07-02T08:15:41.100810Z",
          "iopub.status.idle": "2021-07-02T08:47:52.200137Z",
          "shell.execute_reply.started": "2021-07-02T08:15:41.100783Z",
          "shell.execute_reply": "2021-07-02T08:47:52.199139Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a823475f6afe4e2f8a6895f7fefcaa0d",
            "a92e7eddbbda46298396270c55f39a4b",
            "b8187474591446c3b6cd9cce810c474a",
            "d4d348c96ace470e8844221c280c1892",
            "5e89f66f2a5b4d03bcf4010a8c93326a",
            "d0674ff2ef24428d9ac7b639db4da7cd",
            "d7b3dafebaf544f8a3b8bac747640c63",
            "891eac0414404f48a8b86aca2a5f0b6a"
          ]
        },
        "outputId": "550d9bf9-3faf-45b3-c99a-e8abfa0ffecd"
      },
      "source": [
        "fit(t_params=t_params, m_params=m_params, logger=logger)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a823475f6afe4e2f8a6895f7fefcaa0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178793939.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading Data\n",
            "Dataset split: train\n",
            "Unique images: 6000\n",
            "Total size: 18000\n",
            "Dataset split: val\n",
            "Unique images: 1000\n",
            "Total size: 3000\n",
            "__________________________________________________\n",
            "-------------------- Fitting --------------------\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/71]\tBatch Time 10.000 (10.000)\tData Load Time 4.665 (4.665)\tLoss 8.9380 (8.9380)\tTop-5 Accuracy 0.122 (0.122)\n",
            "Epoch train time 191.608 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.636 (6.636)\tLoss 4.7598 (4.7598)\tTop-5 Accuracy 58.225 (58.225)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 63.2881016448392\n",
            "2: 43.23017418363778\n",
            "3: 25.534674601162727\n",
            "4: 15.0724907619684\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.902, TOP-5 ACCURACY - 57.098, BLEU-4 - 15.0724907619684\n",
            "\n",
            "Epoch validation time 38.831 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [1][0/71]\tBatch Time 6.587 (6.587)\tData Load Time 3.837 (3.837)\tLoss 3.9533 (3.9533)\tTop-5 Accuracy 63.711 (63.711)\n",
            "Epoch train time 184.460 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.258 (6.258)\tLoss 5.1243 (5.1243)\tTop-5 Accuracy 57.104 (57.104)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 63.32358023018996\n",
            "2: 45.900808237815305\n",
            "3: 28.746861903093148\n",
            "4: 18.468522499977034\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.803, TOP-5 ACCURACY - 59.736, BLEU-4 - 18.468522499977034\n",
            "\n",
            "Epoch validation time 35.464 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [2][0/71]\tBatch Time 6.501 (6.501)\tData Load Time 3.944 (3.944)\tLoss 3.7183 (3.7183)\tTop-5 Accuracy 67.233 (67.233)\n",
            "Epoch train time 184.367 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.725 (6.725)\tLoss 4.5832 (4.5832)\tTop-5 Accuracy 62.545 (62.545)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.65493939683755\n",
            "2: 51.554702217333094\n",
            "3: 33.68741518230697\n",
            "4: 22.4084278994587\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.672, TOP-5 ACCURACY - 61.825, BLEU-4 - 22.4084278994587\n",
            "\n",
            "Epoch validation time 35.197 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [3][0/71]\tBatch Time 6.262 (6.262)\tData Load Time 3.725 (3.725)\tLoss 3.5466 (3.5466)\tTop-5 Accuracy 69.198 (69.198)\n",
            "Epoch train time 184.000 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.736 (6.736)\tLoss 4.7838 (4.7838)\tTop-5 Accuracy 61.561 (61.561)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.67257215285015\n",
            "2: 49.99800568130966\n",
            "3: 33.15836200488258\n",
            "4: 22.319816718725047\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.620, TOP-5 ACCURACY - 62.821, BLEU-4 - 22.319816718725047\n",
            "\n",
            "Epoch validation time 37.254 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (1,)\n",
            "Epoch     4: reducing learning rate of group 0 to 4.0000e-05.\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [4][0/71]\tBatch Time 6.461 (6.461)\tData Load Time 3.865 (3.865)\tLoss 3.4795 (3.4795)\tTop-5 Accuracy 70.197 (70.197)\n",
            "Epoch train time 185.221 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.560 (6.560)\tLoss 4.5813 (4.5813)\tTop-5 Accuracy 63.283 (63.283)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.01107835597102\n",
            "2: 51.26329725878674\n",
            "3: 34.215051820411794\n",
            "4: 23.306383513814726\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.580, TOP-5 ACCURACY - 63.445, BLEU-4 - 23.306383513814726\n",
            "\n",
            "Epoch validation time 36.272 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [5][0/71]\tBatch Time 6.721 (6.721)\tData Load Time 3.998 (3.998)\tLoss 3.3673 (3.3673)\tTop-5 Accuracy 70.896 (70.896)\n",
            "Epoch train time 184.308 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.512 (6.512)\tLoss 4.6781 (4.6781)\tTop-5 Accuracy 62.589 (62.589)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.07548493803768\n",
            "2: 51.33001667942977\n",
            "3: 34.40367525772597\n",
            "4: 23.49843727078223\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.577, TOP-5 ACCURACY - 63.598, BLEU-4 - 23.49843727078223\n",
            "\n",
            "Epoch validation time 35.583 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [6][0/71]\tBatch Time 6.297 (6.297)\tData Load Time 3.801 (3.801)\tLoss 3.3673 (3.3673)\tTop-5 Accuracy 71.075 (71.075)\n",
            "Epoch train time 188.527 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.576 (6.576)\tLoss 4.6129 (4.6129)\tTop-5 Accuracy 63.598 (63.598)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.88770756434319\n",
            "2: 51.417853490869234\n",
            "3: 34.52191165823768\n",
            "4: 23.64870583972747\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.579, TOP-5 ACCURACY - 63.738, BLEU-4 - 23.64870583972747\n",
            "\n",
            "Epoch validation time 35.035 (epoch_time.avg:.3f)\n",
            "Epoch     7: reducing learning rate of group 0 to 4.0000e-06.\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [7][0/71]\tBatch Time 6.372 (6.372)\tData Load Time 3.750 (3.750)\tLoss 3.2802 (3.2802)\tTop-5 Accuracy 72.152 (72.152)\n",
            "Epoch train time 186.460 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.462 (6.462)\tLoss 4.4231 (4.4231)\tTop-5 Accuracy 64.918 (64.918)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.98406689949682\n",
            "2: 51.37193511092409\n",
            "3: 34.42836633715436\n",
            "4: 23.54268985493326\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.579, TOP-5 ACCURACY - 63.712, BLEU-4 - 23.54268985493326\n",
            "\n",
            "Epoch validation time 36.255 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (1,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [8][0/71]\tBatch Time 6.322 (6.322)\tData Load Time 3.728 (3.728)\tLoss 3.3497 (3.3497)\tTop-5 Accuracy 71.117 (71.117)\n",
            "Epoch train time 185.647 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.349 (6.349)\tLoss 4.7992 (4.7992)\tTop-5 Accuracy 62.036 (62.036)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.9958022048304\n",
            "2: 51.3763476607291\n",
            "3: 34.438166225513335\n",
            "4: 23.549843228428575\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.581, TOP-5 ACCURACY - 63.747, BLEU-4 - 23.549843228428575\n",
            "\n",
            "Epoch validation time 34.314 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (2,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [9][0/71]\tBatch Time 6.631 (6.631)\tData Load Time 3.989 (3.989)\tLoss 3.3795 (3.3795)\tTop-5 Accuracy 71.414 (71.414)\n",
            "Epoch train time 186.510 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.447 (6.447)\tLoss 4.5224 (4.5224)\tTop-5 Accuracy 64.443 (64.443)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.95738016816205\n",
            "2: 51.338605382472004\n",
            "3: 34.40859275915968\n",
            "4: 23.530168151839828\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.582, TOP-5 ACCURACY - 63.768, BLEU-4 - 23.530168151839828\n",
            "\n",
            "Epoch validation time 34.850 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (3,)\n",
            "Epoch    10: reducing learning rate of group 0 to 4.0000e-07.\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [10][0/71]\tBatch Time 6.297 (6.297)\tData Load Time 3.706 (3.706)\tLoss 3.3166 (3.3166)\tTop-5 Accuracy 72.669 (72.669)\n",
            "Epoch train time 186.275 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.591 (6.591)\tLoss 4.6832 (4.6832)\tTop-5 Accuracy 62.869 (62.869)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.00594934770645\n",
            "2: 51.38221951189068\n",
            "3: 34.46819015153822\n",
            "4: 23.57614744926983\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.583, TOP-5 ACCURACY - 63.784, BLEU-4 - 23.57614744926983\n",
            "\n",
            "Epoch validation time 34.707 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (4,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [11][0/71]\tBatch Time 6.455 (6.455)\tData Load Time 3.827 (3.827)\tLoss 3.3560 (3.3560)\tTop-5 Accuracy 71.285 (71.285)\n",
            "Epoch train time 186.389 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/12]\tBatch Time 6.447 (6.447)\tLoss 4.6002 (4.6002)\tTop-5 Accuracy 63.388 (63.388)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.01192715736615\n",
            "2: 51.384777287083935\n",
            "3: 34.42768181184052\n",
            "4: 23.53528872867777\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.582, TOP-5 ACCURACY - 63.773, BLEU-4 - 23.53528872867777\n",
            "\n",
            "Epoch validation time 35.364 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (5,)\n",
            "No improvement for 5 consecutive epochs, terminating...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-02T09:07:59.797661Z",
          "iopub.execute_input": "2021-07-02T09:07:59.798068Z",
          "iopub.status.idle": "2021-07-02T09:08:00.766078Z",
          "shell.execute_reply.started": "2021-07-02T09:07:59.798032Z",
          "shell.execute_reply": "2021-07-02T09:08:00.765167Z"
        },
        "trusted": true,
        "id": "k_MBtdppTIs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9fd82a-b355-4840-a8a3-bc6df8b151fb"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aravec.txt\n",
            "BEST_checkpoint_flickr8k_ar_arabert_pretrained.pth.tar\n",
            "checkpoint_flickr8k_ar_arabert_pretrained.pth.tar\n",
            "drive\n",
            "experiments\n",
            "flickr8k\n",
            "full_grams_cbow_300_wiki.mdl\n",
            "full_grams_cbow_300_wiki.mdl.trainables.syn1neg.npy\n",
            "full_grams_cbow_300_wiki.mdl.wv.vectors.npy\n",
            "full_grams_cbow_300_wiki.zip\n",
            "Image-Captioning\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgtlSQWzjNwz",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:08:22.119697Z",
          "iopub.execute_input": "2021-07-02T09:08:22.120013Z",
          "iopub.status.idle": "2021-07-02T09:08:22.495609Z",
          "shell.execute_reply.started": "2021-07-02T09:08:22.119986Z",
          "shell.execute_reply": "2021-07-02T09:08:22.494672Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f120ae-66ca-4c0e-9efa-ccef897b0926"
      },
      "source": [
        "m = load_checkpoint(\"BEST_checkpoint_flickr8k_ar_arabert_pretrained.pth.tar\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 6\n",
            "Best Bleu-4: 23.64870583972747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJAelDR8ZDsk",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:08:43.082843Z",
          "iopub.execute_input": "2021-07-02T09:08:43.083173Z",
          "iopub.status.idle": "2021-07-02T09:08:43.153713Z",
          "shell.execute_reply.started": "2021-07-02T09:08:43.083143Z",
          "shell.execute_reply": "2021-07-02T09:08:43.152656Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa20674-c5da-4aa5-cc45-bc5fb017f3c9"
      },
      "source": [
        "batch_size = 64\n",
        "fine_tune_encoder = True\n",
        "checkpoint = 'BEST_checkpoint_flickr8k_ar_arabert_pretrained.pth.tar'\n",
        "# epochs = 30\n",
        "\n",
        "t_params['batch_size'] = batch_size\n",
        "t_params['data_name'] = t_params['data_name'] + \"_finetune\" \n",
        "t_params['fine_tune_encoder'] = True\n",
        "t_params['decoder_lr'] = t_params['decoder_lr'] / 10\n",
        "# t_params['epochs'] = epochs\n",
        "t_params"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'data_name': 'flickr8k_ar_arabert_pretrained_finetune',\n",
              " 'decoder_lr': 4e-05,\n",
              " 'df_path': 'Image-Captioning/ar_data.json',\n",
              " 'encoder_lr': 0.0001,\n",
              " 'epochs': 30,\n",
              " 'fine_tune_embeddings': True,\n",
              " 'fine_tune_encoder': True,\n",
              " 'imgs_path': 'flickr8k/images/',\n",
              " 'pretrained_embeddings': True,\n",
              " 'vocab': <dataset.Vocabulary at 0x7f1e787bea10>,\n",
              " 'workers': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYhbvtifjXD6",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:08:46.126090Z",
          "iopub.execute_input": "2021-07-02T09:08:46.126466Z",
          "iopub.status.idle": "2021-07-02T09:34:21.399955Z",
          "shell.execute_reply.started": "2021-07-02T09:08:46.126433Z",
          "shell.execute_reply": "2021-07-02T09:34:21.398331Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8d1a55-b81b-421c-d32d-36b062f20abe"
      },
      "source": [
        "fit(t_params, checkpoint=checkpoint, m_params=m_params, logger=logger)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Starting Epoch: 7\n",
            "Loading Data\n",
            "Dataset split: train\n",
            "Unique images: 6000\n",
            "Total size: 18000\n",
            "Dataset split: val\n",
            "Unique images: 1000\n",
            "Total size: 3000\n",
            "__________________________________________________\n",
            "-------------------- Fitting --------------------\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [7][0/282]\tBatch Time 4.982 (4.982)\tData Load Time 1.191 (1.191)\tLoss 3.3514 (3.3514)\tTop-5 Accuracy 72.744 (72.744)\n",
            "Epoch: [7][100/282]\tBatch Time 1.390 (1.427)\tData Load Time 0.001 (0.013)\tLoss 3.3280 (3.3822)\tTop-5 Accuracy 72.120 (71.114)\n",
            "Epoch: [7][200/282]\tBatch Time 1.370 (1.407)\tData Load Time 0.003 (0.007)\tLoss 3.3692 (3.3701)\tTop-5 Accuracy 72.269 (71.366)\n",
            "Epoch train time 394.684 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.635 (1.635)\tLoss 4.3954 (4.3954)\tTop-5 Accuracy 65.733 (65.733)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.14225421073516\n",
            "2: 51.78001946828103\n",
            "3: 35.161198830649646\n",
            "4: 24.293698607873516\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.583, TOP-5 ACCURACY - 63.617, BLEU-4 - 24.293698607873516\n",
            "\n",
            "Epoch validation time 35.162 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [8][0/282]\tBatch Time 2.541 (2.541)\tData Load Time 1.165 (1.165)\tLoss 3.3940 (3.3940)\tTop-5 Accuracy 70.763 (70.763)\n",
            "Epoch: [8][100/282]\tBatch Time 1.384 (1.402)\tData Load Time 0.001 (0.013)\tLoss 3.2660 (3.3008)\tTop-5 Accuracy 73.583 (72.286)\n",
            "Epoch: [8][200/282]\tBatch Time 1.364 (1.391)\tData Load Time 0.001 (0.007)\tLoss 3.2762 (3.2997)\tTop-5 Accuracy 72.755 (72.361)\n",
            "Epoch train time 390.384 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.802 (1.802)\tLoss 5.1167 (5.1167)\tTop-5 Accuracy 61.506 (61.506)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 69.80788266604\n",
            "2: 51.513419741587306\n",
            "3: 35.03044932605629\n",
            "4: 24.315390933388173\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.575, TOP-5 ACCURACY - 64.061, BLEU-4 - 24.315390933388173\n",
            "\n",
            "Epoch validation time 35.988 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [9][0/282]\tBatch Time 2.524 (2.524)\tData Load Time 1.140 (1.140)\tLoss 3.2076 (3.2076)\tTop-5 Accuracy 71.888 (71.888)\n",
            "Epoch: [9][100/282]\tBatch Time 1.369 (1.400)\tData Load Time 0.001 (0.012)\tLoss 3.2724 (3.2585)\tTop-5 Accuracy 72.313 (73.064)\n",
            "Epoch: [9][200/282]\tBatch Time 1.426 (1.390)\tData Load Time 0.000 (0.007)\tLoss 3.3640 (3.2541)\tTop-5 Accuracy 70.255 (73.108)\n",
            "Epoch train time 390.599 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.625 (1.625)\tLoss 4.1885 (4.1885)\tTop-5 Accuracy 66.519 (66.519)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.63874351076959\n",
            "2: 52.42886751359374\n",
            "3: 35.8296799557061\n",
            "4: 24.949378413361714\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.584, TOP-5 ACCURACY - 64.224, BLEU-4 - 24.949378413361714\n",
            "\n",
            "Epoch validation time 33.692 (epoch_time.avg:.3f)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [10][0/282]\tBatch Time 2.491 (2.491)\tData Load Time 1.096 (1.096)\tLoss 3.3201 (3.3201)\tTop-5 Accuracy 72.754 (72.754)\n",
            "Epoch: [10][100/282]\tBatch Time 1.418 (1.396)\tData Load Time 0.001 (0.012)\tLoss 3.3493 (3.2158)\tTop-5 Accuracy 71.907 (73.669)\n",
            "Epoch: [10][200/282]\tBatch Time 1.380 (1.389)\tData Load Time 0.001 (0.006)\tLoss 3.2475 (3.2180)\tTop-5 Accuracy 73.628 (73.643)\n",
            "Epoch train time 390.598 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.731 (1.731)\tLoss 4.5972 (4.5972)\tTop-5 Accuracy 63.483 (63.483)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.50656789809156\n",
            "2: 52.23808419436923\n",
            "3: 35.61945610703764\n",
            "4: 24.75931809750531\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.588, TOP-5 ACCURACY - 64.312, BLEU-4 - 24.75931809750531\n",
            "\n",
            "Epoch validation time 32.636 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (1,)\n",
            "Epoch     4: reducing learning rate of group 0 to 4.0000e-07.\n",
            "Epoch     4: reducing learning rate of group 0 to 1.0000e-05.\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [11][0/282]\tBatch Time 2.409 (2.409)\tData Load Time 1.020 (1.020)\tLoss 3.2515 (3.2515)\tTop-5 Accuracy 73.129 (73.129)\n",
            "Epoch: [11][100/282]\tBatch Time 1.356 (1.401)\tData Load Time 0.001 (0.011)\tLoss 3.3178 (3.1677)\tTop-5 Accuracy 72.030 (74.301)\n",
            "Epoch: [11][200/282]\tBatch Time 1.373 (1.392)\tData Load Time 0.000 (0.006)\tLoss 3.2044 (3.1656)\tTop-5 Accuracy 73.550 (74.320)\n",
            "Epoch train time 390.395 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.741 (1.741)\tLoss 4.9036 (4.9036)\tTop-5 Accuracy 62.355 (62.355)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.59662124168447\n",
            "2: 52.204109928378585\n",
            "3: 35.70822191822237\n",
            "4: 24.818428886722398\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.587, TOP-5 ACCURACY - 64.477, BLEU-4 - 24.818428886722398\n",
            "\n",
            "Epoch validation time 32.601 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (2,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [12][0/282]\tBatch Time 2.496 (2.496)\tData Load Time 1.052 (1.052)\tLoss 3.0330 (3.0330)\tTop-5 Accuracy 75.806 (75.806)\n",
            "Epoch: [12][100/282]\tBatch Time 1.358 (1.397)\tData Load Time 0.001 (0.011)\tLoss 3.1605 (3.1655)\tTop-5 Accuracy 75.377 (74.187)\n",
            "Epoch: [12][200/282]\tBatch Time 1.362 (1.390)\tData Load Time 0.000 (0.006)\tLoss 3.1211 (3.1517)\tTop-5 Accuracy 74.896 (74.455)\n",
            "Epoch train time 390.375 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.679 (1.679)\tLoss 4.1498 (4.1498)\tTop-5 Accuracy 67.933 (67.933)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.5005770963341\n",
            "2: 52.27955752019226\n",
            "3: 35.82897839460508\n",
            "4: 24.913143783034908\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.597, TOP-5 ACCURACY - 64.424, BLEU-4 - 24.913143783034908\n",
            "\n",
            "Epoch validation time 33.238 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (3,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [13][0/282]\tBatch Time 2.414 (2.414)\tData Load Time 1.034 (1.034)\tLoss 3.0285 (3.0285)\tTop-5 Accuracy 76.091 (76.091)\n",
            "Epoch: [13][100/282]\tBatch Time 1.405 (1.399)\tData Load Time 0.001 (0.011)\tLoss 3.1335 (3.1470)\tTop-5 Accuracy 75.024 (74.496)\n",
            "Epoch: [13][200/282]\tBatch Time 1.380 (1.389)\tData Load Time 0.001 (0.006)\tLoss 3.2257 (3.1455)\tTop-5 Accuracy 73.399 (74.625)\n",
            "Epoch train time 390.248 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.727 (1.727)\tLoss 4.7231 (4.7231)\tTop-5 Accuracy 62.844 (62.844)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.48933306344793\n",
            "2: 52.162023172971104\n",
            "3: 35.677580246452855\n",
            "4: 24.835065245862495\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.595, TOP-5 ACCURACY - 64.447, BLEU-4 - 24.835065245862495\n",
            "\n",
            "Epoch validation time 32.814 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (4,)\n",
            "Epoch     7: reducing learning rate of group 0 to 4.0000e-08.\n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-06.\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [14][0/282]\tBatch Time 2.620 (2.620)\tData Load Time 1.188 (1.188)\tLoss 3.1580 (3.1580)\tTop-5 Accuracy 75.076 (75.076)\n",
            "Epoch: [14][100/282]\tBatch Time 1.341 (1.398)\tData Load Time 0.001 (0.013)\tLoss 3.0585 (3.1425)\tTop-5 Accuracy 77.297 (74.626)\n",
            "Epoch: [14][200/282]\tBatch Time 1.409 (1.393)\tData Load Time 0.001 (0.007)\tLoss 3.1293 (3.1346)\tTop-5 Accuracy 74.410 (74.758)\n",
            "Epoch train time 391.134 (epoch_time.avg:.3f)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/47]\tBatch Time 1.603 (1.603)\tLoss 4.7218 (4.7218)\tTop-5 Accuracy 63.604 (63.604)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 70.42748079332306\n",
            "2: 52.13992941871136\n",
            "3: 35.64567925588605\n",
            "4: 24.815355764643208\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.599, TOP-5 ACCURACY - 64.426, BLEU-4 - 24.815355764643208\n",
            "\n",
            "Epoch validation time 31.324 (epoch_time.avg:.3f)\n",
            "\n",
            "Epochs since last improvement: (5,)\n",
            "No improvement for 5 consecutive epochs, terminating...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMMO_K-2LaYR"
      },
      "source": [
        "### Test Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knnG-VzB6gu7",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:52:17.735653Z",
          "iopub.execute_input": "2021-07-02T09:52:17.735973Z",
          "iopub.status.idle": "2021-07-02T09:52:18.162418Z",
          "shell.execute_reply.started": "2021-07-02T09:52:17.735943Z",
          "shell.execute_reply": "2021-07-02T09:52:18.161492Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddde2f7-cad2-46fc-c0f4-a52a273b5bf1"
      },
      "source": [
        "checkpoint = load_checkpoint(\"BEST_checkpoint_flickr8k_ar_arabert_pretrained_finetune.pth.tar\")\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval();"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 9\n",
            "Best Bleu-4: 24.949378413361714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reIYwZ6O6ncg",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:52:35.779796Z",
          "iopub.execute_input": "2021-07-02T09:52:35.780129Z",
          "iopub.status.idle": "2021-07-02T09:59:00.851666Z",
          "shell.execute_reply.started": "2021-07-02T09:52:35.780099Z",
          "shell.execute_reply": "2021-07-02T09:59:00.850406Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9f5c89-c07a-4934-9bf7-429ed99f7573"
      },
      "source": [
        "from eval import test_score\n",
        "\n",
        "test_dict = {}\n",
        "\n",
        "for i in [1, 3, 5]:\n",
        "    \n",
        "    b1, b2, b3, b4 = test_score(i, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab)\n",
        "    if i == 3:\n",
        "        test_dict['b1'] = b1\n",
        "        test_dict['b2'] = b2\n",
        "        test_dict['b3'] = b3\n",
        "    \n",
        "    test_dict[f'b4-b{i}'] = b4"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\rEVALUATING AT BEAM SIZE 1:   0%|          | 0/3000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 3000/3000 [02:11<00:00, 22.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/3000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: 59.27134312126155\n",
            "2: 45.52397958654338\n",
            "3: 33.58850576504064\n",
            "4: 24.918812277227662\n",
            "-------------------------\n",
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 3000/3000 [02:27<00:00, 20.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 60.32593136195551\n",
            "2: 47.5536072957737\n",
            "3: 36.147037636633875\n",
            "4: 27.524282207029003\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\rEVALUATING AT BEAM SIZE 5:   0%|          | 0/3000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 5: 100%|██████████| 3000/3000 [02:42<00:00, 18.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 58.678484239386094\n",
            "2: 46.85683508774053\n",
            "3: 36.14555791431082\n",
            "4: 27.864202291806382\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzgkO1EF6p_R",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:59:00.853468Z",
          "iopub.execute_input": "2021-07-02T09:59:00.853845Z",
          "iopub.status.idle": "2021-07-02T09:59:00.927715Z",
          "shell.execute_reply.started": "2021-07-02T09:59:00.853802Z",
          "shell.execute_reply": "2021-07-02T09:59:00.926813Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a9a26e-e2f9-4c03-d03d-5f7da85c02ad"
      },
      "source": [
        "test_dict"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b1': 60.32593136195551,\n",
              " 'b2': 47.5536072957737,\n",
              " 'b3': 36.147037636633875,\n",
              " 'b4-b1': 24.918812277227662,\n",
              " 'b4-b3': 27.524282207029003,\n",
              " 'b4-b5': 27.864202291806382}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i5rSOU78FlS",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:59:10.858838Z",
          "iopub.execute_input": "2021-07-02T09:59:10.859205Z",
          "iopub.status.idle": "2021-07-02T09:59:10.923520Z",
          "shell.execute_reply.started": "2021-07-02T09:59:10.859172Z",
          "shell.execute_reply": "2021-07-02T09:59:10.922447Z"
        },
        "trusted": true
      },
      "source": [
        "# final results -> different from training and validation scalars\n",
        "results_dic =  {\n",
        "    # train & valid\n",
        "    'total_epochs': 5.653,\n",
        "    'b-1/test': test_dict['b1'],\n",
        "    'b-2/test': test_dict['b2'],\n",
        "    'b-3/test': test_dict['b3'],\n",
        "    'b-4/b3': test_dict['b4-b3'],\n",
        "    'b-4/b1': test_dict['b4-b1'],\n",
        "    'b-4/b5': test_dict['b4-b5']\n",
        "}"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mStUSnYm8fPC",
        "execution": {
          "iopub.status.busy": "2021-07-02T09:59:18.114884Z",
          "iopub.execute_input": "2021-07-02T09:59:18.115237Z",
          "iopub.status.idle": "2021-07-02T09:59:18.193545Z",
          "shell.execute_reply.started": "2021-07-02T09:59:18.115204Z",
          "shell.execute_reply": "2021-07-02T09:59:18.192755Z"
        },
        "trusted": true
      },
      "source": [
        "logger.add_hparams(logger_dic, results_dic, run_name='pretrianed')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE_gfxdIHTR4"
      },
      "source": [
        "### Test Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV3iqilrHUT2"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oltMOg3bHZkV"
      },
      "source": [
        "def caption_image(img_path, beam_size=3):\n",
        "    \n",
        "    # transforms\n",
        "    tt = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "\n",
        "    # english\n",
        "    # vocab = build_vocab('data.json')\n",
        "    # checkpoint = load_checkpoint('E:\\GP\\Image-Captioning\\models\\BEST_checkpoint_flickr8k_finetune.pth.tar', cpu=True)\n",
        "\n",
        "    # arabic\n",
        "    vocab = build_vocab('/content/Image-Captioning/ar_data.json')\n",
        "    checkpoint = load_checkpoint('BEST_checkpoint_flickr8k_ar_arabert_pretrained_finetune.pth.tar', cpu=True)\n",
        "\n",
        "\n",
        "    addit_tokens = [vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']]\n",
        "    device = torch.device( 'cpu')\n",
        "\n",
        "\n",
        "    encoder = checkpoint['encoder'].to(device)\n",
        "    decoder = checkpoint['decoder'].to(device)\n",
        "\n",
        "    #def cap_image(encoder, decoder, image_path, vocab):\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = tt(img).unsqueeze(0) # transform and batch\n",
        "    image = img.to(device)\n",
        "\n",
        "    #encoder\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "\n",
        "\n",
        "    k = beam_size \n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "        \n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "#         print(seqs[prev_word_inds], prev_word_inds)\n",
        "#         if step == 5:\n",
        "#             return seqs\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != vocab.stoi['<eos>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    print(complete_seqs_scores)\n",
        "    # print(seq)\n",
        "    all_caps = [\" \".join([vocab.itos[i] for i in sent if i not in addit_tokens]) for sent in complete_seqs]\n",
        "    all_b_caps = \"\"\n",
        "    z = 1\n",
        "    for cap in all_caps:\n",
        "        all_b_caps += str(z) + \". \" + cap + \" || <br> \"\n",
        "        z += 1\n",
        "    # all_b_caps = [\" || \".join(all_caps)][0]\n",
        "\n",
        "\n",
        "    # return seq, alphas, complete_seqs, i\n",
        "    # return [\" \".join([vocab.itos[i] for i in seq if i not in addit_tokens])][0]\n",
        "    # return all_b_caps\n",
        "    return alphas, seq, all_caps"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wes35OpHtVB",
        "outputId": "ef598ba4-f940-441c-8fd2-2207b9b64beb"
      },
      "source": [
        "alphas, seq, all_caps = caption_image(\"/content/Image-Captioning/test_examples/dog.jpg\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 24000/24000 [00:00<00:00, 189372.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 9\n",
            "Best Bleu-4: 24.949378413361714\n",
            "[tensor(-5.3566, grad_fn=<UnbindBackward>), tensor(-40.7110, grad_fn=<UnbindBackward>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5E_lC1Hw2Z",
        "outputId": "25f43e05-18e8-448a-df12-9f20ee9e5bf1"
      },
      "source": [
        "seq"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 12, 65, 41, 18, 10, 4, 47, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9BVrrPXH9_o",
        "outputId": "20c83e23-88d5-4842-8c74-89a156252e6b"
      },
      "source": [
        "[vocab.itos[i] for i in seq[1:-1]]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['كلب', 'بني', 'يركض', 'على', 'ال', '+', 'شاطئ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7_Hm1OjId0K"
      },
      "source": [
        "# # libraries for arabert\n",
        "# !pip install farasapy\n",
        "# !pip install pyarabic\n",
        "# !pip install fuzzysearch"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgWX2AemIefg",
        "outputId": "84f7c78f-55f2-4c1b-df3c-67e7394bc325"
      },
      "source": [
        "!git clone https://github.com/aub-mind/arabert"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 530, done.\u001b[K\n",
            "remote: Counting objects: 100% (316/316), done.\u001b[K\n",
            "remote: Compressing objects: 100% (228/228), done.\u001b[K\n",
            "remote: Total 530 (delta 167), reused 226 (delta 82), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (530/530), 4.86 MiB | 23.70 MiB/s, done.\n",
            "Resolving deltas: 100% (290/290), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "v7mPwg45Il3w",
        "outputId": "124f9535-548e-41d3-ae33-3f75c39decb0"
      },
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "arabert_prep.unpreprocess(\" \".join([vocab.itos[i] for i in seq[1:-1]]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 239M/241M [00:19<00:00, 12.1MiB/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-07-17 15:44:16,059 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'كلب بني يركض على الشاطئ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4yuNrLSJdXl"
      },
      "source": [
        "### Test without replication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzVIukCLB-F"
      },
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    \"\"\" \n",
        "    Caption Dataset Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, imgs_dir, captions_file, vocab, transforms=None, split='train'):\n",
        "        \"\"\"\n",
        "        :param imgs_dir: folder where images are stored\n",
        "        :param captions_file: the df file with all caption information\n",
        "        :param vocab: vocabuary object\n",
        "        :param transforms: image transforms pipeline\n",
        "        :param split: data split\n",
        "        \"\"\"\n",
        "\n",
        "        # split has to be one of {'train', 'val', 'test'}\n",
        "        assert split in {'train', 'val', 'test'}\n",
        "\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.df = pd.read_json(captions_file)\n",
        "        self.df = self.df[self.df['split'] == split]\n",
        "        self.vocab = vocab\n",
        "        self.transforms = transforms\n",
        "        self.split = split\n",
        "\n",
        "        self.dataset_size = self.df.shape[0]\n",
        "        # printing some info\n",
        "        print(f\"Dataset split: {split}\")\n",
        "        print(f\"Unique images: {self.df.file_name.nunique()}\")\n",
        "        print(f\"Total size: {self.dataset_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # loading the image\n",
        "        img_id = self.df['file_name'].values[index]\n",
        "        img = Image.open(self.imgs_dir+img_id).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = transfroms.ToTensor()(img)\n",
        "\n",
        "        # loading current caption\n",
        "        cap_len = self.df['tok_len'].values[index] + 2 # <sos> and <eos>\n",
        "        tokens = self.df['tokens'].values[index]\n",
        "        caption = torch.LongTensor(self.vocab.numericalize(tokens, cap_len))\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, cap_len\n",
        "        else:\n",
        "            # for val and test return all captions for calculate the bleu scores\n",
        "            captions_tokens = self.df[self.df['file_name'] == img_id].tokens.values\n",
        "            captions_lens = self.df[self.df['file_name'] == img_id].tok_len.values\n",
        "            all_tokens = []\n",
        "            for token, cap_len in zip(captions_tokens, captions_lens):\n",
        "                all_tokens.append(self.vocab.numericalize(token, cap_len)[1:]) # remove <sos>\n",
        "\n",
        "            return img, caption, cap_len, torch.tensor(all_tokens), img_id"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLYO5vZ1LE6n",
        "outputId": "0d3e855a-da0b-4df0-8e19-5005e3f8efe4"
      },
      "source": [
        "bs = 1\n",
        "\n",
        "loader = DataLoader(\n",
        "            dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
        "                                    transforms=transform, vocab=vocab, split='test'),\n",
        "            batch_size=bs,\n",
        "            num_workers=7,\n",
        "            shuffle=True,\n",
        "            pin_memory=True\n",
        "        )"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 7 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4C5omQGJC0V"
      },
      "source": [
        "# Test without replication \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from utils import load_checkpoint\n",
        "from dataset import build_vocab, get_loaders\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from utils import print_scores\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def evaluate(beam_size):\n",
        "    vocab_size = len(vocab)\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "    img_ids = list()\n",
        "    \n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n",
        "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
        "        \n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "        \n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
        "          \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "            \n",
        "#             print(top_k_scores, top_k_words)\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab.stoi['<eos>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        if len(complete_seqs_scores) == 0:\n",
        "            continue\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
        "        \n",
        "        img_ids.append(img_id[0])\n",
        "        assert len(references) == len(hypotheses) == len(img_ids)\n",
        "    # Calculate BLEU-4 scores\n",
        "#     bleu4 = corpus_bleu(references, hypotheses)\n",
        "    return references, hypotheses, img_ids\n",
        "    # print_scores(references, hypotheses, nltk=True)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjJs1fKwKcM-",
        "outputId": "1668bb51-c8fb-4e88-ecb7-a47bafd09630"
      },
      "source": [
        "references, hypothesis, img_ids = evaluate(1)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 1:   0%|          | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 7 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 3000/3000 [02:12<00:00, 22.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1dUKVEhLvDf",
        "outputId": "da09ad9a-eb9b-42ce-b45b-02a9929812ae"
      },
      "source": [
        "print_scores(references, hypothesis)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 59.27134312126155\n",
            "2: 45.52397958654338\n",
            "3: 33.58850576504064\n",
            "4: 24.918812277227662\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59.27134312126155, 45.52397958654338, 33.58850576504064, 24.918812277227662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RGUNDr_kKpHv",
        "outputId": "b5e773b4-de03-421d-a6d4-f8e9ee3ad9e6"
      },
      "source": [
        "df = pd.DataFrame.from_dict({\"file_name\":img_ids, \"references\":references, \"hypothesis\": hypothesis})\n",
        "df.head()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>references</th>\n",
              "      <th>hypothesis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>354642192_3b7666a2dd.jpg</td>\n",
              "      <td>[[78, 1315, 16, 10, 4, 66, 21, 4, 10, 4, 44, 4...</td>\n",
              "      <td>[10, 4, 6, 4, 5, 16, 10, 4, 66]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1237985362_dbafc59280.jpg</td>\n",
              "      <td>[[283, 4, 5, 399, 20, 4, 5, 7, 4, 5, 400, 401,...</td>\n",
              "      <td>[20, 4, 5, 7, 4, 5, 16, 10, 4, 71, 10, 4, 166,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2641770481_c98465ff35.jpg</td>\n",
              "      <td>[[28, 33, 98, 4, 5, 46, 106, 21, 4, 10, 4, 44,...</td>\n",
              "      <td>[28, 33, 122, 4, 61, 102, 15, 4, 14, 4, 5, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3485425825_c2f3446e73.jpg</td>\n",
              "      <td>[[28, 15, 4, 1903, 353, 190, 4, 5, 21, 4, 29, ...</td>\n",
              "      <td>[112, 4, 5, 16, 10, 4, 190, 4, 5, 10, 4, 278, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1554713437_61b64527dd.jpg</td>\n",
              "      <td>[[12, 12, 53, 356, 18, 992, 4, 5, 16, 10, 4, 3...</td>\n",
              "      <td>[12, 65, 41, 18, 10, 4, 42]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name  ...                                         hypothesis\n",
              "0   354642192_3b7666a2dd.jpg  ...                    [10, 4, 6, 4, 5, 16, 10, 4, 66]\n",
              "1  1237985362_dbafc59280.jpg  ...  [20, 4, 5, 7, 4, 5, 16, 10, 4, 71, 10, 4, 166,...\n",
              "2  2641770481_c98465ff35.jpg  ...  [28, 33, 122, 4, 61, 102, 15, 4, 14, 4, 5, 10,...\n",
              "3  3485425825_c2f3446e73.jpg  ...  [112, 4, 5, 16, 10, 4, 190, 4, 5, 10, 4, 278, ...\n",
              "4  1554713437_61b64527dd.jpg  ...                        [12, 65, 41, 18, 10, 4, 42]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1moZvKfFLuKR",
        "outputId": "c3b87d95-28f6-446e-a68f-df3397d354f2"
      },
      "source": [
        "df.sort_values(\"file_name\").head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>references</th>\n",
              "      <th>hypothesis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2392</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[112, 4, 5, 151, 16, 122, 109, 664, 689, 4, 5...</td>\n",
              "      <td>[20, 4, 5, 7, 4, 5, 16, 10, 4, 71]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1289</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[112, 4, 5, 151, 16, 122, 109, 664, 689, 4, 5...</td>\n",
              "      <td>[20, 4, 5, 7, 4, 5, 16, 10, 4, 71]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[112, 4, 5, 151, 16, 122, 109, 664, 689, 4, 5...</td>\n",
              "      <td>[20, 4, 5, 7, 4, 5, 16, 10, 4, 71]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>[[49, 16, 236, 10, 4, 237, 4, 5, 10, 4, 93, 18...</td>\n",
              "      <td>[49, 7, 33, 122, 4, 61, 109, 15, 4, 14, 4, 61]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2160</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>[[49, 16, 236, 10, 4, 237, 4, 5, 10, 4, 93, 18...</td>\n",
              "      <td>[49, 7, 33, 122, 4, 61, 109, 15, 4, 14, 4, 61]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      file_name  ...                                      hypothesis\n",
              "2392  1056338697_4f7d7ce270.jpg  ...              [20, 4, 5, 7, 4, 5, 16, 10, 4, 71]\n",
              "1289  1056338697_4f7d7ce270.jpg  ...              [20, 4, 5, 7, 4, 5, 16, 10, 4, 71]\n",
              "439   1056338697_4f7d7ce270.jpg  ...              [20, 4, 5, 7, 4, 5, 16, 10, 4, 71]\n",
              "43     106490881_5a2dd9b7bd.jpg  ...  [49, 7, 33, 122, 4, 61, 109, 15, 4, 14, 4, 61]\n",
              "2160   106490881_5a2dd9b7bd.jpg  ...  [49, 7, 33, 122, 4, 61, 109, 15, 4, 14, 4, 61]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwe6gI2gLql2"
      },
      "source": [
        "df.to_json(\"arabic_bert_results.json\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmNgyE7xMKw6",
        "outputId": "9f8ce382-b49b-42e3-e012-a51a920c87a5"
      },
      "source": [
        "refes = []\n",
        "hypos = []\n",
        "for fname in tqdm(df.file_name.unique()):\n",
        "  refes.append(df[df.file_name==fname].references.to_list()[0])\n",
        "  hypos.append(df[df.file_name==fname].hypothesis.to_list()[0])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/841 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 70/841 [00:00<00:01, 695.43it/s]\u001b[A\n",
            " 16%|█▋        | 137/841 [00:00<00:01, 686.52it/s]\u001b[A\n",
            " 25%|██▍       | 209/841 [00:00<00:00, 694.42it/s]\u001b[A\n",
            " 33%|███▎      | 278/841 [00:00<00:00, 692.15it/s]\u001b[A\n",
            " 41%|████▏     | 349/841 [00:00<00:00, 695.80it/s]\u001b[A\n",
            " 50%|█████     | 421/841 [00:00<00:00, 700.29it/s]\u001b[A\n",
            " 59%|█████▉    | 495/841 [00:00<00:00, 711.12it/s]\u001b[A\n",
            " 67%|██████▋   | 563/841 [00:00<00:00, 699.29it/s]\u001b[A\n",
            " 75%|███████▌  | 634/841 [00:00<00:00, 701.53it/s]\u001b[A\n",
            " 83%|████████▎ | 702/841 [00:01<00:00, 674.74it/s]\u001b[A\n",
            " 91%|█████████▏| 768/841 [00:01<00:00, 657.36it/s]\u001b[A\n",
            "100%|██████████| 841/841 [00:01<00:00, 678.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHDC6QxoMVyi",
        "outputId": "cb1750ea-136d-4850-bea1-65665707a429"
      },
      "source": [
        "len(refes), len(hypos)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(841, 841)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45dgYUZNMYDo",
        "outputId": "cda88183-5f54-4a01-b9da-622d5dd0cf35"
      },
      "source": [
        "print_scores(refes, hypos)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 59.27134312126155\n",
            "2: 45.52397958654338\n",
            "3: 33.58850576504064\n",
            "4: 24.918812277227662\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59.27134312126155, 45.52397958654338, 33.58850576504064, 24.918812277227662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}