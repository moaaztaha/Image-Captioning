{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccab5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from train import train, validate\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# training parameters\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "print_freq = 100  # print training/validation stats every __ batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f5b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(t_params, checkpoint=None, m_params=None):\n",
    "\n",
    "    # info\n",
    "    data_name = t_params['data_name']\n",
    "    imgs_path = t_params['imgs_path']\n",
    "    df_path = t_params['df_path']\n",
    "    vocab = t_params['vocab']\n",
    "\n",
    "    start_epoch = 0\n",
    "    epochs_since_improvement = 0\n",
    "    best_bleu4 = 0\n",
    "    epochs = t_params['epochs']\n",
    "    batch_size = t_params['batch_size']\n",
    "    workers = t_params['workers']\n",
    "    encoder_lr = t_params['encoder_lr']\n",
    "    decoder_lr = t_params['decoder_lr']\n",
    "    fine_tune_encoder = t_params['fine_tune_encoder']\n",
    "    \n",
    "    pretrained_embeddings = t_params['pretrained_embeddings']\n",
    "    fine_tune_embeddings = t_params['fine_tune_embeddings']\n",
    "    embeddings_matrix = t_params['embeddings_matrix']\n",
    "\n",
    "    # init / load checkpoint\n",
    "    if checkpoint is None:\n",
    "\n",
    "        # getting hyperparameters\n",
    "        attention_dim = m_params['attention_dim']\n",
    "        embed_dim = m_params['embed_dim']\n",
    "        decoder_dim = m_params['decoder_dim']\n",
    "        encoder_dim = m_params['encoder_dim']\n",
    "        dropout = m_params['dropout']\n",
    "\n",
    "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                      embed_dim=embed_dim,\n",
    "                                      decoder_dim=decoder_dim,\n",
    "                                      encoder_dim=encoder_dim,\n",
    "                                      vocab_size=len(vocab),\n",
    "                                      dropout=dropout)\n",
    "        if pretrained_embeddings:\n",
    "            decoder.load_pretrained_embeddings(embeddings_matrix)\n",
    "            decoder.fine_tune_embeddings(fine_tune=fine_tune_embeddings)\n",
    "        \n",
    "        decoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, decoder.parameters()),\n",
    "                                            lr=decoder_lr)\n",
    "        \n",
    "        encoder=Encoder()\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, encoder.parameters()),\n",
    "                                            lr=encoder_lr) if fine_tune_encoder else None\n",
    "    # load checkpoint\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        print('Loaded Checkpoint!!')\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Starting Epoch: {start_epoch}\")\n",
    "        epochs_since_improvement = checkpoint['epochs_since_imrovment']\n",
    "        best_bleu4 = checkpoint['bleu-4']\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder_optimizer = checkpoint['deocder_optimizer']\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "            encoder.fine_tune(fine_tune_encoder)\n",
    "            encoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, encoder.parameters()),\n",
    "                                                lr=encoder_lr)\n",
    "    # move to gpu, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    print('Loading Data')\n",
    "    train_loader, val_loader = get_loaders(batch_size, imgs_path, df_path, transform, vocab, False ,workers)\n",
    "    print('_'*50)\n",
    "\n",
    "    print('-'*20, 'Fitting', '-'*20)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        # decay lr is there is no improvement for 8 consecutive epochs and terminate after 20\n",
    "        if epochs_since_improvement == 20:\n",
    "            print('No improvement for 20 consecutive epochs, terminating...')\n",
    "            break\n",
    "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "            if fine_tune_encoder:\n",
    "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "        \n",
    "        print('_'*50)\n",
    "        print('-'*20, 'Training', '-'*20)\n",
    "        # one epoch of training\n",
    "        train(train_loader=train_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion,\n",
    "            encoder_optimizer=encoder_optimizer,\n",
    "            decoder_optimizer=decoder_optimizer,\n",
    "            epoch=epoch)\n",
    "        \n",
    "        # one epoch of validation\n",
    "        print('-'*20, 'Validation', '-'*20)\n",
    "        recent_bleu4 = validate(val_loader=val_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion,\n",
    "            vocab=vocab)\n",
    "\n",
    "        \n",
    "        # check for improvement\n",
    "        is_best = recent_bleu4 > best_bleu4\n",
    "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(f'\\nEpochs since last improvement: {epochs_since_improvement,}')\n",
    "        else:\n",
    "            # reset\n",
    "            epochs_since_improvement = 0\n",
    "        \n",
    "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "            decoder_optimizer, recent_bleu4, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc53d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 7  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 256\n",
    "workers = 2\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder\n",
    "pretrained_embeddings = True\n",
    "fine_tune_embeddings = False\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162d6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 356660.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5089"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = build_vocab(DATA_JSON_PATH); len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29458662",
   "metadata": {},
   "source": [
    "### Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82290e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1d9d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop',\n",
       " 'imgs_path': 'flickr/Images/',\n",
       " 'df_path': 'data.json',\n",
       " 'vocab': <dataset.Vocabulary at 0x7f581a6bd4d0>,\n",
       " 'epochs': 7,\n",
       " 'batch_size': 256,\n",
       " 'workers': 2,\n",
       " 'decoder_lr': 0.0004,\n",
       " 'encoder_lr': 0.0001,\n",
       " 'fine_tune_encoder': False,\n",
       " 'pretrained_embeddings': True,\n",
       " 'fine_tune_embeddings': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_params = {\n",
    "    'data_name': DATA_NAME,\n",
    "    'imgs_path': IMGS_PATH,\n",
    "    'df_path': DATA_JSON_PATH,\n",
    "    'vocab': vocab,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'workers': workers,\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder,\n",
    "    'pretrained_embeddings': pretrained_embeddings,\n",
    "    'fine_tune_embeddings': fine_tune_embeddings,\n",
    "}\n",
    "\n",
    "m_params = {\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim,\n",
    "    'dropout': dropout.\n",
    "    'embedding_matrix': \n",
    "}\n",
    "\n",
    "t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87543910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
