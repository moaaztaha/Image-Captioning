{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa47541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce62dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from train import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os import path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc614e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 300  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 1  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 64\n",
    "workers = 2\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder\n",
    "pretrained_embeddings = True\n",
    "fine_tune_embeddings = False\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309ae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc1498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 388031.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5089"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = build_vocab(DATA_JSON_PATH); len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54451598",
   "metadata": {},
   "source": [
    "### Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b107aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv('glove.6B/glove.6B.300d.txt', sep=' ', quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5a262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, embedding_dict, dimension):\n",
    "    embedding_matrix = np.zeros((len(vocab), dimension))\n",
    "    \n",
    "    for word, index in vocab.stoi.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "        else: \n",
    "            embedding_matrix[index] = np.random.uniform(-.1, .1, size=dimension)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(vocab, glove_embedding, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b53765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5089, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08de5875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('embeddings.pkl','wb') as f: pickle.dump(embedding_matrix, f)\n",
    "\n",
    "with open('embeddings.pkl','rb') as f: embedding_matrix1 = pickle.load(f)\n",
    "\n",
    "np.array_equal(embedding_matrix,embedding_matrix1) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b56834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop',\n",
       " 'imgs_path': 'flickr/Images/',\n",
       " 'df_path': 'data.json',\n",
       " 'vocab': <dataset.Vocabulary at 0x7f3d6dbd0350>,\n",
       " 'epochs': 1,\n",
       " 'batch_size': 64,\n",
       " 'workers': 2,\n",
       " 'decoder_lr': 0.0004,\n",
       " 'encoder_lr': 0.0001,\n",
       " 'fine_tune_encoder': False,\n",
       " 'pretrained_embeddings': True,\n",
       " 'fine_tune_embeddings': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_params = {\n",
    "    'data_name': DATA_NAME,\n",
    "    'imgs_path': IMGS_PATH,\n",
    "    'df_path': DATA_JSON_PATH,\n",
    "    'vocab': vocab,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'workers': workers,\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder,\n",
    "    'pretrained_embeddings': pretrained_embeddings,\n",
    "    'fine_tune_embeddings': fine_tune_embeddings,\n",
    "}\n",
    "\n",
    "m_params = {\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim,\n",
    "    'dropout': dropout,\n",
    "    'embeddings_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c82997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Dataset split: train\n",
      "Unique images: 6000\n",
      "Total size: 30000\n",
      "Dataset split: val\n",
      "Unique images: 1000\n",
      "Total size: 5000\n",
      "__________________________________________________\n",
      "-------------------- Fitting --------------------\n",
      "__________________________________________________\n",
      "-------------------- Training --------------------\n",
      "Epoch: [0][0/469]\tBatch Time 6.728 (6.728)\tData Load Time 3.440 (3.440)\tLoss 9.4371 (9.4371)\tTop-5 Accuracy 0.000 (0.000)\n",
      "Epoch: [0][100/469]\tBatch Time 1.121 (1.090)\tData Load Time 0.000 (0.034)\tLoss 5.1338 (5.7062)\tTop-5 Accuracy 49.603 (43.075)\n",
      "Epoch: [0][200/469]\tBatch Time 1.105 (1.083)\tData Load Time 0.000 (0.017)\tLoss 5.0455 (5.3644)\tTop-5 Accuracy 50.611 (47.638)\n",
      "Epoch: [0][300/469]\tBatch Time 1.086 (1.072)\tData Load Time 0.000 (0.012)\tLoss 4.6390 (5.1817)\tTop-5 Accuracy 56.369 (50.053)\n",
      "Epoch: [0][400/469]\tBatch Time 1.001 (1.064)\tData Load Time 0.000 (0.009)\tLoss 4.8202 (5.0634)\tTop-5 Accuracy 56.081 (51.656)\n",
      "-------------------- Validation --------------------\n",
      "Validation: [0/79]\tBatch Time 2.916 (2.916)\tLoss 5.5565 (5.5565)\tTop-5 Accuracy 50.958 (50.958)\t\n",
      "----- Bleu-n Scores -----\n",
      "1: 63.52849726683869\n",
      "2: 38.93060987384824\n",
      "3: 22.463913491080632\n",
      "4: 12.840941076866258\n",
      "-------------------------\n",
      "\n",
      " * LOSS - 5.247, TOP-5 ACCURACY - 54.947, BLEU-4 - 0.12980155038238195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(t_params=t_params, m_params=m_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
