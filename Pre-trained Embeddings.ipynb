{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa47541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce62dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from train import train, validate\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# training parameters\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "print_freq = 100  # print training/validation stats every __ batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f16606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(t_params, checkpoint=None, m_params=None):\n",
    "\n",
    "    # info\n",
    "    data_name = t_params['data_name']\n",
    "    imgs_path = t_params['imgs_path']\n",
    "    df_path = t_params['df_path']\n",
    "    vocab = t_params['vocab']\n",
    "\n",
    "    start_epoch = 0\n",
    "    epochs_since_improvement = 0\n",
    "    best_bleu4 = 0\n",
    "    epochs = t_params['epochs']\n",
    "    batch_size = t_params['batch_size']\n",
    "    workers = t_params['workers']\n",
    "    encoder_lr = t_params['encoder_lr']\n",
    "    decoder_lr = t_params['decoder_lr']\n",
    "    fine_tune_encoder = t_params['fine_tune_encoder']\n",
    "    \n",
    "    pretrained_embeddings = t_params['pretrained_embeddings']\n",
    "    fine_tune_embeddings = t_params['fine_tune_embeddings']\n",
    "    embeddings_matrix = m_params['embeddings_matrix']\n",
    "\n",
    "    # init / load checkpoint\n",
    "    if checkpoint is None:\n",
    "\n",
    "        # getting hyperparameters\n",
    "        attention_dim = m_params['attention_dim']\n",
    "        embed_dim = m_params['embed_dim']\n",
    "        decoder_dim = m_params['decoder_dim']\n",
    "        encoder_dim = m_params['encoder_dim']\n",
    "        dropout = m_params['dropout']\n",
    "\n",
    "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                      embed_dim=embed_dim,\n",
    "                                      decoder_dim=decoder_dim,\n",
    "                                      encoder_dim=encoder_dim,\n",
    "                                      vocab_size=len(vocab),\n",
    "                                      dropout=dropout)\n",
    "        if pretrained_embeddings:\n",
    "            decoder.load_pretrained_embeddings(torch.tensor(embeddings_matrix, dtype=torch.float32))\n",
    "            decoder.fine_tune_embeddings(fine_tune=fine_tune_embeddings)\n",
    "        \n",
    "        decoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, decoder.parameters()),\n",
    "                                            lr=decoder_lr)\n",
    "        \n",
    "        encoder=Encoder()\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, encoder.parameters()),\n",
    "                                            lr=encoder_lr) if fine_tune_encoder else None\n",
    "    # load checkpoint\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        print('Loaded Checkpoint!!')\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Starting Epoch: {start_epoch}\")\n",
    "        epochs_since_improvement = checkpoint['epochs_since_imrovment']\n",
    "        best_bleu4 = checkpoint['bleu-4']\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder_optimizer = checkpoint['deocder_optimizer']\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "            encoder.fine_tune(fine_tune_encoder)\n",
    "            encoder_optimizer = torch.optim.RMSprop(params=filter(lambda p:p.requires_grad, encoder.parameters()),\n",
    "                                                lr=encoder_lr)\n",
    "    # move to gpu, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    print('Loading Data')\n",
    "    train_loader, val_loader = get_loaders(batch_size, imgs_path, df_path, transform, vocab, False ,workers)\n",
    "    print('_'*50)\n",
    "\n",
    "    print('-'*20, 'Fitting', '-'*20)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        # decay lr is there is no improvement for 8 consecutive epochs and terminate after 20\n",
    "        if epochs_since_improvement == 20:\n",
    "            print('No improvement for 20 consecutive epochs, terminating...')\n",
    "            break\n",
    "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "            if fine_tune_encoder:\n",
    "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "        \n",
    "        print('_'*50)\n",
    "        print('-'*20, 'Training', '-'*20)\n",
    "        # one epoch of training\n",
    "        train(train_loader=train_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion,\n",
    "            encoder_optimizer=encoder_optimizer,\n",
    "            decoder_optimizer=decoder_optimizer,\n",
    "            epoch=epoch)\n",
    "        \n",
    "        # one epoch of validation\n",
    "        print('-'*20, 'Validation', '-'*20)\n",
    "        recent_bleu4 = validate(val_loader=val_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion,\n",
    "            vocab=vocab)\n",
    "\n",
    "        \n",
    "        # check for improvement\n",
    "        is_best = recent_bleu4 > best_bleu4\n",
    "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(f'\\nEpochs since last improvement: {epochs_since_improvement,}')\n",
    "        else:\n",
    "            # reset\n",
    "            epochs_since_improvement = 0\n",
    "        \n",
    "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "            decoder_optimizer, recent_bleu4, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc614e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 300  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 1  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 64\n",
    "workers = 2\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder\n",
    "pretrained_embeddings = True\n",
    "fine_tune_embeddings = False\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309ae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc1498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 388031.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5089"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = build_vocab(DATA_JSON_PATH); len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54451598",
   "metadata": {},
   "source": [
    "### Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b107aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv('glove.6B/glove.6B.300d.txt', sep=' ', quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5a262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, embedding_dict, dimension):\n",
    "    embedding_matrix = np.zeros((len(vocab), dimension))\n",
    "    \n",
    "    for word, index in vocab.stoi.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "        else: \n",
    "            embedding_matrix[index] = np.random.uniform(-.1, .1, size=dimension)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(vocab, glove_embedding, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b53765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5089, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08de5875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('embeddings.pkl','wb') as f: pickle.dump(embedding_matrix, f)\n",
    "\n",
    "with open('embeddings.pkl','rb') as f: embedding_matrix1 = pickle.load(f)\n",
    "\n",
    "np.array_equal(embedding_matrix,embedding_matrix1) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b56834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop',\n",
       " 'imgs_path': 'flickr/Images/',\n",
       " 'df_path': 'data.json',\n",
       " 'vocab': <dataset.Vocabulary at 0x7f3d6dbd0350>,\n",
       " 'epochs': 1,\n",
       " 'batch_size': 64,\n",
       " 'workers': 2,\n",
       " 'decoder_lr': 0.0004,\n",
       " 'encoder_lr': 0.0001,\n",
       " 'fine_tune_encoder': False,\n",
       " 'pretrained_embeddings': True,\n",
       " 'fine_tune_embeddings': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_params = {\n",
    "    'data_name': DATA_NAME,\n",
    "    'imgs_path': IMGS_PATH,\n",
    "    'df_path': DATA_JSON_PATH,\n",
    "    'vocab': vocab,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'workers': workers,\n",
    "    'decoder_lr': decoder_lr,\n",
    "    'encoder_lr': encoder_lr,\n",
    "    'fine_tune_encoder': fine_tune_encoder,\n",
    "    'pretrained_embeddings': pretrained_embeddings,\n",
    "    'fine_tune_embeddings': fine_tune_embeddings,\n",
    "}\n",
    "\n",
    "m_params = {\n",
    "    'attention_dim': attention_dim,\n",
    "    'embed_dim': emb_dim,\n",
    "    'decoder_dim': decoder_dim,\n",
    "    'encoder_dim': encoder_dim,\n",
    "    'dropout': dropout,\n",
    "    'embeddings_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c82997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Dataset split: train\n",
      "Unique images: 6000\n",
      "Total size: 30000\n",
      "Dataset split: val\n",
      "Unique images: 1000\n",
      "Total size: 5000\n",
      "__________________________________________________\n",
      "-------------------- Fitting --------------------\n",
      "__________________________________________________\n",
      "-------------------- Training --------------------\n",
      "Epoch: [0][0/469]\tBatch Time 6.728 (6.728)\tData Load Time 3.440 (3.440)\tLoss 9.4371 (9.4371)\tTop-5 Accuracy 0.000 (0.000)\n",
      "Epoch: [0][100/469]\tBatch Time 1.121 (1.090)\tData Load Time 0.000 (0.034)\tLoss 5.1338 (5.7062)\tTop-5 Accuracy 49.603 (43.075)\n",
      "Epoch: [0][200/469]\tBatch Time 1.105 (1.083)\tData Load Time 0.000 (0.017)\tLoss 5.0455 (5.3644)\tTop-5 Accuracy 50.611 (47.638)\n",
      "Epoch: [0][300/469]\tBatch Time 1.086 (1.072)\tData Load Time 0.000 (0.012)\tLoss 4.6390 (5.1817)\tTop-5 Accuracy 56.369 (50.053)\n",
      "Epoch: [0][400/469]\tBatch Time 1.001 (1.064)\tData Load Time 0.000 (0.009)\tLoss 4.8202 (5.0634)\tTop-5 Accuracy 56.081 (51.656)\n",
      "-------------------- Validation --------------------\n",
      "Validation: [0/79]\tBatch Time 2.916 (2.916)\tLoss 5.5565 (5.5565)\tTop-5 Accuracy 50.958 (50.958)\t\n",
      "----- Bleu-n Scores -----\n",
      "1: 63.52849726683869\n",
      "2: 38.93060987384824\n",
      "3: 22.463913491080632\n",
      "4: 12.840941076866258\n",
      "-------------------------\n",
      "\n",
      " * LOSS - 5.247, TOP-5 ACCURACY - 54.947, BLEU-4 - 0.12980155038238195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(t_params=t_params, m_params=m_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
