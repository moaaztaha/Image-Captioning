{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v18vuSVO97ze",
        "outputId": "ebd447f7-e64f-4f2f-8994-b6ec08522f7d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul  2 08:13:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKxrApT2-E71",
        "outputId": "d2806b85-ff7e-43ae-8dc7-b68d90d9ee8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV32uxub-ewx"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqMvvx6P-jd7",
        "outputId": "180aef9a-2c82-413e-d560-87b729f1cfa2"
      },
      "source": [
        "!pip install kaggle -q\n",
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions\n",
        "!unzip -q flickr8kimagescaptions.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            " 99% 1.02G/1.04G [00:23<00:00, 42.7MB/s]\n",
            "100% 1.04G/1.04G [00:24<00:00, 46.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvFgu78d-lC9",
        "outputId": "2f253136-fce5-44cf-991b-04d4f91167c1"
      },
      "source": [
        "# get the code form github\n",
        "!git clone https://github.com/moaaztaha/Image-Captioning\n",
        "py_files_path = 'Image-Captioning/'\n",
        "import sys\n",
        "sys.path.append(py_files_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning'...\n",
            "remote: Enumerating objects: 617, done.\u001b[K\n",
            "remote: Counting objects: 100% (617/617), done.\u001b[K\n",
            "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
            "remote: Total 617 (delta 370), reused 555 (delta 308), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (617/617), 38.54 MiB | 4.65 MiB/s, done.\n",
            "Resolving deltas: 100% (370/370), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ4lhGja9h7N"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpDx8Ad9h7N"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from utils import load_checkpoint\n",
        "from dataset import build_vocab, get_loaders\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from utils import print_scores\n",
        "import pandas as pd"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gPxE2X69h7O"
      },
      "source": [
        "# DATA_NAME = 'flickr8k_ar'\n",
        "\n",
        "# local\n",
        "# DATA_JSON_PATH = 'ar_data.json'\n",
        "# IMGS_PATH = 'flickr/Images/'\n",
        "# kaggle paths\n",
        "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
        "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
        "#colab\n",
        "DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
        "IMGS_PATH = 'flickr8k/images/'"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5g3rBl39h7O"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVqmuxp8_Ghy"
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/MyDrive/ImageCaptioning/flickr8/BEST_checkpoint_flickr8k_finetune.pth.tar'"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "riR8ECa99h7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452fff81-42b0-4267-e45e-c550c127cbb2"
      },
      "source": [
        "# Load model\n",
        "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval();"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 12\n",
            "Best Bleu-4: 15.97917426288958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ54asZj9h7Q"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btBk9a7v9h7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2bda47-b8b4-4bad-99ac-7a2772773adc"
      },
      "source": [
        "vocab = build_vocab(DATA_JSON_PATH)\n",
        "len(vocab)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 216330.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er36e9Xd9h7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccadadb-838c-4376-d3e3-7d235ce659d8"
      },
      "source": [
        "bs = 1\n",
        "beam_size=3\n",
        "loader = get_loaders(bs, IMGS_PATH, DATA_JSON_PATH, transform, vocab, test=True, n_workers=8)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDwVRMkS9h7R"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "# import skimage.transform\n",
        "# import argparse\n",
        "from PIL import Image\n",
        "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "\n",
        "    # Read image and process\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    image = transform(img).to(device)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[word_map.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "        \n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "#         print(seqs[prev_word_inds], prev_word_inds)\n",
        "#         if step == 5:\n",
        "#             return seqs\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != word_map.stoi['<eos>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    return seq, alphas, complete_seqs"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kUoFkU9h7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6275f4d0-9cd8-4f01-c914-7c98cb4989e6"
      },
      "source": [
        "seq, _, comp_seqs = caption_image_beam_search(encoder, decoder, 'flickr/Images/3514019869_7de4ece2a5.jpg', vocab, beam_size=2)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-0567a4699776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption_image_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flickr/Images/3514019869_7de4ece2a5.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-580de353acc1>\u001b[0m in \u001b[0;36mcaption_image_beam_search\u001b[0;34m(encoder, decoder, image_path, word_map, beam_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Read image and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     transform = transforms.Compose([\n\u001b[1;32m     19\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flickr/Images/3514019869_7de4ece2a5.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorBFmY-9h7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "99759469-4047-4372-dd1b-b204ce859c8d"
      },
      "source": [
        "[sent for sent in comp_seqs][0]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-a1455f45f60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'comp_seqs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N7pGppNz9h7Z",
        "outputId": "0f521bf3-d13b-4d8e-b782-a1292b734995"
      },
      "source": [
        "[\" \".join([vocab.itos[i] for i in sent]) for sent in comp_seqs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> a dog runs through an obstacle <eos>',\n",
              " '<sos> a dog jumps over a hurdle <eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScYKDOWa9h7Z",
        "outputId": "c95b8012-12df-45f7-b165-376afb09f247"
      },
      "source": [
        "[vocab.itos[i] for i in seq]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>', 'a', 'dog', 'runs', 'through', 'an', 'obstacle', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LdecEyO9h7a"
      },
      "source": [
        "def evaluate(beam_size):\n",
        "\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
        "        \n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "        \n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
        "          \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "            \n",
        "#             print(top_k_scores, top_k_words)\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab.stoi['<eos>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        if len(complete_seqs_scores) == 0:\n",
        "            continue\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "#     bleu4 = corpus_bleu(references, hypotheses)\n",
        "    return references, hypotheses\n",
        "    print_scores(references, hypotheses, nltk=True)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BACVNjSR9h7c"
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjTiKGgE9h7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a447d8-29cd-44f4-d52c-c461adadc520"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAq8Tncv9h7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0664d216-ab9f-4057-986c-4e0bc4c59b9d"
      },
      "source": [
        "references, hypotheses = evaluate(3)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [06:24<00:00, 14.29it/s]\n",
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [06:25<00:00, 12.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mPHr3WRcR3g",
        "outputId": "9d6465cf-e16a-452a-bfb3-42acd69182e1"
      },
      "source": [
        "len(references), len(hypotheses)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4995, 4995)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GD5Hrki9h7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a316b0f-dedd-4ca7-9cfc-51c7953b210f"
      },
      "source": [
        "print_scores(references, hypotheses)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.08114558472555\n",
            "2: 46.50725094600124\n",
            "3: 32.63582641164054\n",
            "4: 22.48417748427286\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64.08114558472555, 46.50725094600124, 32.63582641164054, 22.48417748427286)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IQtARUd9h7e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsp-b6GU9h7f"
      },
      "source": [
        "hs = [\" \".join(word for word in sent) for sent in vocab.indextostring(hypotheses)]\n",
        "rs = []\n",
        "for r in references:\n",
        "    rs.append([\" \".join(word for word in sent) for sent in vocab.indextostring(r)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoYzEDD19h7f",
        "outputId": "a0e96b41-1fda-44da-b610-f04ab848844b"
      },
      "source": [
        "hs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a black and white dog is running through the grass'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-r0_OHI9h7f",
        "outputId": "fd76c552-4c65-4c58-e283-01aad1fa0889"
      },
      "source": [
        "rs[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a boy pushes a wagon full of pumpkins',\n",
              " 'a boy pushes a wagon with two pumpkins',\n",
              " 'a boy smiling leaning over a wagon filled with two large pumpkins',\n",
              " 'a child squats behind a wagon with two pumpkins in it',\n",
              " 'boy pushing wagon with two pumpkins in it']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rG48jpR9h7g",
        "outputId": "2d962224-7c38-49c2-a132-6d960e88a4a9"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "total_meteor = 0\n",
        "\n",
        "for r, h in tqdm(zip(rs, hs), total=len(rs)):\n",
        "    total_meteor += meteor_score(r, h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:10<00:00, 460.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvvK5b3p9h7g",
        "outputId": "08e77b61-6309-41ca-fb65-07feab06abab"
      },
      "source": [
        "total_meteor/len(rs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.428133409401112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ixYpe89h7g"
      },
      "source": [
        "### turn outputs into strings -> bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcSyF5Tu9h7g",
        "outputId": "788f2594-434c-4d64-85c7-390e7979c625"
      },
      "source": [
        "print_scores(rs, hs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.62017684887459\n",
            "2: 47.13660963689657\n",
            "3: 33.759143347900405\n",
            "4: 23.833249590210322\n",
            "-------------------------\n",
            "----- METEOR Score -----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'itos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-f9e9a3469961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36mprint_scores\u001b[0;34m(trgs, preds, vocab)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'itos'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk-dM2zK9h7h"
      },
      "source": [
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tmh004i9h7h",
        "outputId": "ccef767c-64cd-46c5-98dc-fad5ab045992"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/kelwa/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-TQvupc9h7h"
      },
      "source": [
        "hs = [\" \".join([vocab.itos[i] for i in sent[0]]) for sent in hypotheses]\n",
        "rs = []\n",
        "for r in references:\n",
        "    rs.append([\" \".join([vocab.itos[i] for i in sent]) for sent in r])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbI4PEOe9h7h"
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5dojgWd9h7i",
        "outputId": "dd269696-c47a-44e2-e296-67d36857956e"
      },
      "source": [
        "evaluate(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 2: 100%|██████████| 5000/5000 [03:27<00:00, 24.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 63.54625550660793\n",
            "2: 45.03356444855022\n",
            "3: 31.69343570783961\n",
            "4: 22.30082901251822\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUmIumHR9h7i",
        "outputId": "9f5826a4-b645-415b-f028-cd4b7c7ab8e9"
      },
      "source": [
        "evaluate(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:57<00:00, 21.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.31220201306728\n",
            "2: 45.5251368754951\n",
            "3: 32.38723129370634\n",
            "4: 23.07429032664538\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SLb0Ne9h7i",
        "outputId": "3a2eed83-ce17-4fb7-f05d-49a5af28c894"
      },
      "source": [
        "evaluate(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [02:59<00:00, 27.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 61.123197163806296\n",
            "2: 42.936593246185\n",
            "3: 29.775037258795304\n",
            "4: 20.646167109205283\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmgfXpL9h7j",
        "outputId": "db87f206-8700-483f-aa19-d5957a8f5ee2"
      },
      "source": [
        "for i in range(1, 6):\n",
        "    print('*'*15, f\"Beam size of {i}\", '*'*15)\n",
        "    evaluate(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*************** Beam size of 1 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [02:59<00:00, 27.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 61.123197163806296\n",
            "2: 42.936593246185\n",
            "3: 29.775037258795304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 2:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 20.646167109205283\n",
            "-------------------------\n",
            "*************** Beam size of 2 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 2: 100%|██████████| 5000/5000 [03:58<00:00, 20.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 63.54625550660793\n",
            "2: 45.03356444855022\n",
            "3: 31.69343570783961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 22.30082901251822\n",
            "-------------------------\n",
            "*************** Beam size of 3 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [04:43<00:00, 17.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.31220201306728\n",
            "2: 45.5251368754951\n",
            "3: 32.38723129370634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 4:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 23.07429032664538\n",
            "-------------------------\n",
            "*************** Beam size of 4 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 4: 100%|██████████| 5000/5000 [04:58<00:00, 16.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.40847503864691\n",
            "2: 45.8716912526639\n",
            "3: 32.61201588518963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 5:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 23.235058922423306\n",
            "-------------------------\n",
            "*************** Beam size of 5 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 5: 100%|██████████| 5000/5000 [05:38<00:00, 14.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.8227213662521\n",
            "2: 46.212067416932584\n",
            "3: 32.929193446645684\n",
            "4: 23.41989863202648\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_rpjq-_Zbi"
      },
      "source": [
        "### End-to-End Arabic VS Translated English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX9sq2mK_XTr"
      },
      "source": [
        "# getting the English captions \n",
        "def evaluate(beam_size):\n",
        "\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "    img_ids = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n",
        "        tqdm(test_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
        "        \n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "        \n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
        "          \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "            \n",
        "#             print(top_k_scores, top_k_words)\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab.stoi['<eos>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        if len(complete_seqs_scores) == 0:\n",
        "            continue\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
        "\n",
        "        img_ids.append(img_id[0])\n",
        "\n",
        "        # print(img_ids)\n",
        "        # break\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "#     bleu4 = corpus_bleu(references, hypotheses)\n",
        "    return references, hypotheses, img_ids\n",
        "    print_scores(references, hypotheses, nltk=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxdAzw7GARGU"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataset import CaptionDataset"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2kYxJzYkcVD"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
        "import torchvision.transforms as transfroms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    \"\"\" \n",
        "    Caption Dataset Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, imgs_dir, captions_file, vocab, transforms=None, split='train'):\n",
        "        \"\"\"\n",
        "        :param imgs_dir: folder where images are stored\n",
        "        :param captions_file: the df file with all caption information\n",
        "        :param vocab: vocabuary object\n",
        "        :param transforms: image transforms pipeline\n",
        "        :param split: data split\n",
        "        \"\"\"\n",
        "\n",
        "        # split has to be one of {'train', 'val', 'test'}\n",
        "        assert split in {'train', 'val', 'test'}\n",
        "\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.df = pd.read_json(captions_file)\n",
        "        self.df = self.df[self.df['split'] == split]\n",
        "        self.vocab = vocab\n",
        "        self.transforms = transforms\n",
        "        self.split = split\n",
        "\n",
        "        self.dataset_size = self.df.shape[0]\n",
        "        # printing some info\n",
        "        print(f\"Dataset split: {split}\")\n",
        "        print(f\"Unique images: {self.df.file_name.nunique()}\")\n",
        "        print(f\"Total size: {self.dataset_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # loading the image\n",
        "        img_id = self.df['file_name'].values[index]\n",
        "        img = Image.open(self.imgs_dir+img_id).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = transfroms.ToTensor()(img)\n",
        "\n",
        "        # loading current caption\n",
        "        cap_len = self.df['tok_len'].values[index] + 2 # <sos> and <eos>\n",
        "        tokens = self.df['tokens'].values[index]\n",
        "        caption = torch.LongTensor(self.vocab.numericalize(tokens, cap_len))\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, cap_len\n",
        "        else:\n",
        "            # for val and test return all captions for calculate the bleu scores\n",
        "            captions_tokens = self.df[self.df['file_name'] == img_id].tokens.values\n",
        "            captions_lens = self.df[self.df['file_name'] == img_id].tok_len.values\n",
        "            all_tokens = []\n",
        "            for token, cap_len in zip(captions_tokens, captions_lens):\n",
        "                all_tokens.append(self.vocab.numericalize(token, cap_len)[1:]) # remove <sos>\n",
        "\n",
        "            return img, caption, cap_len, torch.tensor(all_tokens), img_id"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJeuRP1N_XzU",
        "outputId": "19371184-f101-4d52-fa14-9a75578cbaa4"
      },
      "source": [
        "# getting the test data loader with shuffle=False\n",
        "bs = 1\n",
        "vocab_size = len(vocab)\n",
        "DATA_JSON_PATH = 'Image-Captioning/ar_data.json'\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
        "                            transforms=transform, vocab=vocab, split='test'),\n",
        "    batch_size=bs,\n",
        "    num_workers=2,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "Cg-THzTGAQNz",
        "outputId": "46b20841-ea16-4467-d0df-26e9a09b406d"
      },
      "source": [
        "refs, hypos, img_ids = evaluate(3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3:  33%|███▎      | 1000/3000 [01:18<02:43, 12.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-1908034cad9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-376b10500fd4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(beam_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# For each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n\u001b[0;32m---> 10\u001b[0;31m         tqdm(test_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-51-611c6710041b>\", line 48, in __getitem__\n    img_id = self.df['file_name'].unique()[index]\nIndexError: index 1000 is out of bounds for axis 0 with size 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PX4A4TYaRAQ",
        "outputId": "5b0f56f9-e5c1-4e42-d8ab-39850774372e"
      },
      "source": [
        "len(refs)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqmmMPx-aSwv",
        "outputId": "3547d2b1-71f5-48b4-f93c-790fe5a6ffc1"
      },
      "source": [
        "len(hypos)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOlZXTkGBoS6"
      },
      "source": [
        "addit_tokens = [vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU6jEz6-Aoja"
      },
      "source": [
        "hypothesis_text = [\" \".join([vocab.itos[i] for i in sent if i not in addit_tokens]) for sent in hypos]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wGPHDfI6CQ2y",
        "outputId": "4f3d4f34-a0a6-4768-b2c1-c3d70a4dc2e2"
      },
      "source": [
        "hypothesis_text[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a young boy in a blue shirt is running on the street'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5nWqyPDefR"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc4qUdsKCoTq"
      },
      "source": [
        "# add hypothesis and references to a df\n",
        "cap_df = pd.DataFrame.from_dict({\"file_name\": img_ids,\"En hyps\": hypothesis_text})"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7kElLUz-Dxto",
        "outputId": "214d08be-7172-4ef3-a925-fe6ecfcc0fdf"
      },
      "source": [
        "cap_df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>En hyps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2992</th>\n",
              "      <td>979383193_0a542a059d.jpg</td>\n",
              "      <td>two children are sitting on a bench</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2993</th>\n",
              "      <td>979383193_0a542a059d.jpg</td>\n",
              "      <td>two children are sitting on a bench</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>a man is climbing a rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>a man is climbing a rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>a man is climbing a rock</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2997 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      file_name                                            En hyps\n",
              "0     1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "1     1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "2     1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "3      106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o...\n",
              "4      106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o...\n",
              "...                         ...                                                ...\n",
              "2992   979383193_0a542a059d.jpg                two children are sitting on a bench\n",
              "2993   979383193_0a542a059d.jpg                two children are sitting on a bench\n",
              "2994   997722733_0cb5439472.jpg                           a man is climbing a rock\n",
              "2995   997722733_0cb5439472.jpg                           a man is climbing a rock\n",
              "2996   997722733_0cb5439472.jpg                           a man is climbing a rock\n",
              "\n",
              "[2997 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1CS80XDmzLx",
        "outputId": "14c860d2-b529-4cda-dfff-284889f87b95"
      },
      "source": [
        "cap_df.file_name.nunique()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JggkwoyQBySa"
      },
      "source": [
        "cap_df.to_csv(\"cap_eng_output.csv\", index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "soCAfb7pT9PZ",
        "outputId": "3b1c2671-46c9-4dbb-c255-d9b188258810"
      },
      "source": [
        "cap_df = pd.read_csv(\"cap_eng_output.csv\")\n",
        "cap_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>En hyps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name                                            En hyps\n",
              "0  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "1  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "2  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "3   106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o...\n",
              "4   106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RIWLRBxzULlc",
        "outputId": "856ecbd1-0e83-453d-e226-91c570c36198"
      },
      "source": [
        "cap_df = pd.read_excel(\"/content/cap_to_ar.xlsx\")\n",
        "cap_df.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>en_hyps</th>\n",
              "      <th>ar_trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يقف على الشاطئ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يقف على الشاطئ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name  ...                                   ar_trans\n",
              "0  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "1  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "2  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "3   106490881_5a2dd9b7bd.jpg  ...   صبي صغير يرتدي قميصا أزرق يقف على الشاطئ\n",
              "4   106490881_5a2dd9b7bd.jpg  ...   صبي صغير يرتدي قميصا أزرق يقف على الشاطئ\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "9ZMcE9iAfiUz",
        "outputId": "107f3908-75a8-4ec2-eb79-00d7e424c650"
      },
      "source": [
        "df_org = pd.read_json(\"/content/Image-Captioning/ar_data.json\")\n",
        "df_test = df_org[df_org.split=='test']\n",
        "df_test.head(5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>caption</th>\n",
              "      <th>split</th>\n",
              "      <th>tok_len</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>امرأة شقراء في قميص أزرق تنتظر رحلة</td>\n",
              "      <td>test</td>\n",
              "      <td>7</td>\n",
              "      <td>[امرأة, شقراء, في, قميص, أزرق, تنتظر, رحلة]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>امرأة شقراء في الشارع تشير الى سيارة أجرة</td>\n",
              "      <td>test</td>\n",
              "      <td>8</td>\n",
              "      <td>[امرأة, شقراء, في, الشارع, تشير, الى, سيارة, أ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>المرأة في الثوب الأزرق تمد بذراعها لحركة المرو...</td>\n",
              "      <td>test</td>\n",
              "      <td>9</td>\n",
              "      <td>[المرأة, في, الثوب, الأزرق, تمد, بذراعها, لحرك...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>صبي في ملابس السباحة الزرقاء على الشاطئ</td>\n",
              "      <td>test</td>\n",
              "      <td>7</td>\n",
              "      <td>[صبي, في, ملابس, السباحة, الزرقاء, على, الشاطئ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>صبي يبتسم للكاميرا على الشاطئ</td>\n",
              "      <td>test</td>\n",
              "      <td>5</td>\n",
              "      <td>[صبي, يبتسم, للكاميرا, على, الشاطئ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     file_name  ...                                             tokens\n",
              "126  1056338697_4f7d7ce270.jpg  ...        [امرأة, شقراء, في, قميص, أزرق, تنتظر, رحلة]\n",
              "127  1056338697_4f7d7ce270.jpg  ...  [امرأة, شقراء, في, الشارع, تشير, الى, سيارة, أ...\n",
              "128  1056338697_4f7d7ce270.jpg  ...  [المرأة, في, الثوب, الأزرق, تمد, بذراعها, لحرك...\n",
              "144   106490881_5a2dd9b7bd.jpg  ...    [صبي, في, ملابس, السباحة, الزرقاء, على, الشاطئ]\n",
              "145   106490881_5a2dd9b7bd.jpg  ...                [صبي, يبتسم, للكاميرا, على, الشاطئ]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrzQgNpngLGz"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG8yonRphSFi"
      },
      "source": [
        "translated_caps = []\n",
        "original_caps = []\n",
        "for idx, i in enumerate(cap_df.file_name.to_list()):\n",
        "  translated_caps.append(cap_df.iloc[idx].ar_trans.split())\n",
        "  caps = []\n",
        "  for k in df_test[df_test.file_name==i].caption.to_list():\n",
        "    caps.append(k.split())\n",
        "  original_caps.append(caps)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpdV_3_KiJ3N",
        "outputId": "5e273450-9596-4cd0-b56f-a1b3546063d9"
      },
      "source": [
        "len(translated_caps), len(original_caps)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2997, 2997)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nBoWVpGiWA_",
        "outputId": "a1fd9d1e-f04c-4126-982b-f00a36a38799"
      },
      "source": [
        "translated_caps[1]"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['صبي', 'صغير', 'يرتدي', 'قميصا', 'أزرق', 'يعمل', 'على', 'الشارع']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-oU0MrXLlA",
        "outputId": "6caa358f-f89d-4b00-eb38-3c885824120f"
      },
      "source": [
        "original_caps[1]"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['امرأة', 'شقراء', 'في', 'قميص', 'أزرق', 'تنتظر', 'رحلة'],\n",
              " ['امرأة', 'شقراء', 'في', 'الشارع', 'تشير', 'الى', 'سيارة', 'أجرة'],\n",
              " ['المرأة',\n",
              "  'في',\n",
              "  'الثوب',\n",
              "  'الأزرق',\n",
              "  'تمد',\n",
              "  'بذراعها',\n",
              "  'لحركة',\n",
              "  'المرور',\n",
              "  'القادمة']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKlo8mVjR1B",
        "outputId": "ca0f1174-29ae-43b2-e62c-267562f2a9bc"
      },
      "source": [
        "translated_caps[100]"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['مجموعة', 'من', 'الناس', 'يقفون', 'أمام', 'مبنى']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znHpc0ZTjUE1",
        "outputId": "0cfecca0-7d09-4a8e-8567-31d302e65210"
      },
      "source": [
        "original_caps[100]"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['رجل', 'وفتاة', 'يجلسان', 'على', 'الأرض', 'ويأكلان'],\n",
              " ['رجل', 'وفتاة', 'صغيرة', 'يجلسان', 'على', 'رصيف', 'قرب', 'حقيبة', 'زرقاء'],\n",
              " ['رجل', 'وفتاة', 'يأكلان', 'وجبة', 'في', 'أحد', 'شوارع', 'المدينة']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFnNz-xNj04K",
        "outputId": "cd642bc8-f083-4a20-cf3c-5a3d85651aa7"
      },
      "source": [
        "print_scores(original_caps, translated_caps)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 34.233436623874866\n",
            "2: 20.012890946720674\n",
            "3: 11.004236540712997\n",
            "4: 5.5992246799395\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34.233436623874866, 20.012890946720674, 11.004236540712997, 5.5992246799395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsJLIchlc98g",
        "outputId": "3852b9d4-8135-433b-e4ac-5c522c2fb62f"
      },
      "source": [
        "original_caps[0]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['امرأة شقراء في قميص أزرق تنتظر رحلة',\n",
              " 'امرأة شقراء في الشارع تشير الى سيارة أجرة',\n",
              " 'المرأة في الثوب الأزرق تمد بذراعها لحركة المرور القادمة']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrMZQ4CTc_T3",
        "outputId": "b5615aef-e86b-4258-edfe-8702f0a61186"
      },
      "source": [
        "cap_df.ar_trans.nunique()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOfoIgt7d0zv",
        "outputId": "9f6a87b3-4106-4864-a655-8ce4db669afb"
      },
      "source": [
        "!wget https://kkb-production.jupyter-proxy.kaggle.net/k/67249843/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..QxozAcCRQtqJg9saA6D1_A.Dyw9Y41owBppV8NPupo93f_ChdCabBRRSRa4u1mX2l0eqah7jd0e63V1AO2Nj0viz2ftYrW-kBJDRfKnxjg201a08nxF-P1seOVy_bJoSZm9ZKZC37-H9OU_I63F5UZx1zbvLdDcIrjF0BCOBPFAaYtF39TofvrZuYHprJbHQ4HPbEGRL73PZQhvIaveoi1Vs_Vfwmy_bcg__5jUopuUCw.azKye_nBu246g19zT-1Tsw/proxy/files/exps.zip"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-02 09:50:28--  https://kkb-production.jupyter-proxy.kaggle.net/k/67249843/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..QxozAcCRQtqJg9saA6D1_A.Dyw9Y41owBppV8NPupo93f_ChdCabBRRSRa4u1mX2l0eqah7jd0e63V1AO2Nj0viz2ftYrW-kBJDRfKnxjg201a08nxF-P1seOVy_bJoSZm9ZKZC37-H9OU_I63F5UZx1zbvLdDcIrjF0BCOBPFAaYtF39TofvrZuYHprJbHQ4HPbEGRL73PZQhvIaveoi1Vs_Vfwmy_bcg__5jUopuUCw.azKye_nBu246g19zT-1Tsw/proxy/files/exps.zip\n",
            "Resolving kkb-production.jupyter-proxy.kaggle.net (kkb-production.jupyter-proxy.kaggle.net)... 35.244.180.134\n",
            "Connecting to kkb-production.jupyter-proxy.kaggle.net (kkb-production.jupyter-proxy.kaggle.net)|35.244.180.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107999 (105K) [application/zip]\n",
            "Saving to: ‘exps.zip’\n",
            "\n",
            "exps.zip            100%[===================>] 105.47K   258KB/s    in 0.4s    \n",
            "\n",
            "2021-07-02 09:50:28 (258 KB/s) - ‘exps.zip’ saved [107999/107999]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}