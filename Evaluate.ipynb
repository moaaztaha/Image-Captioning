{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from utils import load_checkpoint\n",
    "from dataset import build_vocab, get_loaders\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 400361.20it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocab('data.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "CHECKPOINT_PATH = 'models/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101.pth.tar'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint!!\n",
      "Last Epoch: 8\n",
      "Best Bleu-4: 0.13934344941076532\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 290537.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4451"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(DATA_JSON_PATH)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "beam_size=1\n",
    "test_loader = get_loaders(bs, IMGS_PATH, DATA_JSON_PATH, transform, vocab, test=True, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating at Beam size 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0009], device='cuda:0')\n",
      "tensor([0.0009], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d36de7286a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# add new words to sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_word_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# which sequences are incomplete - didn't reach <end>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "references = list()\n",
    "hypotheses = list()\n",
    "\n",
    "for i, (image, caps, caplens, allcaps) in enumerate(\n",
    "        tqdm(test_loader, desc=f'Evaluating at Beam size {beam_size}')):\n",
    "    \n",
    "    k = beam_size\n",
    "    \n",
    "    image = image.to(device)\n",
    "    \n",
    "    # encoder\n",
    "    encoder_out = encoder(image) # [1, enc_img_size, enc_img_size, encoder_dim]\n",
    "    enc_img_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "    \n",
    "    # flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim) # [1, num_pixels, encoder_dim]\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    \n",
    "    # treat it as we have a batch of size k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim) # [k, num_pixels, encoder_dim]\n",
    "    \n",
    "    # tensor to store top k previous words at each step; currently it's only <sos>\n",
    "    k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]]*k).to(device) # [k, 1]\n",
    "    \n",
    "    # tensor to store top k sequences\n",
    "    seqs = k_prev_words\n",
    "    \n",
    "    # tensor to store top k sequences' scores\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)\n",
    "    \n",
    "    # lists to store completed sequences and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "    \n",
    "    # decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "    \n",
    "    while True:\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1) # [s, embed_dim]\n",
    "        \n",
    "        awe, _ = decoder.attention(encoder_out, h) # [s, encoder_dim], [s, num_pixels]\n",
    "        \n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))\n",
    "        awe = gate * awe\n",
    "        \n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))\n",
    "        \n",
    "        scores = decoder.fc(h) # [s, vocab size]\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        # add \n",
    "        scores = top_k_scores.expand_as(scores) + scores # [s, vocab_size]\n",
    "        \n",
    "        # for the first step: all k points will have the same score; since same k previous words, h,c\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "        else:\n",
    "            # unroll and find top scores and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
    "            \n",
    "        # convert unrolled indices to actual indices of scores\n",
    "        print(prev_word_inds)\n",
    "        prev_word_inds = top_k_words / len(vocab)\n",
    "        next_word_inds = top_k_words % len(vocab)\n",
    "        \n",
    "        print(prev_word_inds)\n",
    "        print(next_word_inds)\n",
    "        \n",
    "        # add new words to sequences\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # which sequences are incomplete - didn't reach <end>\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != vocab['<eos>']]\n",
    "        \n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "        \n",
    "        # set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds) # reduce beam length accordingly\n",
    "        \n",
    "        # proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        \n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "        \n",
    "        # break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[1]\n",
    "    \n",
    "    # references \n",
    "    for j in range(allcaps.shape[0]):\n",
    "            img_caps = allcaps[j].tolist()\n",
    "            references.append(vocab.indextostring(img_caps))\n",
    "\n",
    "    # hypotheses\n",
    "    hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
    "    \n",
    "    assert len(references) == len(hypotheses)\n",
    "    \n",
    "bleu4 = corpus_bleu(references, hypotheses)\n",
    "\n",
    "print(bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4451"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
