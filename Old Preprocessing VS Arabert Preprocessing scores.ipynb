{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>caption</th>\n",
       "      <th>split</th>\n",
       "      <th>tok_len</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>طفلة صغيرة تتسلق إلى مسرح خشبي</td>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "      <td>[طفلة, صغيرة, تتسلق, إلى, مسرح, خشبي]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>طفلة صغيرة تتسلق الدرج إلى منزلها</td>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "      <td>[طفلة, صغيرة, تتسلق, الدرج, إلى, منزلها]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>فتاة صغيرة في ثوب وردي تذهب إلى المقصورة الخشبية</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>[فتاة, صغيرة, في, ثوب, وردي, تذهب, إلى, المقصو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>كلب أسود وكلب ثلاثي الألوان يلعبان مع بعضهما ا...</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>[كلب, أسود, وكلب, ثلاثي, الألوان, يلعبان, مع, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>كلب أسود وكلب أبيض ببقع بنية يحدقان في بعضهما ...</td>\n",
       "      <td>train</td>\n",
       "      <td>12</td>\n",
       "      <td>[كلب, أسود, وكلب, أبيض, ببقع, بنية, يحدقان, في...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file_name  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1001773457_577c3a7d70.jpg   \n",
       "4  1001773457_577c3a7d70.jpg   \n",
       "\n",
       "                                             caption  split  tok_len  \\\n",
       "0                     طفلة صغيرة تتسلق إلى مسرح خشبي  train        6   \n",
       "1                  طفلة صغيرة تتسلق الدرج إلى منزلها  train        6   \n",
       "2   فتاة صغيرة في ثوب وردي تذهب إلى المقصورة الخشبية  train        9   \n",
       "3  كلب أسود وكلب ثلاثي الألوان يلعبان مع بعضهما ا...  train       11   \n",
       "4  كلب أسود وكلب أبيض ببقع بنية يحدقان في بعضهما ...  train       12   \n",
       "\n",
       "                                              tokens  \n",
       "0              [طفلة, صغيرة, تتسلق, إلى, مسرح, خشبي]  \n",
       "1           [طفلة, صغيرة, تتسلق, الدرج, إلى, منزلها]  \n",
       "2  [فتاة, صغيرة, في, ثوب, وردي, تذهب, إلى, المقصو...  \n",
       "3  [كلب, أسود, وكلب, ثلاثي, الألوان, يلعبان, مع, ...  \n",
       "4  [كلب, أسود, وكلب, أبيض, ببقع, بنية, يحدقان, في...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('old_ar_data.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:06:17.133766Z",
     "iopub.status.busy": "2021-07-17T13:06:17.133354Z",
     "iopub.status.idle": "2021-07-17T13:06:17.181700Z",
     "shell.execute_reply": "2021-07-17T13:06:17.180619Z",
     "shell.execute_reply.started": "2021-07-17T13:06:17.133730Z"
    },
    "id": "b4366e4d"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:06:18.808551Z",
     "iopub.status.busy": "2021-07-17T13:06:18.808173Z",
     "iopub.status.idle": "2021-07-17T13:06:23.008079Z",
     "shell.execute_reply": "2021-07-17T13:06:23.007029Z",
     "shell.execute_reply.started": "2021-07-17T13:06:18.808492Z"
    },
    "id": "4034c69e"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from train import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os import path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:06:38.929331Z",
     "iopub.status.busy": "2021-07-17T13:06:38.928921Z",
     "iopub.status.idle": "2021-07-17T13:06:38.978858Z",
     "shell.execute_reply": "2021-07-17T13:06:38.977752Z",
     "shell.execute_reply.started": "2021-07-17T13:06:38.929301Z"
    },
    "id": "b8ce76f9"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "encoder_dim = 2048 # resnet101\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# training parameters\n",
    "epochs = 30  # number of epochs to train for (if early stopping is not triggered)\n",
    "batch_size = 256\n",
    "workers = 2\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "pretrained_embeddings = False\n",
    "fine_tune_embeddings = False\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Preprocessing to Arabert  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:07:02.652391Z",
     "iopub.status.busy": "2021-07-17T13:07:02.652020Z",
     "iopub.status.idle": "2021-07-17T13:07:02.698928Z",
     "shell.execute_reply": "2021-07-17T13:07:02.697758Z",
     "shell.execute_reply.started": "2021-07-17T13:07:02.652357Z"
    },
    "id": "d7a302e3"
   },
   "outputs": [],
   "source": [
    "DATA_JSON_PATH = 'old_ar_data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "DATA_NAME = 'TESTING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:07:07.551421Z",
     "iopub.status.busy": "2021-07-17T13:07:07.551059Z",
     "iopub.status.idle": "2021-07-17T13:07:08.027838Z",
     "shell.execute_reply": "2021-07-17T13:07:08.026347Z",
     "shell.execute_reply.started": "2021-07-17T13:07:07.551388Z"
    },
    "id": "a4b1a677",
    "outputId": "9356268d-4297-4f43-c44e-3aef77004680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [00:00<00:00, 498812.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq = 30\n",
    "vocab = build_vocab(DATA_JSON_PATH, max_seq=max_seq)\n",
    "vocab_len = len(vocab); vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T13:07:28.390568Z",
     "iopub.status.busy": "2021-07-17T13:07:28.390150Z",
     "iopub.status.idle": "2021-07-17T13:07:28.442817Z",
     "shell.execute_reply": "2021-07-17T13:07:28.441335Z",
     "shell.execute_reply.started": "2021-07-17T13:07:28.390526Z"
    },
    "id": "ff6df2d0",
    "outputId": "dcf14d12-9a82-49ef-b74b-66b728e23823"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " ['<pad>',\n",
       "  '<sos>',\n",
       "  '<eos>',\n",
       "  '<unk>',\n",
       "  'طفلة',\n",
       "  'صغيرة',\n",
       "  'تتسلق',\n",
       "  'إلى',\n",
       "  'كلب',\n",
       "  'أسود'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.itos.keys())[:10], list(vocab.itos.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T14:31:30.992363Z",
     "iopub.status.busy": "2021-07-17T14:31:30.991995Z",
     "iopub.status.idle": "2021-07-17T14:31:31.341563Z",
     "shell.execute_reply": "2021-07-17T14:31:31.340228Z",
     "shell.execute_reply.started": "2021-07-17T14:31:30.992328Z"
    },
    "id": "cgtlSQWzjNwz",
    "outputId": "9df0886b-4b65-4a7b-d4bc-36b41f0c9e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint!!\n",
      "Last Epoch: 19\n",
      "Best Bleu-4: 6.862300456763069\n"
     ]
    }
   ],
   "source": [
    "m = load_checkpoint(\"models/BEST_checkpoint_flickr8k_ar_finetune.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = m['encoder'].eval()\n",
    "decoder = m['decoder'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T15:07:27.757142Z",
     "iopub.status.busy": "2021-07-17T15:07:27.756791Z",
     "iopub.status.idle": "2021-07-17T15:21:46.839602Z",
     "shell.execute_reply": "2021-07-17T15:21:46.838395Z",
     "shell.execute_reply.started": "2021-07-17T15:07:27.757112Z"
    },
    "id": "reIYwZ6O6ncg",
    "outputId": "56059b76-ff0b-4176-eef8-67e938da89d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 1:   0%|          | 0/3000 [00:00<?, ?it/s]/home/kelwa/anaconda3/envs/yolo5/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/kelwa/anaconda3/envs/yolo5/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "EVALUATING AT BEAM SIZE 1: 100%|██████████| 3000/3000 [01:15<00:00, 39.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 39.95516959427624\n",
      "2: 25.77931292578018\n",
      "3: 15.10781606368901\n",
      "4: 8.64667118061901\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 2: 100%|██████████| 3000/3000 [01:22<00:00, 36.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.68928313941436\n",
      "2: 26.982549212073188\n",
      "3: 16.459546825901818\n",
      "4: 9.631675158047099\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|██████████| 3000/3000 [01:31<00:00, 32.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.373911854332825\n",
      "2: 26.615634981477243\n",
      "3: 16.09711443584611\n",
      "4: 9.384744789396482\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 4: 100%|██████████| 3000/3000 [01:41<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.48892525456145\n",
      "2: 26.861057340847083\n",
      "3: 16.299550030840223\n",
      "4: 9.621167436145724\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5: 100%|██████████| 3000/3000 [01:50<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 39.84779810028518\n",
      "2: 26.47471808239697\n",
      "3: 15.929967050767372\n",
      "4: 9.330062157193685\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "from eval import test_score\n",
    "\n",
    "for i in range(1, 6):\n",
    "    b1, b2, b3, b4 = test_score(i, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results of the best beam size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 2: 100%|██████████| 3000/3000 [01:31<00:00, 32.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.68928313941436\n",
      "2: 26.982549212073188\n",
      "3: 16.459546825901818\n",
      "4: 9.631675158047099\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "references, hypotheses = test_score(2, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab, return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_old_tokens = [[[vocab.itos[i] for i in refe] for refe in refes] for refes in references]\n",
    "hypotheses_old_tokens = [[vocab.itos[i] for i in hypo] for hypo in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-09-27 17:38:21,743 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_ara_tokens = [araby.tokenize(arabert_prep.preprocess(\" \".join(i))) for i in hypotheses_old_tokens]\n",
    "references_ara_tokens = [[araby.tokenize(arabert_prep.preprocess(\" \".join(i))) for i in ref] for ref in references_old_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 54.85289890804387\n",
      "2: 43.93811436252674\n",
      "3: 34.76736607534273\n",
      "4: 27.80241332075155\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54.85289890804387, 43.93811436252674, 34.76736607534273, 27.80241332075155)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_scores(references_ara_tokens, hypotheses_ara_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabet to old Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JSON_PATH = 'data/ar_data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "DATA_NAME = 'TESTING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [00:00<00:00, 310986.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3309"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq = 65\n",
    "vocab = build_vocab(DATA_JSON_PATH, max_seq=max_seq)\n",
    "vocab_len = len(vocab); vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " ['<pad>', '<sos>', '<eos>', '<unk>', '+', 'ة', 'طفل', 'صغير', 'تتسلق', 'إلى'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.itos.keys())[:10], list(vocab.itos.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint!!\n",
      "Last Epoch: 9\n",
      "Best Bleu-4: 24.949378413361714\n"
     ]
    }
   ],
   "source": [
    "m = load_checkpoint(\"ar_models/BEST_checkpoint_flickr8k_ar_arabert_pretrained_finetune.pth.tar\")\n",
    "encoder = m['encoder'].eval()\n",
    "decoder = m['decoder'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 1:   0%|          | 0/3000 [00:00<?, ?it/s]/home/kelwa/anaconda3/envs/yolo5/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/kelwa/anaconda3/envs/yolo5/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "EVALUATING AT BEAM SIZE 1: 100%|██████████| 3000/3000 [01:51<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 59.27134312126155\n",
      "2: 45.52397958654338\n",
      "3: 33.58850576504064\n",
      "4: 24.918812277227662\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 2: 100%|██████████| 3000/3000 [02:12<00:00, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 59.68623977817169\n",
      "2: 46.88248014003475\n",
      "3: 35.44609658225986\n",
      "4: 26.921966136090035\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|██████████| 3000/3000 [02:42<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 60.32593136195551\n",
      "2: 47.5536072957737\n",
      "3: 36.147037636633875\n",
      "4: 27.524282207029003\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 4: 100%|██████████| 3000/3000 [03:08<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 59.51223743176995\n",
      "2: 47.318183310877345\n",
      "3: 36.32951749139962\n",
      "4: 27.893442570349023\n",
      "-------------------------\n",
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5: 100%|██████████| 3000/3000 [03:11<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 58.678484239386094\n",
      "2: 46.85683508774053\n",
      "3: 36.14555791431082\n",
      "4: 27.864202291806382\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "from eval import test_score\n",
    "for i in range(1, 6):\n",
    "    b1, b2, b3, b4 = test_score(i, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 4: 100%|██████████| 3000/3000 [03:12<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 59.51223743176995\n",
      "2: 47.318183310877345\n",
      "3: 36.32951749139962\n",
      "4: 27.893442570349023\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "references, hypotheses = test_score(4, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab, return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_ara_tokens = [[[vocab.itos[i] for i in refe] for refe in refes] for refes in references]\n",
    "hypotheses_ara_tokens = [[vocab.itos[i] for i in hypo] for hypo in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_sent_tokens = [[arabert_prep.unpreprocess(' '.join(w for w in i)) for i in refe] for refe in references_ara_tokens] \n",
    "hypotheses_sent_tokens = [arabert_prep.unpreprocess(' '.join(w for w in i)) for i in hypotheses_ara_tokens]\n",
    "assert len(references_sent_tokens) == len(hypotheses_sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_old_tokens = [[araby.tokenize(i)  for i in refe] for refe in references_sent_tokens] \n",
    "hypotheses_old_tokens = [araby.tokenize(i)  for i in hypotheses_sent_tokens]\n",
    "assert len(references_old_tokens) == len(hypotheses_old_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 32.26797327283087\n",
      "2: 20.74686106020747\n",
      "3: 11.73920224562103\n",
      "4: 6.022065664696533\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.26797327283087, 20.74686106020747, 11.73920224562103, 6.022065664696533)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_scores(references_old_tokens, hypotheses_old_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
