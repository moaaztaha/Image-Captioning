{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v18vuSVO97ze",
    "outputId": "d01118d1-479a-44e8-9a96-bc345dd1f056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  6 23:49:43 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   86C    P2    54W /  N/A |   3658MiB /  6078MiB |     86%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2451      G   /usr/lib/Xorg                      45MiB |\r\n",
      "|    0   N/A  N/A     27635      C   ...s/kaggle_torch/bin/python     3609MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sZ4lhGja9h7N"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TSpDx8Ad9h7N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from utils import load_checkpoint\n",
    "from dataset import build_vocab, get_loaders, get_10k_vocab, top10k_vocab\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from utils import print_scores\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gPxE2X69h7O"
   },
   "outputs": [],
   "source": [
    "# DATA_NAME = 'flickr8k_ar'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M5g3rBl39h7O"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oVqmuxp8_Ghy"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = 'models//run/media/kelwa/DEV/GP/Image-Captioning/models/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop_finetune.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riR8ECa99h7P",
    "outputId": "3aa0b9ff-1237-4778-dcc9-ce1907901f53",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models//run/media/kelwa/DEV/GP/Image-Captioning/models/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop_finetune.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b06a6b75133c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(path, cpu)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded Checkpoint!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models//run/media/kelwa/DEV/GP/Image-Captioning/models/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop_finetune.pth.tar'"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wQ54asZj9h7Q"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btBk9a7v9h7Q",
    "outputId": "6d2385eb-559f-467c-a6e1-9b4f93ef5e51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [11:12<?, ?it/s]\n",
      "100%|██████████| 155070/155070 [00:00<00:00, 319657.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12096"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(DATA_JSON_PATH)\n",
    "# top10k_words = get_10k_vocab(\"10k_words.txt\")\n",
    "# vocab = top10k_vocab(top10k_words)\n",
    "vocab_len = len(vocab)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAq8Tncv9h7e",
    "outputId": "d8adf96f-e5e4-4133-b197-21ddddc1367f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [03:03<00:00, 27.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 60.22017745645745\n",
      "2: 42.03301963954654\n",
      "3: 28.865543593536668\n",
      "4: 19.822668164309842\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:55<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 64.22550530227498\n",
      "2: 45.20326452084617\n",
      "3: 31.54755348941619\n",
      "4: 21.90451521810326\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 5:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5: 100%|██████████| 5000/5000 [05:07<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 65.29163468917882\n",
      "2: 45.991178094289644\n",
      "3: 32.14624016285145\n",
      "4: 22.262874291057862\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6, 2):\n",
    "    test_score(i, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "import torchvision.transforms as transfroms\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Caption Dataset Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgs_dir, captions_file, vocab, transforms=None, split='train'):\n",
    "        \"\"\"\n",
    "        :param imgs_dir: folder where images are stored\n",
    "        :param captions_file: the df file with all caption information\n",
    "        :param vocab: vocabuary object\n",
    "        :param transforms: image transforms pipeline\n",
    "        :param split: data split\n",
    "        \"\"\"\n",
    "\n",
    "        # split has to be one of {'train', 'val', 'test'}\n",
    "        assert split in {'train', 'val', 'test'}\n",
    "\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.df = pd.read_json(captions_file)\n",
    "        self.df = self.df[self.df['split'] == split]\n",
    "        self.vocab = vocab\n",
    "        self.transforms = transforms\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset_size = self.df.shape[0]\n",
    "        # printing some info\n",
    "        print(f\"Dataset split: {split}\")\n",
    "        print(f\"Unique images: {self.df.file_name.nunique()}\")\n",
    "        print(f\"Total size: {self.dataset_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # loading the image\n",
    "        img_id = self.df['file_name'].values[index]\n",
    "        img = Image.open(self.imgs_dir+img_id).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = transfroms.ToTensor()(img)\n",
    "\n",
    "        # loading current caption\n",
    "        cap_len = self.df['tok_len'].values[index] + 2 # <sos> and <eos>\n",
    "        tokens = self.df['tokens'].values[index]\n",
    "        caption = torch.LongTensor(self.vocab.numericalize(tokens, cap_len))\n",
    "\n",
    "        if self.split is 'train':\n",
    "            return img, caption, cap_len\n",
    "        else:\n",
    "            # for val and test return all captions for calculate the bleu scores\n",
    "            captions_tokens = self.df[self.df['file_name'] == img_id].tokens.values\n",
    "            captions_lens = self.df[self.df['file_name'] == img_id].tok_len.values\n",
    "            all_tokens = []\n",
    "            for token, cap_len in zip(captions_tokens, captions_lens):\n",
    "                all_tokens.append(self.vocab.numericalize(token, cap_len)[1:]) # remove <sos>\n",
    "\n",
    "            return img, caption, cap_len, torch.tensor(all_tokens), img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "\n",
    "loader = DataLoader(\n",
    "            dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
    "                                    transforms=transform, vocab=vocab, split='test'),\n",
    "            batch_size=bs,\n",
    "            num_workers=7,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(beam_size):\n",
    "\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "    img_ids = list()\n",
    "    \n",
    "    # For each image\n",
    "    for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n",
    "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
    "        \n",
    "        k = beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        image = image.to(device)  # (1, 3, 256, 256)\n",
    "\n",
    "        # Encode\n",
    "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "        enc_image_size = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(3)\n",
    "\n",
    "        # Flatten encoding\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
    "        \n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "            awe = gate * awe\n",
    "\n",
    "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "            scores = decoder.fc(h)  # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "            # Add\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
    "          \n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "            \n",
    "#             print(top_k_scores, top_k_words)\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != vocab.stoi['<eos>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "        \n",
    "        if len(complete_seqs_scores) == 0:\n",
    "            continue\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
    "        \n",
    "        img_ids.append(img_id[0])\n",
    "        assert len(references) == len(hypotheses) == len(img_ids)\n",
    "    # Calculate BLEU-4 scores\n",
    "#     bleu4 = corpus_bleu(references, hypotheses)\n",
    "    return references, hypotheses, img_ids\n",
    "    print_scores(references, hypotheses, nltk=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5:  40%|███▉      | 1998/5000 [02:15<03:10, 15.73it/s]"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab_len\n",
    "references, hypotheses, img_ids = evaluate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\"file_name\":img_ids, \"references\":references, \"hypothesis\": hypotheses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>references</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3028145992.jpg</td>\n",
       "      <td>[[4, 20, 5, 4, 22, 56, 10, 153, 1982, 13, 4, 3...</td>\n",
       "      <td>[4, 34, 8, 18, 52, 30, 6, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3643021980.jpg</td>\n",
       "      <td>[[4, 31, 54, 5, 4, 180, 174, 650, 631, 71, 6, ...</td>\n",
       "      <td>[4, 31, 54, 11, 338, 7, 4, 394]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226481576.jpg</td>\n",
       "      <td>[[4, 377, 554, 1036, 40, 15, 1246, 153, 10, 46...</td>\n",
       "      <td>[4, 12, 5, 4, 43, 56, 10, 463, 11, 25, 4, 183]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1153704539.jpg</td>\n",
       "      <td>[[4, 31, 54, 40, 4, 43, 56, 1908, 5, 4, 637, 4...</td>\n",
       "      <td>[4, 12, 5, 4, 43, 56, 10, 22, 364, 11, 47, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4808256003.jpg</td>\n",
       "      <td>[[14, 114, 65, 19, 15, 342, 2100, 32, 197, 366...</td>\n",
       "      <td>[4, 12, 5, 4, 36, 56, 11, 21, 5, 38, 8, 4, 352]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name                                         references  \\\n",
       "0  3028145992.jpg  [[4, 20, 5, 4, 22, 56, 10, 153, 1982, 13, 4, 3...   \n",
       "1  3643021980.jpg  [[4, 31, 54, 5, 4, 180, 174, 650, 631, 71, 6, ...   \n",
       "2   226481576.jpg  [[4, 377, 554, 1036, 40, 15, 1246, 153, 10, 46...   \n",
       "3  1153704539.jpg  [[4, 31, 54, 40, 4, 43, 56, 1908, 5, 4, 637, 4...   \n",
       "4  4808256003.jpg  [[14, 114, 65, 19, 15, 342, 2100, 32, 197, 366...   \n",
       "\n",
       "                                          hypothesis  \n",
       "0                      [4, 34, 8, 18, 52, 30, 6, 24]  \n",
       "1                    [4, 31, 54, 11, 338, 7, 4, 394]  \n",
       "2     [4, 12, 5, 4, 43, 56, 10, 463, 11, 25, 4, 183]  \n",
       "3  [4, 12, 5, 4, 43, 56, 10, 22, 364, 11, 47, 4, ...  \n",
       "4    [4, 12, 5, 4, 36, 56, 11, 21, 5, 38, 8, 4, 352]  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"predicted_captions_30.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▍         | 47/1000 [00:00<00:02, 466.23it/s]\u001b[A\n",
      " 10%|█         | 103/1000 [00:00<00:01, 520.01it/s]\u001b[A\n",
      " 16%|█▋        | 163/1000 [00:00<00:01, 556.20it/s]\u001b[A\n",
      " 23%|██▎       | 230/1000 [00:00<00:01, 598.90it/s]\u001b[A\n",
      " 29%|██▉       | 290/1000 [00:00<00:01, 585.91it/s]\u001b[A\n",
      " 36%|███▌      | 357/1000 [00:00<00:01, 612.12it/s]\u001b[A\n",
      " 42%|████▏     | 419/1000 [00:00<00:00, 606.19it/s]\u001b[A\n",
      " 49%|████▉     | 488/1000 [00:00<00:00, 630.35it/s]\u001b[A\n",
      " 55%|█████▌    | 552/1000 [00:00<00:00, 619.04it/s]\u001b[A\n",
      " 61%|██████▏   | 614/1000 [00:01<00:00, 589.35it/s]\u001b[A\n",
      " 67%|██████▋   | 674/1000 [00:01<00:00, 539.84it/s]\u001b[A\n",
      " 73%|███████▎  | 729/1000 [00:01<00:00, 538.93it/s]\u001b[A\n",
      " 79%|███████▉  | 790/1000 [00:01<00:00, 557.38it/s]\u001b[A\n",
      " 85%|████████▍ | 849/1000 [00:01<00:00, 565.39it/s]\u001b[A\n",
      " 91%|█████████ | 906/1000 [00:01<00:00, 558.83it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 572.05it/s][A\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "hypothesis = []\n",
    "for fname in tqdm(df.file_name.unique()):\n",
    "  references.append(df[df.file_name==fname].references.to_list()[0])\n",
    "  hypothesis.append(df[df.file_name==fname].hypothesis.to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 297658.36it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:41<?, ?it/s]\u001b[A\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 42332.93it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_tokens = [\" \".join([vocab.itos[i] for i in seq]) for seq in tqdm(hypothesis)]\n",
    "\n",
    "refes_tokens = []\n",
    "for ref in tqdm(references):\n",
    "  refes_tokens.append([\" \".join([vocab.itos[i] for i in seq]) for seq in ref]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = {idx: [tokens] for idx, tokens in enumerate(preds_tokens)}\n",
    "refs = {idx: tokens for idx, tokens in enumerate(refes_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refs), len(hypo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/salaniz/pycocoevalcap.git\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-req-build-4ckm44vg\n",
      "  Running command git clone -q https://github.com/salaniz/pycocoevalcap.git /tmp/pip-req-build-4ckm44vg\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (57.1.0)\n",
      "Collecting cython>=0.27.3\n",
      "  Using cached Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.20.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (8.2.0)\n",
      "Requirement already satisfied: six in /home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.15.0)\n",
      "Building wheels for collected packages: pycocoevalcap, pycocotools\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312215 sha256=1d7914621181b950036c9496e47d66a66a01a9cdddb07c161864c07483ebc1f0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-417vz9_y/wheels/6f/c9/51/e266f0496048c16686e133d8e33644d692931a356bfb372aae\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=369350 sha256=bbcd28ab29252b45359ba81dab44987cac06529708d2111c65aed82ab467283b\n",
      "  Stored in directory: /home/kelwa/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "Successfully built pycocoevalcap pycocotools\n",
      "Installing collected packages: cython, pycocotools, pycocoevalcap\n",
      "Successfully installed cython-0.29.23 pycocoevalcap-1.2 pycocotools-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.spice.spice import Spice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 10424, 'reflen': 10402, 'guess': [10424, 9424, 8424, 7424], 'correct': [6806, 3053, 1323, 549]}\n",
      "ratio: 1.0021149778887712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6529163468917255,\n",
       " 0.45991178094284996,\n",
       " 0.32146240162848005,\n",
       " 0.22262874291055326]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Bleu(4).compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meteor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18458572048818053"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Meteor().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44392078717000505"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Rouge().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42666098298752114"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Cider().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmp2zxdf5ks', '-cache', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/cache', '-out', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpf4c0j_qd', '-subset', '-silent']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-3e6afe3660b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/spice.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     74\u001b[0m         ]\n\u001b[1;32m     75\u001b[0m         subprocess.check_call(spice_cmd, \n\u001b[0;32m---> 76\u001b[0;31m             cwd=os.path.dirname(os.path.abspath(__file__)))\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Read and process results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmp2zxdf5ks', '-cache', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/cache', '-out', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpf4c0j_qd', '-subset', '-silent']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# score, scores = Spice().compute_score(refs, hypo)\n",
    "# score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gY_eV0-W629y"
   ],
   "name": "Evaluate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
