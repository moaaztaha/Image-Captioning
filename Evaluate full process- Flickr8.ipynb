{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v18vuSVO97ze",
    "outputId": "d01118d1-479a-44e8-9a96-bc345dd1f056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  6 23:49:43 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   86C    P2    54W /  N/A |   3658MiB /  6078MiB |     86%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2451      G   /usr/lib/Xorg                      45MiB |\r\n",
      "|    0   N/A  N/A     27635      C   ...s/kaggle_torch/bin/python     3609MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sZ4lhGja9h7N"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TSpDx8Ad9h7N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from utils import load_checkpoint\n",
    "from dataset import build_vocab, get_loaders, get_10k_vocab, top10k_vocab\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from utils import print_scores\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gPxE2X69h7O"
   },
   "outputs": [],
   "source": [
    "# DATA_NAME = 'flickr8k_ar'\n",
    "\n",
    "# local\n",
    "DATA_JSON_PATH = 'data.json'\n",
    "IMGS_PATH = 'flickr/Images/'\n",
    "# kaggle paths\n",
    "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
    "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
    "#colab\n",
    "# DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
    "# IMGS_PATH = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M5g3rBl39h7O"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oVqmuxp8_Ghy"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = 'models/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop_finetune.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riR8ECa99h7P",
    "outputId": "3aa0b9ff-1237-4778-dcc9-ce1907901f53",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint!!\n",
      "Last Epoch: 12\n",
      "Best Bleu-4: 0.1655661704030113\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wQ54asZj9h7Q"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btBk9a7v9h7Q",
    "outputId": "6d2385eb-559f-467c-a6e1-9b4f93ef5e51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 385585.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5089"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(DATA_JSON_PATH)\n",
    "# top10k_words = get_10k_vocab(\"10k_words.txt\")\n",
    "# vocab = top10k_vocab(top10k_words)\n",
    "vocab_len = len(vocab)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAq8Tncv9h7e",
    "outputId": "d8adf96f-e5e4-4133-b197-21ddddc1367f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [03:00<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 61.43412605118004\n",
      "2: 43.73592487063295\n",
      "3: 30.494683313046384\n",
      "4: 21.115915659719683\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [04:02<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 64.62017684887459\n",
      "2: 47.13660963689657\n",
      "3: 33.759143347900405\n",
      "4: 23.833249590210322\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EVALUATING AT BEAM SIZE 5:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5: 100%|██████████| 5000/5000 [04:56<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 64.13697522927086\n",
      "2: 46.566142126174284\n",
      "3: 33.21296701917001\n",
      "4: 23.38092328232358\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6, 2):\n",
    "    test_score(i, encoder, decoder, IMGS_PATH, DATA_JSON_PATH, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "import torchvision.transforms as transfroms\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Caption Dataset Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgs_dir, captions_file, vocab, transforms=None, split='train'):\n",
    "        \"\"\"\n",
    "        :param imgs_dir: folder where images are stored\n",
    "        :param captions_file: the df file with all caption information\n",
    "        :param vocab: vocabuary object\n",
    "        :param transforms: image transforms pipeline\n",
    "        :param split: data split\n",
    "        \"\"\"\n",
    "\n",
    "        # split has to be one of {'train', 'val', 'test'}\n",
    "        assert split in {'train', 'val', 'test'}\n",
    "\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.df = pd.read_json(captions_file)\n",
    "        self.df = self.df[self.df['split'] == split]\n",
    "        self.vocab = vocab\n",
    "        self.transforms = transforms\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset_size = self.df.shape[0]\n",
    "        # printing some info\n",
    "        print(f\"Dataset split: {split}\")\n",
    "        print(f\"Unique images: {self.df.file_name.nunique()}\")\n",
    "        print(f\"Total size: {self.dataset_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # loading the image\n",
    "        img_id = self.df['file_name'].values[index]\n",
    "        img = Image.open(self.imgs_dir+img_id).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = transfroms.ToTensor()(img)\n",
    "\n",
    "        # loading current caption\n",
    "        cap_len = self.df['tok_len'].values[index] + 2 # <sos> and <eos>\n",
    "        tokens = self.df['tokens'].values[index]\n",
    "        caption = torch.LongTensor(self.vocab.numericalize(tokens, cap_len))\n",
    "\n",
    "        if self.split is 'train':\n",
    "            return img, caption, cap_len\n",
    "        else:\n",
    "            # for val and test return all captions for calculate the bleu scores\n",
    "            captions_tokens = self.df[self.df['file_name'] == img_id].tokens.values\n",
    "            captions_lens = self.df[self.df['file_name'] == img_id].tok_len.values\n",
    "            all_tokens = []\n",
    "            for token, cap_len in zip(captions_tokens, captions_lens):\n",
    "                all_tokens.append(self.vocab.numericalize(token, cap_len)[1:]) # remove <sos>\n",
    "\n",
    "            return img, caption, cap_len, torch.tensor(all_tokens), img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: test\n",
      "Unique images: 1000\n",
      "Total size: 5000\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "\n",
    "loader = DataLoader(\n",
    "            dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
    "                                    transforms=transform, vocab=vocab, split='test'),\n",
    "            batch_size=bs,\n",
    "            num_workers=7,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(beam_size):\n",
    "\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "    img_ids = list()\n",
    "    \n",
    "    # For each image\n",
    "    for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n",
    "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
    "        \n",
    "        k = beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        image = image.to(device)  # (1, 3, 256, 256)\n",
    "\n",
    "        # Encode\n",
    "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "        enc_image_size = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(3)\n",
    "\n",
    "        # Flatten encoding\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
    "        \n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "            awe = gate * awe\n",
    "\n",
    "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "            scores = decoder.fc(h)  # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "            # Add\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
    "          \n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "            \n",
    "#             print(top_k_scores, top_k_words)\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != vocab.stoi['<eos>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "        \n",
    "        if len(complete_seqs_scores) == 0:\n",
    "            continue\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
    "        \n",
    "        img_ids.append(img_id[0])\n",
    "        assert len(references) == len(hypotheses) == len(img_ids)\n",
    "    # Calculate BLEU-4 scores\n",
    "#     bleu4 = corpus_bleu(references, hypotheses)\n",
    "    return references, hypotheses, img_ids\n",
    "    print_scores(references, hypotheses, nltk=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 5: 100%|██████████| 5000/5000 [05:46<00:00, 14.42it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab_len\n",
    "references, hypotheses, img_ids = evaluate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\"file_name\":img_ids, \"references\":references, \"hypothesis\": hypotheses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>references</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2073964624_52da3a0fc4.jpg</td>\n",
       "      <td>[[4, 84, 11, 4, 26, 123, 22, 4, 947, 49, 197, ...</td>\n",
       "      <td>[4, 84, 11, 4, 52, 78, 22, 4, 44, 42, 4, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3234115903_f4dfc8fc75.jpg</td>\n",
       "      <td>[[4, 706, 565, 158, 11, 49, 17, 2963, 8, 158, ...</td>\n",
       "      <td>[9, 565, 564, 11, 49, 22, 36, 166]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>533483374_86c5d4c13e.jpg</td>\n",
       "      <td>[[4, 25, 22, 36, 5, 17, 59, 34, 4, 32, 21, 4, ...</td>\n",
       "      <td>[4, 5, 65, 34, 8, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224369028_b1ac40d1fa.jpg</td>\n",
       "      <td>[[4, 14, 422, 41, 17, 59, 11, 57, 19, 4, 33, 9...</td>\n",
       "      <td>[4, 51, 11, 4, 36, 47, 17, 59, 11, 4, 912]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1082379191_ec1e53f996.jpg</td>\n",
       "      <td>[[4, 97, 22, 4, 51, 21, 1117, 47, 207, 34, 4, ...</td>\n",
       "      <td>[9, 64, 128, 207, 34, 4, 518, 264]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file_name  \\\n",
       "0  2073964624_52da3a0fc4.jpg   \n",
       "1  3234115903_f4dfc8fc75.jpg   \n",
       "2   533483374_86c5d4c13e.jpg   \n",
       "3   224369028_b1ac40d1fa.jpg   \n",
       "4  1082379191_ec1e53f996.jpg   \n",
       "\n",
       "                                          references  \\\n",
       "0  [[4, 84, 11, 4, 26, 123, 22, 4, 947, 49, 197, ...   \n",
       "1  [[4, 706, 565, 158, 11, 49, 17, 2963, 8, 158, ...   \n",
       "2  [[4, 25, 22, 36, 5, 17, 59, 34, 4, 32, 21, 4, ...   \n",
       "3  [[4, 14, 422, 41, 17, 59, 11, 57, 19, 4, 33, 9...   \n",
       "4  [[4, 97, 22, 4, 51, 21, 1117, 47, 207, 34, 4, ...   \n",
       "\n",
       "                                     hypothesis  \n",
       "0  [4, 84, 11, 4, 52, 78, 22, 4, 44, 42, 4, 45]  \n",
       "1            [9, 565, 564, 11, 49, 22, 36, 166]  \n",
       "2                         [4, 5, 65, 34, 8, 32]  \n",
       "3    [4, 51, 11, 4, 36, 47, 17, 59, 11, 4, 912]  \n",
       "4            [9, 64, 128, 207, 34, 4, 518, 264]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"predicted_captions_8.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 621.86it/s]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "hypothesis = []\n",
    "for fname in tqdm(df.file_name.unique()):\n",
    "  references.append(df[df.file_name==fname].references.to_list()[0])\n",
    "  hypothesis.append(df[df.file_name==fname].hypothesis.to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 271493.56it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 55123.66it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_tokens = [\" \".join([vocab.itos[i] for i in seq]) for seq in tqdm(hypothesis)]\n",
    "\n",
    "refes_tokens = []\n",
    "for ref in tqdm(references):\n",
    "  refes_tokens.append([\" \".join([vocab.itos[i] for i in seq]) for seq in ref]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = {idx: [tokens] for idx, tokens in enumerate(preds_tokens)}\n",
    "refs = {idx: tokens for idx, tokens in enumerate(refes_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refs), len(hypo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.spice.spice import Spice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 9421, 'reflen': 9549, 'guess': [9421, 8421, 7421, 6421], 'correct': [6125, 2886, 1271, 531]}\n",
      "ratio: 0.9865954550213648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6413697522925723,\n",
       " 0.4656614212616409,\n",
       " 0.3321296701916249,\n",
       " 0.23381833559452128]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Bleu(4).compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meteor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20481473884620033"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Meteor().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47537734113205354"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Rouge().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5821376081925969"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, scores = Cider().compute_score(refs, hypo)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpnpo59mr7', '-cache', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/cache', '-out', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpzk3a355_', '-subset', '-silent']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3e6afe3660b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/spice.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     74\u001b[0m         ]\n\u001b[1;32m     75\u001b[0m         subprocess.check_call(spice_cmd, \n\u001b[0;32m---> 76\u001b[0;31m             cwd=os.path.dirname(os.path.abspath(__file__)))\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Read and process results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_torch/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpnpo59mr7', '-cache', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/cache', '-out', '/home/kelwa/anaconda3/envs/kaggle_torch/lib/python3.7/site-packages/pycocoevalcap/spice/tmp/tmpzk3a355_', '-subset', '-silent']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "score, scores = Spice().compute_score(refs, hypo)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gY_eV0-W629y"
   ],
   "name": "Evaluate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
