{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gY_eV0-W629y"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v18vuSVO97ze",
        "outputId": "d01118d1-479a-44e8-9a96-bc345dd1f056"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  3 19:48:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKxrApT2-E71",
        "outputId": "1b8cf5d6-6617-4c97-da51-0fbd503a1d3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV32uxub-ewx"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqMvvx6P-jd7",
        "outputId": "090f85eb-ff94-4023-d086-fe46e229488f"
      },
      "source": [
        "!pip install kaggle -q\n",
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions\n",
        "!unzip -q flickr8kimagescaptions.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            " 99% 1.03G/1.04G [00:04<00:00, 235MB/s]\n",
            "100% 1.04G/1.04G [00:04<00:00, 241MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvFgu78d-lC9",
        "outputId": "1c950fa3-7c69-469a-b42c-cf552e6b4f2a"
      },
      "source": [
        "# get the code form github\n",
        "!git clone https://github.com/moaaztaha/Image-Captioning\n",
        "py_files_path = 'Image-Captioning/'\n",
        "import sys\n",
        "sys.path.append(py_files_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning'...\n",
            "remote: Enumerating objects: 630, done.\u001b[K\n",
            "remote: Counting objects: 100% (630/630), done.\u001b[K\n",
            "remote: Compressing objects: 100% (308/308), done.\u001b[K\n",
            "remote: Total 630 (delta 380), reused 561 (delta 311), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (630/630), 39.18 MiB | 21.14 MiB/s, done.\n",
            "Resolving deltas: 100% (380/380), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ4lhGja9h7N"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpDx8Ad9h7N"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from utils import load_checkpoint\n",
        "from dataset import build_vocab, get_loaders\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from utils import print_scores\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gPxE2X69h7O"
      },
      "source": [
        "# DATA_NAME = 'flickr8k_ar'\n",
        "\n",
        "# local\n",
        "# DATA_JSON_PATH = 'ar_data.json'\n",
        "# IMGS_PATH = 'flickr/Images/'\n",
        "# kaggle paths\n",
        "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
        "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
        "#colab\n",
        "DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
        "IMGS_PATH = 'flickr8k/images/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5g3rBl39h7O"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVqmuxp8_Ghy"
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/MyDrive/ImageCaptioning/flickr8/BEST_checkpoint_flickr8k_finetune.pth.tar'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "riR8ECa99h7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa0b9ff-1237-4778-dcc9-ce1907901f53"
      },
      "source": [
        "# Load model\n",
        "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval();"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 12\n",
            "Best Bleu-4: 15.97917426288958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ54asZj9h7Q"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btBk9a7v9h7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2385eb-559f-467c-a6e1-9b4f93ef5e51"
      },
      "source": [
        "vocab = build_vocab(DATA_JSON_PATH)\n",
        "len(vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 310651.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er36e9Xd9h7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad745b6-cb3a-441d-cedd-5e590ac101c6"
      },
      "source": [
        "bs = 1\n",
        "beam_size=3\n",
        "loader = get_loaders(bs, IMGS_PATH, DATA_JSON_PATH, transform, vocab, test=True, n_workers=8)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDwVRMkS9h7R"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "# import skimage.transform\n",
        "# import argparse\n",
        "from PIL import Image\n",
        "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "\n",
        "    # Read image and process\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    image = transform(img).to(device)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[word_map.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "        \n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "#         print(seqs[prev_word_inds], prev_word_inds)\n",
        "#         if step == 5:\n",
        "#             return seqs\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != word_map.stoi['<eos>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    return seq, alphas, complete_seqs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kUoFkU9h7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6275f4d0-9cd8-4f01-c914-7c98cb4989e6"
      },
      "source": [
        "seq, _, comp_seqs = caption_image_beam_search(encoder, decoder, 'flickr/Images/3514019869_7de4ece2a5.jpg', vocab, beam_size=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-0567a4699776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption_image_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flickr/Images/3514019869_7de4ece2a5.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-580de353acc1>\u001b[0m in \u001b[0;36mcaption_image_beam_search\u001b[0;34m(encoder, decoder, image_path, word_map, beam_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Read image and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     transform = transforms.Compose([\n\u001b[1;32m     19\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flickr/Images/3514019869_7de4ece2a5.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorBFmY-9h7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "99759469-4047-4372-dd1b-b204ce859c8d"
      },
      "source": [
        "[sent for sent in comp_seqs][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-a1455f45f60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'comp_seqs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N7pGppNz9h7Z",
        "outputId": "0f521bf3-d13b-4d8e-b782-a1292b734995"
      },
      "source": [
        "[\" \".join([vocab.itos[i] for i in sent]) for sent in comp_seqs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> a dog runs through an obstacle <eos>',\n",
              " '<sos> a dog jumps over a hurdle <eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScYKDOWa9h7Z",
        "outputId": "c95b8012-12df-45f7-b165-376afb09f247"
      },
      "source": [
        "[vocab.itos[i] for i in seq]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>', 'a', 'dog', 'runs', 'through', 'an', 'obstacle', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LdecEyO9h7a"
      },
      "source": [
        "def evaluate(beam_size):\n",
        "\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "        tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
        "        \n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "        \n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
        "          \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "            \n",
        "#             print(top_k_scores, top_k_words)\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab.stoi['<eos>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        if len(complete_seqs_scores) == 0:\n",
        "            continue\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "#     bleu4 = corpus_bleu(references, hypotheses)\n",
        "    return references, hypotheses\n",
        "    print_scores(references, hypotheses, nltk=True)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BACVNjSR9h7c"
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjTiKGgE9h7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24119a7d-55c2-4e80-c0ad-df637d287638"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAq8Tncv9h7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8adf96f-e5e4-4133-b197-21ddddc1367f"
      },
      "source": [
        "references, hypotheses = evaluate(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:15<00:00, 25.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mPHr3WRcR3g",
        "outputId": "7a157053-d6b9-40fc-ec56-2f66dcadda1a"
      },
      "source": [
        "len(references), len(hypotheses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4995, 4995)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GD5Hrki9h7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af13d2b-11e6-4990-aac1-72c8ce0cae7f"
      },
      "source": [
        "print_scores(references, hypotheses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.08114558472555\n",
            "2: 46.50725094600124\n",
            "3: 32.63582641164054\n",
            "4: 22.48417748427286\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64.08114558472555, 46.50725094600124, 32.63582641164054, 22.48417748427286)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IQtARUd9h7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bee6352b-2f8c-44fc-99ad-cbc01bb1732b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict({\"references\": references, \"hypotheses\": hypotheses})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>references</th>\n",
              "      <th>hypotheses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[4, 63, 11, 2453, 841, 17, 90, 106, 8, 491, 1...</td>\n",
              "      <td>[4, 64, 62, 11, 4, 49, 47, 17, 90, 34, 4, 373]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[4, 51, 11, 93, 79, 21, 130, 529], [4, 51, 11...</td>\n",
              "      <td>[4, 51, 11, 4, 93, 47, 17, 206, 11, 4, 76]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[4, 51, 11, 4, 1971, 83, 930, 2791, 1595], [4...</td>\n",
              "      <td>[4, 51, 11, 4, 83, 78, 17, 59, 34, 4, 373]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[4, 6, 22, 36, 5, 17, 206, 11, 8, 7, 71, 4, 3...</td>\n",
              "      <td>[4, 6, 22, 36, 5, 17, 12, 13, 8, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[4, 51, 398, 4, 826, 859, 259, 4, 201], [8, 5...</td>\n",
              "      <td>[4, 75, 314, 50, 4, 194, 255]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          references                                      hypotheses\n",
              "0  [[4, 63, 11, 2453, 841, 17, 90, 106, 8, 491, 1...  [4, 64, 62, 11, 4, 49, 47, 17, 90, 34, 4, 373]\n",
              "1  [[4, 51, 11, 93, 79, 21, 130, 529], [4, 51, 11...      [4, 51, 11, 4, 93, 47, 17, 206, 11, 4, 76]\n",
              "2  [[4, 51, 11, 4, 1971, 83, 930, 2791, 1595], [4...      [4, 51, 11, 4, 83, 78, 17, 59, 34, 4, 373]\n",
              "3  [[4, 6, 22, 36, 5, 17, 206, 11, 8, 7, 71, 4, 3...             [4, 6, 22, 36, 5, 17, 12, 13, 8, 7]\n",
              "4  [[4, 51, 398, 4, 826, 859, 259, 4, 201], [8, 5...                   [4, 75, 314, 50, 4, 194, 255]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXQaU1Ks-JVV"
      },
      "source": [
        "df.to_json(\"evaluation_df.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qsp0_Hbw-OAz",
        "outputId": "624cef27-b4af-4e24-d2fb-f4bcecfa0f43"
      },
      "source": [
        "df = pd.read_json(\"evaluation_df.json\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>references</th>\n",
              "      <th>hypotheses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[4, 63, 11, 2453, 841, 17, 90, 106, 8, 491, 1...</td>\n",
              "      <td>[4, 64, 62, 11, 4, 49, 47, 17, 90, 34, 4, 373]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[4, 51, 11, 93, 79, 21, 130, 529], [4, 51, 11...</td>\n",
              "      <td>[4, 51, 11, 4, 93, 47, 17, 206, 11, 4, 76]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[4, 51, 11, 4, 1971, 83, 930, 2791, 1595], [4...</td>\n",
              "      <td>[4, 51, 11, 4, 83, 78, 17, 59, 34, 4, 373]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[4, 6, 22, 36, 5, 17, 206, 11, 8, 7, 71, 4, 3...</td>\n",
              "      <td>[4, 6, 22, 36, 5, 17, 12, 13, 8, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[4, 51, 398, 4, 826, 859, 259, 4, 201], [8, 5...</td>\n",
              "      <td>[4, 75, 314, 50, 4, 194, 255]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          references                                      hypotheses\n",
              "0  [[4, 63, 11, 2453, 841, 17, 90, 106, 8, 491, 1...  [4, 64, 62, 11, 4, 49, 47, 17, 90, 34, 4, 373]\n",
              "1  [[4, 51, 11, 93, 79, 21, 130, 529], [4, 51, 11...      [4, 51, 11, 4, 93, 47, 17, 206, 11, 4, 76]\n",
              "2  [[4, 51, 11, 4, 1971, 83, 930, 2791, 1595], [4...      [4, 51, 11, 4, 83, 78, 17, 59, 34, 4, 373]\n",
              "3  [[4, 6, 22, 36, 5, 17, 206, 11, 8, 7, 71, 4, 3...             [4, 6, 22, 36, 5, 17, 12, 13, 8, 7]\n",
              "4  [[4, 51, 398, 4, 826, 859, 259, 4, 201], [8, 5...                   [4, 75, 314, 50, 4, 194, 255]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsp-b6GU9h7f"
      },
      "source": [
        "hs = [\" \".join(word for word in sent) for sent in vocab.indextostring(hypotheses)]\n",
        "rs = []\n",
        "for r in references:\n",
        "    rs.append([\" \".join(word for word in sent) for sent in vocab.indextostring(r)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoYzEDD19h7f",
        "outputId": "a0e96b41-1fda-44da-b610-f04ab848844b"
      },
      "source": [
        "hs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a black and white dog is running through the grass'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-r0_OHI9h7f",
        "outputId": "fd76c552-4c65-4c58-e283-01aad1fa0889"
      },
      "source": [
        "rs[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a boy pushes a wagon full of pumpkins',\n",
              " 'a boy pushes a wagon with two pumpkins',\n",
              " 'a boy smiling leaning over a wagon filled with two large pumpkins',\n",
              " 'a child squats behind a wagon with two pumpkins in it',\n",
              " 'boy pushing wagon with two pumpkins in it']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rG48jpR9h7g",
        "outputId": "2d962224-7c38-49c2-a132-6d960e88a4a9"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "total_meteor = 0\n",
        "\n",
        "for r, h in tqdm(zip(rs, hs), total=len(rs)):\n",
        "    total_meteor += meteor_score(r, h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:10<00:00, 460.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvvK5b3p9h7g",
        "outputId": "08e77b61-6309-41ca-fb65-07feab06abab"
      },
      "source": [
        "total_meteor/len(rs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.428133409401112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ixYpe89h7g"
      },
      "source": [
        "### turn outputs into strings -> bleu_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcSyF5Tu9h7g",
        "outputId": "788f2594-434c-4d64-85c7-390e7979c625"
      },
      "source": [
        "print_scores(rs, hs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.62017684887459\n",
            "2: 47.13660963689657\n",
            "3: 33.759143347900405\n",
            "4: 23.833249590210322\n",
            "-------------------------\n",
            "----- METEOR Score -----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'itos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-f9e9a3469961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36mprint_scores\u001b[0;34m(trgs, preds, vocab)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/run/media/kelwa/DEV/GP/Image-Captioning/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----- METEOR Score -----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ids to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'itos'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk-dM2zK9h7h"
      },
      "source": [
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tmh004i9h7h",
        "outputId": "ccef767c-64cd-46c5-98dc-fad5ab045992"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/kelwa/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-TQvupc9h7h"
      },
      "source": [
        "hs = [\" \".join([vocab.itos[i] for i in sent[0]]) for sent in hypotheses]\n",
        "rs = []\n",
        "for r in references:\n",
        "    rs.append([\" \".join([vocab.itos[i] for i in sent]) for sent in r])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbI4PEOe9h7h"
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5dojgWd9h7i",
        "outputId": "dd269696-c47a-44e2-e296-67d36857956e"
      },
      "source": [
        "evaluate(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 2: 100%|██████████| 5000/5000 [03:27<00:00, 24.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 63.54625550660793\n",
            "2: 45.03356444855022\n",
            "3: 31.69343570783961\n",
            "4: 22.30082901251822\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUmIumHR9h7i",
        "outputId": "9f5826a4-b645-415b-f028-cd4b7c7ab8e9"
      },
      "source": [
        "evaluate(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:57<00:00, 21.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.31220201306728\n",
            "2: 45.5251368754951\n",
            "3: 32.38723129370634\n",
            "4: 23.07429032664538\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SLb0Ne9h7i",
        "outputId": "3a2eed83-ce17-4fb7-f05d-49a5af28c894"
      },
      "source": [
        "evaluate(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [02:59<00:00, 27.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 61.123197163806296\n",
            "2: 42.936593246185\n",
            "3: 29.775037258795304\n",
            "4: 20.646167109205283\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmgfXpL9h7j",
        "outputId": "db87f206-8700-483f-aa19-d5957a8f5ee2"
      },
      "source": [
        "for i in range(1, 6):\n",
        "    print('*'*15, f\"Beam size of {i}\", '*'*15)\n",
        "    evaluate(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*************** Beam size of 1 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [02:59<00:00, 27.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 61.123197163806296\n",
            "2: 42.936593246185\n",
            "3: 29.775037258795304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 2:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 20.646167109205283\n",
            "-------------------------\n",
            "*************** Beam size of 2 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 2: 100%|██████████| 5000/5000 [03:58<00:00, 20.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 63.54625550660793\n",
            "2: 45.03356444855022\n",
            "3: 31.69343570783961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 22.30082901251822\n",
            "-------------------------\n",
            "*************** Beam size of 3 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [04:43<00:00, 17.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.31220201306728\n",
            "2: 45.5251368754951\n",
            "3: 32.38723129370634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 4:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 23.07429032664538\n",
            "-------------------------\n",
            "*************** Beam size of 4 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 4: 100%|██████████| 5000/5000 [04:58<00:00, 16.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.40847503864691\n",
            "2: 45.8716912526639\n",
            "3: 32.61201588518963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 5:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4: 23.235058922423306\n",
            "-------------------------\n",
            "*************** Beam size of 5 ***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 5: 100%|██████████| 5000/5000 [05:38<00:00, 14.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 64.8227213662521\n",
            "2: 46.212067416932584\n",
            "3: 32.929193446645684\n",
            "4: 23.41989863202648\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_rpjq-_Zbi"
      },
      "source": [
        "### End-to-End Arabic VS Translated English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NJG4gEs9OSS"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2kYxJzYkcVD"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
        "import torchvision.transforms as transfroms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    \"\"\" \n",
        "    Caption Dataset Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, imgs_dir, captions_file, vocab, transforms=None, split='train'):\n",
        "        \"\"\"\n",
        "        :param imgs_dir: folder where images are stored\n",
        "        :param captions_file: the df file with all caption information\n",
        "        :param vocab: vocabuary object\n",
        "        :param transforms: image transforms pipeline\n",
        "        :param split: data split\n",
        "        \"\"\"\n",
        "\n",
        "        # split has to be one of {'train', 'val', 'test'}\n",
        "        assert split in {'train', 'val', 'test'}\n",
        "\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.df = pd.read_json(captions_file)\n",
        "        self.df = self.df[self.df['split'] == split]\n",
        "        self.vocab = vocab\n",
        "        self.transforms = transforms\n",
        "        self.split = split\n",
        "\n",
        "        self.dataset_size = self.df.shape[0]\n",
        "        # printing some info\n",
        "        print(f\"Dataset split: {split}\")\n",
        "        print(f\"Unique images: {self.df.file_name.nunique()}\")\n",
        "        print(f\"Total size: {self.dataset_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # loading the image\n",
        "        img_id = self.df['file_name'].values[index]\n",
        "        img = Image.open(self.imgs_dir+img_id).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = transfroms.ToTensor()(img)\n",
        "\n",
        "        # loading current caption\n",
        "        cap_len = self.df['tok_len'].values[index] + 2 # <sos> and <eos>\n",
        "        tokens = self.df['tokens'].values[index]\n",
        "        caption = torch.LongTensor(self.vocab.numericalize(tokens, cap_len))\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, cap_len\n",
        "        else:\n",
        "            # for val and test return all captions for calculate the bleu scores\n",
        "            captions_tokens = self.df[self.df['file_name'] == img_id].tokens.values\n",
        "            captions_lens = self.df[self.df['file_name'] == img_id].tok_len.values\n",
        "            all_tokens = []\n",
        "            for token, cap_len in zip(captions_tokens, captions_lens):\n",
        "                all_tokens.append(self.vocab.numericalize(token, cap_len)[1:]) # remove <sos>\n",
        "\n",
        "            return img, caption, cap_len, torch.tensor(all_tokens), img_id"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG6JrJGf9RNR"
      },
      "source": [
        "# getting the English captions \n",
        "def evaluate(beam_size):\n",
        "\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "    img_ids = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps, img_id) in enumerate(\n",
        "        tqdm(test_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size), position=0, leave=True)):\n",
        "        \n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab.stoi['<sos>']]] * k).to(device)  # (k, 1)\n",
        "        \n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0)  # (s)\n",
        "          \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "            \n",
        "#             print(top_k_scores, top_k_words)\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab.stoi['<eos>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        if len(complete_seqs_scores) == 0:\n",
        "            continue\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']}])\n",
        "\n",
        "        img_ids.append(img_id[0])\n",
        "\n",
        "        # print(img_ids)\n",
        "        # break\n",
        "        assert len(references) == len(hypotheses) == len(img_ids)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "#     bleu4 = corpus_bleu(references, hypotheses)\n",
        "    return references, hypotheses, img_ids\n",
        "    print_scores(references, hypotheses, nltk=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsqYhiPK62WJ",
        "outputId": "1ee97dc6-18b0-4068-dc70-00304fa0e11c"
      },
      "source": [
        "# getting the test data loader with shuffle=False\n",
        "bs = 1\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
        "                            transforms=transform, vocab=vocab, split='test'),\n",
        "    batch_size=bs,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF1pajmA6_pA",
        "outputId": "a09b4b09-df31-411a-b465-ed7f929089de"
      },
      "source": [
        "refs, hypos, img_ids = evaluate(3)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:12<00:00, 26.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHMVic6H7GSh",
        "outputId": "c229aa03-9f26-4a83-fd77-a9ad01499126"
      },
      "source": [
        "len(refs), len(hypos), len(img_ids)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4995, 4995, 4995)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "R-OFzdAe7zzA",
        "outputId": "3f918847-010d-4b2f-ef2c-af76cce02c38"
      },
      "source": [
        "df = pd.DataFrame.from_dict({\"file_name\": img_ids, \"references\": refs, \"hypothesis\": hypos})\n",
        "df.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>references</th>\n",
              "      <th>hypothesis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2431470169_0eeba7d602.jpg</td>\n",
              "      <td>[[4, 51, 104, 71, 99, 184, 821, 602, 1328, 468...</td>\n",
              "      <td>[4, 64, 62, 104, 34, 4, 264]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2869491449_1041485a6b.jpg</td>\n",
              "      <td>[[9, 4839, 774, 277, 20, 168, 34, 4, 384], [9,...</td>\n",
              "      <td>[9, 10, 167, 12, 13, 8, 243]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>801607443_f15956d1ce.jpg</td>\n",
              "      <td>[[129, 317, 17, 104, 34, 8, 161, 215, 8, 144, ...</td>\n",
              "      <td>[4, 51, 17, 256, 4, 161]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3716244806_97d5a1fb61.jpg</td>\n",
              "      <td>[[4, 113, 104, 34, 320, 121, 55, 8, 69], [8, 2...</td>\n",
              "      <td>[4, 164, 19, 112, 104, 34, 4, 161]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2358898017_24496b80e8.jpg</td>\n",
              "      <td>[[4, 25, 22, 36, 5, 1385, 50, 11, 4, 290, 109]...</td>\n",
              "      <td>[4, 25, 5, 17, 12, 13, 8, 243]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name  ...                          hypothesis\n",
              "0  2431470169_0eeba7d602.jpg  ...        [4, 64, 62, 104, 34, 4, 264]\n",
              "1  2869491449_1041485a6b.jpg  ...        [9, 10, 167, 12, 13, 8, 243]\n",
              "2   801607443_f15956d1ce.jpg  ...            [4, 51, 17, 256, 4, 161]\n",
              "3  3716244806_97d5a1fb61.jpg  ...  [4, 164, 19, 112, 104, 34, 4, 161]\n",
              "4  2358898017_24496b80e8.jpg  ...      [4, 25, 5, 17, 12, 13, 8, 243]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY8RqXYb79lM"
      },
      "source": [
        "df.to_json(\"evaluation_df.json\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY_eV0-W629y"
      },
      "source": [
        "### Arabic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJeuRP1N_XzU",
        "outputId": "841bd9fb-4e17-4a79-a19a-15cc6697100f"
      },
      "source": [
        "# getting the test data loader with shuffle=False\n",
        "bs = 1\n",
        "vocab_size = len(vocab)\n",
        "DATA_JSON_PATH = 'Image-Captioning/ar_data.json'\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=CaptionDataset(IMGS_PATH, DATA_JSON_PATH,\n",
        "                            transforms=transform, vocab=vocab, split='test'),\n",
        "    batch_size=bs,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset split: test\n",
            "Unique images: 1000\n",
            "Total size: 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "Cg-THzTGAQNz",
        "outputId": "c4c450aa-901e-44d6-e6d7-8a9fd5cf7000"
      },
      "source": [
        "refs, hypos, img_ids = evaluate(3)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3:   2%|▏         | 61/3000 [00:02<01:46, 27.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-1908034cad9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-376b10500fd4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(beam_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, enc_image_size, enc_image_size, encoder_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0menc_image_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mencoder_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Image-Captioning/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# [batch size, resnet hid, image_size/32, image_size/32]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# [batch size, resnet hid, encoded image size, encoded image size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PX4A4TYaRAQ",
        "outputId": "4c40a164-2252-4051-a626-24a2f7dde6d1"
      },
      "source": [
        "len(refs)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqmmMPx-aSwv",
        "outputId": "0c6bc344-3177-47f3-b641-fbc6c3f5f578"
      },
      "source": [
        "len(hypos)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ4Q6rvUXj7H"
      },
      "source": [
        "df = pd.DataFrame.from_dict({\"references\": refs, \"hypothesis\": hypos})\n",
        "df.to_json(\"evaluation_df.json\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoamyECeX3Gw",
        "outputId": "6e924aa8-c803-4f9d-d193-39e466e84ddd"
      },
      "source": [
        "refs[2]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3, 3, 3, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOlZXTkGBoS6"
      },
      "source": [
        "addit_tokens = [vocab.stoi['<sos>'], vocab.stoi['<eos>'], vocab.stoi['<pad>']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU6jEz6-Aoja"
      },
      "source": [
        "hypothesis_text = [\" \".join([vocab.itos[i] for i in sent if i not in addit_tokens]) for sent in hypos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wGPHDfI6CQ2y",
        "outputId": "4f3d4f34-a0a6-4768-b2c1-c3d70a4dc2e2"
      },
      "source": [
        "hypothesis_text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a young boy in a blue shirt is running on the street'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5nWqyPDefR"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc4qUdsKCoTq"
      },
      "source": [
        "# add hypothesis and references to a df\n",
        "cap_df = pd.DataFrame.from_dict({\"file_name\": img_ids,\"refs\": refs, \"hypos\": hypos})"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7kElLUz-Dxto",
        "outputId": "76ecff2b-bd67-40ae-fdcf-05091e40cbe2"
      },
      "source": [
        "cap_df"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>refs</th>\n",
              "      <th>hypos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "      <td>[4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "      <td>[4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "      <td>[4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[4, 64, 62, 40, 4, 49, 47, 17, 59, 34, 8, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[4, 64, 62, 40, 4, 49, 47, 17, 59, 34, 8, 32]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2992</th>\n",
              "      <td>979383193_0a542a059d.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[9, 136, 167, 104, 34, 4, 264]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2993</th>\n",
              "      <td>979383193_0a542a059d.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[9, 136, 167, 104, 34, 4, 264]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[4, 51, 17, 256, 4, 161]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[4, 51, 17, 256, 4, 161]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3...</td>\n",
              "      <td>[4, 51, 17, 256, 4, 161]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2997 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      file_name  ...                                          hypos\n",
              "0     1056338697_4f7d7ce270.jpg  ...  [4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]\n",
              "1     1056338697_4f7d7ce270.jpg  ...  [4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]\n",
              "2     1056338697_4f7d7ce270.jpg  ...  [4, 64, 62, 11, 4, 49, 47, 17, 12, 34, 8, 46]\n",
              "3      106490881_5a2dd9b7bd.jpg  ...  [4, 64, 62, 40, 4, 49, 47, 17, 59, 34, 8, 32]\n",
              "4      106490881_5a2dd9b7bd.jpg  ...  [4, 64, 62, 40, 4, 49, 47, 17, 59, 34, 8, 32]\n",
              "...                         ...  ...                                            ...\n",
              "2992   979383193_0a542a059d.jpg  ...                 [9, 136, 167, 104, 34, 4, 264]\n",
              "2993   979383193_0a542a059d.jpg  ...                 [9, 136, 167, 104, 34, 4, 264]\n",
              "2994   997722733_0cb5439472.jpg  ...                       [4, 51, 17, 256, 4, 161]\n",
              "2995   997722733_0cb5439472.jpg  ...                       [4, 51, 17, 256, 4, 161]\n",
              "2996   997722733_0cb5439472.jpg  ...                       [4, 51, 17, 256, 4, 161]\n",
              "\n",
              "[2997 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdYKCr6bYyb5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1CS80XDmzLx",
        "outputId": "14c860d2-b529-4cda-dfff-284889f87b95"
      },
      "source": [
        "cap_df.file_name.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JggkwoyQBySa"
      },
      "source": [
        "cap_df.to_csv(\"cap_eng_output.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "soCAfb7pT9PZ",
        "outputId": "3b1c2671-46c9-4dbb-c255-d9b188258810"
      },
      "source": [
        "cap_df = pd.read_csv(\"cap_eng_output.csv\")\n",
        "cap_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>En hyps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name                                            En hyps\n",
              "0  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "1  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "2  1056338697_4f7d7ce270.jpg  a young boy in a blue shirt is running on the ...\n",
              "3   106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o...\n",
              "4   106490881_5a2dd9b7bd.jpg  a young boy wearing a blue shirt is standing o..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RIWLRBxzULlc",
        "outputId": "856ecbd1-0e83-453d-e226-91c570c36198"
      },
      "source": [
        "cap_df = pd.read_excel(\"/content/cap_to_ar.xlsx\")\n",
        "cap_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>en_hyps</th>\n",
              "      <th>ar_trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>a young boy in a blue shirt is running on the ...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يعمل على الشارع</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يقف على الشاطئ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>a young boy wearing a blue shirt is standing o...</td>\n",
              "      <td>صبي صغير يرتدي قميصا أزرق يقف على الشاطئ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   file_name  ...                                   ar_trans\n",
              "0  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "1  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "2  1056338697_4f7d7ce270.jpg  ...  صبي صغير يرتدي قميصا أزرق يعمل على الشارع\n",
              "3   106490881_5a2dd9b7bd.jpg  ...   صبي صغير يرتدي قميصا أزرق يقف على الشاطئ\n",
              "4   106490881_5a2dd9b7bd.jpg  ...   صبي صغير يرتدي قميصا أزرق يقف على الشاطئ\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "9ZMcE9iAfiUz",
        "outputId": "107f3908-75a8-4ec2-eb79-00d7e424c650"
      },
      "source": [
        "df_org = pd.read_json(\"/content/Image-Captioning/ar_data.json\")\n",
        "df_test = df_org[df_org.split=='test']\n",
        "df_test.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>caption</th>\n",
              "      <th>split</th>\n",
              "      <th>tok_len</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>امرأة شقراء في قميص أزرق تنتظر رحلة</td>\n",
              "      <td>test</td>\n",
              "      <td>7</td>\n",
              "      <td>[امرأة, شقراء, في, قميص, أزرق, تنتظر, رحلة]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>امرأة شقراء في الشارع تشير الى سيارة أجرة</td>\n",
              "      <td>test</td>\n",
              "      <td>8</td>\n",
              "      <td>[امرأة, شقراء, في, الشارع, تشير, الى, سيارة, أ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>1056338697_4f7d7ce270.jpg</td>\n",
              "      <td>المرأة في الثوب الأزرق تمد بذراعها لحركة المرو...</td>\n",
              "      <td>test</td>\n",
              "      <td>9</td>\n",
              "      <td>[المرأة, في, الثوب, الأزرق, تمد, بذراعها, لحرك...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>صبي في ملابس السباحة الزرقاء على الشاطئ</td>\n",
              "      <td>test</td>\n",
              "      <td>7</td>\n",
              "      <td>[صبي, في, ملابس, السباحة, الزرقاء, على, الشاطئ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>106490881_5a2dd9b7bd.jpg</td>\n",
              "      <td>صبي يبتسم للكاميرا على الشاطئ</td>\n",
              "      <td>test</td>\n",
              "      <td>5</td>\n",
              "      <td>[صبي, يبتسم, للكاميرا, على, الشاطئ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     file_name  ...                                             tokens\n",
              "126  1056338697_4f7d7ce270.jpg  ...        [امرأة, شقراء, في, قميص, أزرق, تنتظر, رحلة]\n",
              "127  1056338697_4f7d7ce270.jpg  ...  [امرأة, شقراء, في, الشارع, تشير, الى, سيارة, أ...\n",
              "128  1056338697_4f7d7ce270.jpg  ...  [المرأة, في, الثوب, الأزرق, تمد, بذراعها, لحرك...\n",
              "144   106490881_5a2dd9b7bd.jpg  ...    [صبي, في, ملابس, السباحة, الزرقاء, على, الشاطئ]\n",
              "145   106490881_5a2dd9b7bd.jpg  ...                [صبي, يبتسم, للكاميرا, على, الشاطئ]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrzQgNpngLGz"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG8yonRphSFi"
      },
      "source": [
        "translated_caps = []\n",
        "original_caps = []\n",
        "for idx, i in enumerate(cap_df.file_name.to_list()):\n",
        "  translated_caps.append(cap_df.iloc[idx].ar_trans.split())\n",
        "  caps = []\n",
        "  for k in df_test[df_test.file_name==i].caption.to_list():\n",
        "    caps.append(k.split())\n",
        "  original_caps.append(caps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpdV_3_KiJ3N",
        "outputId": "5e273450-9596-4cd0-b56f-a1b3546063d9"
      },
      "source": [
        "len(translated_caps), len(original_caps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2997, 2997)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nBoWVpGiWA_",
        "outputId": "a1fd9d1e-f04c-4126-982b-f00a36a38799"
      },
      "source": [
        "translated_caps[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['صبي', 'صغير', 'يرتدي', 'قميصا', 'أزرق', 'يعمل', 'على', 'الشارع']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-oU0MrXLlA",
        "outputId": "6caa358f-f89d-4b00-eb38-3c885824120f"
      },
      "source": [
        "original_caps[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['امرأة', 'شقراء', 'في', 'قميص', 'أزرق', 'تنتظر', 'رحلة'],\n",
              " ['امرأة', 'شقراء', 'في', 'الشارع', 'تشير', 'الى', 'سيارة', 'أجرة'],\n",
              " ['المرأة',\n",
              "  'في',\n",
              "  'الثوب',\n",
              "  'الأزرق',\n",
              "  'تمد',\n",
              "  'بذراعها',\n",
              "  'لحركة',\n",
              "  'المرور',\n",
              "  'القادمة']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKlo8mVjR1B",
        "outputId": "ca0f1174-29ae-43b2-e62c-267562f2a9bc"
      },
      "source": [
        "translated_caps[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['مجموعة', 'من', 'الناس', 'يقفون', 'أمام', 'مبنى']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znHpc0ZTjUE1",
        "outputId": "0cfecca0-7d09-4a8e-8567-31d302e65210"
      },
      "source": [
        "original_caps[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['رجل', 'وفتاة', 'يجلسان', 'على', 'الأرض', 'ويأكلان'],\n",
              " ['رجل', 'وفتاة', 'صغيرة', 'يجلسان', 'على', 'رصيف', 'قرب', 'حقيبة', 'زرقاء'],\n",
              " ['رجل', 'وفتاة', 'يأكلان', 'وجبة', 'في', 'أحد', 'شوارع', 'المدينة']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFnNz-xNj04K",
        "outputId": "cd642bc8-f083-4a20-cf3c-5a3d85651aa7"
      },
      "source": [
        "print_scores(original_caps, translated_caps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 34.233436623874866\n",
            "2: 20.012890946720674\n",
            "3: 11.004236540712997\n",
            "4: 5.5992246799395\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34.233436623874866, 20.012890946720674, 11.004236540712997, 5.5992246799395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsJLIchlc98g",
        "outputId": "3852b9d4-8135-433b-e4ac-5c522c2fb62f"
      },
      "source": [
        "original_caps[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['امرأة شقراء في قميص أزرق تنتظر رحلة',\n",
              " 'امرأة شقراء في الشارع تشير الى سيارة أجرة',\n",
              " 'المرأة في الثوب الأزرق تمد بذراعها لحركة المرور القادمة']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrMZQ4CTc_T3",
        "outputId": "b5615aef-e86b-4258-edfe-8702f0a61186"
      },
      "source": [
        "cap_df.ar_trans.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOfoIgt7d0zv",
        "outputId": "9f6a87b3-4106-4864-a655-8ce4db669afb"
      },
      "source": [
        "!wget https://kkb-production.jupyter-proxy.kaggle.net/k/67249843/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..QxozAcCRQtqJg9saA6D1_A.Dyw9Y41owBppV8NPupo93f_ChdCabBRRSRa4u1mX2l0eqah7jd0e63V1AO2Nj0viz2ftYrW-kBJDRfKnxjg201a08nxF-P1seOVy_bJoSZm9ZKZC37-H9OU_I63F5UZx1zbvLdDcIrjF0BCOBPFAaYtF39TofvrZuYHprJbHQ4HPbEGRL73PZQhvIaveoi1Vs_Vfwmy_bcg__5jUopuUCw.azKye_nBu246g19zT-1Tsw/proxy/files/exps.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-02 09:50:28--  https://kkb-production.jupyter-proxy.kaggle.net/k/67249843/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..QxozAcCRQtqJg9saA6D1_A.Dyw9Y41owBppV8NPupo93f_ChdCabBRRSRa4u1mX2l0eqah7jd0e63V1AO2Nj0viz2ftYrW-kBJDRfKnxjg201a08nxF-P1seOVy_bJoSZm9ZKZC37-H9OU_I63F5UZx1zbvLdDcIrjF0BCOBPFAaYtF39TofvrZuYHprJbHQ4HPbEGRL73PZQhvIaveoi1Vs_Vfwmy_bcg__5jUopuUCw.azKye_nBu246g19zT-1Tsw/proxy/files/exps.zip\n",
            "Resolving kkb-production.jupyter-proxy.kaggle.net (kkb-production.jupyter-proxy.kaggle.net)... 35.244.180.134\n",
            "Connecting to kkb-production.jupyter-proxy.kaggle.net (kkb-production.jupyter-proxy.kaggle.net)|35.244.180.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107999 (105K) [application/zip]\n",
            "Saving to: ‘exps.zip’\n",
            "\n",
            "exps.zip            100%[===================>] 105.47K   258KB/s    in 0.4s    \n",
            "\n",
            "2021-07-02 09:50:28 (258 KB/s) - ‘exps.zip’ saved [107999/107999]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}