{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all rmsprop",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yF22FLldBVv",
        "outputId": "9f4158b3-8149-45ad-9584-d049277777bc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May  6 05:46:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    34W / 250W |  11311MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAVbNRecdEmC",
        "outputId": "3655e476-9db9-4273-fea2-0bdadfd6bccb"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FHpPT9-dPip",
        "outputId": "9677b94c-bd26-4c18-c428-2304cac2d83e"
      },
      "source": [
        "!pip install kaggle -q\n",
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions\n",
        "!unzip -q flickr8kimagescaptions.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            " 99% 1.02G/1.04G [00:13<00:00, 75.5MB/s]\n",
            "100% 1.04G/1.04G [00:13<00:00, 80.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhy46y0NmxV"
      },
      "source": [
        "!rm -r Image-Captioning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdb4vWE0dQS6",
        "outputId": "8e99b312-9d85-49b8-bde6-b83036fd8a65"
      },
      "source": [
        "# get the code for kaggle\n",
        "!git clone https://github.com/moaaztaha/Image-Captioning\n",
        "py_files_path = 'Image-Captioning/'\n",
        "import sys\n",
        "sys.path.append(py_files_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image-Captioning'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 354 (delta 206), reused 232 (delta 86), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 30.85 MiB | 22.96 MiB/s, done.\n",
            "Resolving deltas: 100% (206/206), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCupTxpafLCY"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAUwbIDzfLCY"
      },
      "source": [
        "import time \n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from models import Encoder, DecoderWithAttention\n",
        "from dataset import *\n",
        "from utils import *\n",
        "from train import *\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xao3iXYfLCZ"
      },
      "source": [
        "# Model parameters\n",
        "encoder_dim = 2048 # resnet101\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# training parameters\n",
        "epochs = 7  # number of epochs to train for (if early stopping is not triggered)\n",
        "batch_size = 256\n",
        "workers = 2\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "fine_tune_encoder = False  # fine-tune encoder?\n",
        "checkpoint = None  # path to checkpoint, None if none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjGhnGwvfLCZ"
      },
      "source": [
        "DATA_NAME = 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop'\n",
        "\n",
        "# local\n",
        "# DATA_JSON_PATH = 'data.json'\n",
        "# IMGS_PATH = 'flickr/Images/'\n",
        "# kaggle paths\n",
        "# DATA_JSON_PATH = '/kaggle/working/Image-Captioning/data.json'\n",
        "# IMGS_PATH = '../input/flickr8kimagescaptions/flickr8k/images/'\n",
        "#colab\n",
        "DATA_JSON_PATH = 'Image-Captioning/data.json'\n",
        "IMGS_PATH = 'flickr8k/images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohaaGjb7fLCa",
        "outputId": "27947321-ebce-42e2-a588-50ae86acb575"
      },
      "source": [
        "# load vocab\n",
        "vocab = build_vocab(DATA_JSON_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 286477.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEIMTnYEfLCa",
        "outputId": "83fef5d9-9e94-4258-d98a-a830b91fd7a6"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6quGOJjfLCb"
      },
      "source": [
        "t_params = {\n",
        "    'data_name': DATA_NAME,\n",
        "    'imgs_path': IMGS_PATH,\n",
        "    'df_path': DATA_JSON_PATH,\n",
        "    'vocab': vocab,\n",
        "    'epochs': epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'workers': workers,\n",
        "    'decoder_lr': decoder_lr,\n",
        "    'encoder_lr': encoder_lr,\n",
        "    'fine_tune_encoder': fine_tune_encoder\n",
        "}\n",
        "\n",
        "m_params = {\n",
        "    'attention_dim': attention_dim,\n",
        "    'embed_dim': emb_dim,\n",
        "    'decoder_dim': decoder_dim,\n",
        "    'encoder_dim': encoder_dim,\n",
        "    'dropout': dropout\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9hJPzJhh4v4",
        "outputId": "bf668495-24d5-4ec7-ad32-cadf57ba70ec"
      },
      "source": [
        "t_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 256,\n",
              " 'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop',\n",
              " 'decoder_lr': 0.0004,\n",
              " 'df_path': 'Image-Captioning/data.json',\n",
              " 'encoder_lr': 0.0001,\n",
              " 'epochs': 7,\n",
              " 'fine_tune_encoder': False,\n",
              " 'imgs_path': 'flickr8k/images/',\n",
              " 'vocab': <dataset.Vocabulary at 0x7fcd533f4e10>,\n",
              " 'workers': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czDUNMJBfLCc",
        "outputId": "eaf4fa06-f46b-476a-9a5d-0bc0ebbff8c9"
      },
      "source": [
        "fit(t_params=t_params, m_params=m_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data\n",
            "Dataset split: train\n",
            "Unique images: 6000\n",
            "Total size: 30000\n",
            "Dataset split: val\n",
            "Unique images: 1000\n",
            "Total size: 5000\n",
            "__________________________________________________\n",
            "-------------------- Fitting --------------------\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [0][0/118]\tBatch Time 4.755 (4.755)\tData Load Time 3.266 (3.266)\tLoss 9.4578 (9.4578)\tTop-5 Accuracy 0.195 (0.195)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuhRS-t8pA6W"
      },
      "source": [
        "!cp /content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop.pth.tar /content/drive/MyDrive/weights/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdnuK-b4pPd1",
        "outputId": "e88d39aa-2856-418c-f0c4-72ed1e40e8b2"
      },
      "source": [
        "batch_size = 64\n",
        "fine_tune_encoder = True\n",
        "checkpoint = '/content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_rmsprop.pth.tar'\n",
        "epochs = 15\n",
        "\n",
        "t_params = {\n",
        "    'data_name': DATA_NAME+'_finetune',\n",
        "    'imgs_path': IMGS_PATH,\n",
        "    'df_path': DATA_JSON_PATH,\n",
        "    'vocab': vocab,\n",
        "    'epochs': epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'workers': workers,\n",
        "    'decoder_lr': decoder_lr*.8,\n",
        "    'encoder_lr': encoder_lr,\n",
        "    'fine_tune_encoder': fine_tune_encoder\n",
        "}\n",
        "t_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune',\n",
              " 'decoder_lr': 0.00032,\n",
              " 'df_path': 'Image-Captioning/data.json',\n",
              " 'encoder_lr': 0.0001,\n",
              " 'epochs': 15,\n",
              " 'fine_tune_encoder': True,\n",
              " 'imgs_path': 'flickr8k/images/',\n",
              " 'vocab': <dataset.Vocabulary at 0x7fcd545bc0d0>,\n",
              " 'workers': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ptOGGuc8pWY1",
        "outputId": "5101f4bf-1674-4841-ecc3-a6dac069a7ba"
      },
      "source": [
        "fit(t_params, checkpoint=checkpoint, m_params=m_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Starting Epoch: 5\n",
            "Loading Data\n",
            "Dataset split: train\n",
            "Unique images: 6000\n",
            "Total size: 30000\n",
            "Dataset split: val\n",
            "Unique images: 1000\n",
            "Total size: 5000\n",
            "__________________________________________________\n",
            "-------------------- Fitting --------------------\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [5][0/469]\tBatch Time 1.736 (1.736)\tData Load Time 0.902 (0.902)\tLoss 3.8268 (3.8268)\tTop-5 Accuracy 68.293 (68.293)\n",
            "Epoch: [5][100/469]\tBatch Time 0.704 (0.720)\tData Load Time 0.000 (0.010)\tLoss 4.1850 (4.0490)\tTop-5 Accuracy 62.500 (63.931)\n",
            "Epoch: [5][200/469]\tBatch Time 0.705 (0.715)\tData Load Time 0.000 (0.006)\tLoss 4.0673 (4.0145)\tTop-5 Accuracy 64.947 (64.577)\n",
            "Epoch: [5][300/469]\tBatch Time 0.692 (0.713)\tData Load Time 0.000 (0.004)\tLoss 3.8470 (3.9893)\tTop-5 Accuracy 67.878 (64.951)\n",
            "Epoch: [5][400/469]\tBatch Time 0.715 (0.712)\tData Load Time 0.000 (0.004)\tLoss 3.8155 (3.9740)\tTop-5 Accuracy 68.421 (65.176)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/79]\tBatch Time 1.344 (1.344)\tLoss 4.6954 (4.6954)\tTop-5 Accuracy 61.452 (61.452)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 65.4424432710508\n",
            "2: 41.72578622919813\n",
            "3: 25.25512672543196\n",
            "4: 15.100470698138055\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.923, TOP-5 ACCURACY - 60.585, BLEU-4 - 0.15255731663785566\n",
            "\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [6][0/469]\tBatch Time 1.775 (1.775)\tData Load Time 0.966 (0.966)\tLoss 3.7059 (3.7059)\tTop-5 Accuracy 67.871 (67.871)\n",
            "Epoch: [6][100/469]\tBatch Time 0.717 (0.719)\tData Load Time 0.000 (0.012)\tLoss 3.7245 (3.6898)\tTop-5 Accuracy 69.309 (69.209)\n",
            "Epoch: [6][200/469]\tBatch Time 0.694 (0.714)\tData Load Time 0.000 (0.007)\tLoss 3.6809 (3.6931)\tTop-5 Accuracy 67.507 (69.097)\n",
            "Epoch: [6][300/469]\tBatch Time 0.694 (0.713)\tData Load Time 0.000 (0.005)\tLoss 3.5686 (3.6957)\tTop-5 Accuracy 71.028 (69.093)\n",
            "Epoch: [6][400/469]\tBatch Time 0.694 (0.712)\tData Load Time 0.008 (0.004)\tLoss 3.5257 (3.6871)\tTop-5 Accuracy 71.633 (69.229)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/79]\tBatch Time 1.307 (1.307)\tLoss 4.7258 (4.7258)\tTop-5 Accuracy 63.469 (63.469)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 65.8827320630505\n",
            "2: 41.761465649591344\n",
            "3: 25.140217304024944\n",
            "4: 14.775600280611487\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 4.967, TOP-5 ACCURACY - 61.200, BLEU-4 - 0.14921721553438844\n",
            "\n",
            "\n",
            "Epochs since last improvement: (1,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [7][0/469]\tBatch Time 1.775 (1.775)\tData Load Time 0.987 (0.987)\tLoss 3.6065 (3.6065)\tTop-5 Accuracy 71.542 (71.542)\n",
            "Epoch: [7][100/469]\tBatch Time 0.726 (0.720)\tData Load Time 0.007 (0.012)\tLoss 3.6363 (3.4967)\tTop-5 Accuracy 69.520 (71.702)\n",
            "Epoch: [7][200/469]\tBatch Time 0.723 (0.715)\tData Load Time 0.000 (0.007)\tLoss 3.3221 (3.5022)\tTop-5 Accuracy 76.923 (71.741)\n",
            "Epoch: [7][300/469]\tBatch Time 0.700 (0.714)\tData Load Time 0.000 (0.005)\tLoss 3.6172 (3.5069)\tTop-5 Accuracy 70.346 (71.690)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-d10976b43c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Image-Captioning/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(t_params, checkpoint, m_params)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             epoch=epoch)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# one epoch of validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Image-Captioning/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7iGsR2yDqWa"
      },
      "source": [
        "!cp /content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune.pth.tar /content/drive/MyDrive/weights/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkM-H7kopj-m",
        "outputId": "3f100b09-58a5-46b2-cf8d-704ae385abd7"
      },
      "source": [
        "checkpoint = load_checkpoint('/content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune.pth.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 12\n",
            "Best Bleu-4: 0.15974322033502095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97VrCTRMimEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ffca08-67f0-4766-f5d6-b72fe7682943"
      },
      "source": [
        "batch_size = 64\n",
        "fine_tune_encoder = True\n",
        "checkpoint = '/content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune.pth.tar'\n",
        "epochs = 15\n",
        "\n",
        "t_params = {\n",
        "    'data_name': DATA_NAME+'_finetune_new_lr',\n",
        "    'imgs_path': IMGS_PATH,\n",
        "    'df_path': DATA_JSON_PATH,\n",
        "    'vocab': vocab,\n",
        "    'epochs': epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'workers': workers,\n",
        "    'decoder_lr': decoder_lr*.8,\n",
        "    'encoder_lr': encoder_lr*.8,\n",
        "    'fine_tune_encoder': fine_tune_encoder\n",
        "}\n",
        "t_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'data_name': 'flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune_new_lr',\n",
              " 'decoder_lr': 0.00032,\n",
              " 'df_path': 'Image-Captioning/data.json',\n",
              " 'encoder_lr': 8e-05,\n",
              " 'epochs': 15,\n",
              " 'fine_tune_encoder': True,\n",
              " 'imgs_path': 'flickr8k/images/',\n",
              " 'vocab': <dataset.Vocabulary at 0x7fcd545bc0d0>,\n",
              " 'workers': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygbSFSPCD7WL",
        "outputId": "ea8c5bc1-5ede-40c0-fbea-c1c1d5e7323e"
      },
      "source": [
        "fit(t_params, checkpoint=checkpoint, m_params=m_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Starting Epoch: 13\n",
            "Loading Data\n",
            "Dataset split: train\n",
            "Unique images: 6000\n",
            "Total size: 30000\n",
            "Dataset split: val\n",
            "Unique images: 1000\n",
            "Total size: 5000\n",
            "__________________________________________________\n",
            "-------------------- Fitting --------------------\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [13][0/469]\tBatch Time 1.702 (1.702)\tData Load Time 0.922 (0.922)\tLoss 2.9290 (2.9290)\tTop-5 Accuracy 79.778 (79.778)\n",
            "Epoch: [13][100/469]\tBatch Time 0.711 (0.723)\tData Load Time 0.000 (0.011)\tLoss 2.7991 (2.7624)\tTop-5 Accuracy 81.139 (82.264)\n",
            "Epoch: [13][200/469]\tBatch Time 0.719 (0.717)\tData Load Time 0.000 (0.006)\tLoss 2.9192 (2.7909)\tTop-5 Accuracy 80.526 (81.872)\n",
            "Epoch: [13][300/469]\tBatch Time 0.743 (0.715)\tData Load Time 0.012 (0.005)\tLoss 2.8699 (2.8093)\tTop-5 Accuracy 80.472 (81.621)\n",
            "Epoch: [13][400/469]\tBatch Time 0.711 (0.713)\tData Load Time 0.000 (0.004)\tLoss 2.8068 (2.8186)\tTop-5 Accuracy 82.283 (81.484)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/79]\tBatch Time 1.320 (1.320)\tLoss 4.8484 (4.8484)\tTop-5 Accuracy 65.803 (65.803)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 65.39241916377719\n",
            "2: 42.16568133115344\n",
            "3: 25.963963089562274\n",
            "4: 15.576708356424676\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 5.114, TOP-5 ACCURACY - 62.683, BLEU-4 - 0.15726994542258108\n",
            "\n",
            "\n",
            "Epochs since last improvement: (1,)\n",
            "__________________________________________________\n",
            "-------------------- Training --------------------\n",
            "Epoch: [14][0/469]\tBatch Time 1.792 (1.792)\tData Load Time 1.062 (1.062)\tLoss 2.5844 (2.5844)\tTop-5 Accuracy 85.933 (85.933)\n",
            "Epoch: [14][100/469]\tBatch Time 0.717 (0.722)\tData Load Time 0.000 (0.012)\tLoss 2.6657 (2.6788)\tTop-5 Accuracy 85.659 (83.613)\n",
            "Epoch: [14][200/469]\tBatch Time 0.691 (0.716)\tData Load Time 0.000 (0.007)\tLoss 2.7891 (2.7015)\tTop-5 Accuracy 81.604 (83.302)\n",
            "Epoch: [14][300/469]\tBatch Time 0.714 (0.714)\tData Load Time 0.000 (0.005)\tLoss 2.6504 (2.7193)\tTop-5 Accuracy 84.797 (83.033)\n",
            "Epoch: [14][400/469]\tBatch Time 0.745 (0.712)\tData Load Time 0.014 (0.004)\tLoss 2.9042 (2.7365)\tTop-5 Accuracy 79.602 (82.762)\n",
            "-------------------- Validation --------------------\n",
            "Validation: [0/79]\tBatch Time 1.413 (1.413)\tLoss 6.0991 (6.0991)\tTop-5 Accuracy 58.359 (58.359)\t\n",
            "----- Bleu-n Scores -----\n",
            "1: 65.0476172035588\n",
            "2: 41.439224439447095\n",
            "3: 25.308004838458835\n",
            "4: 14.88381782772477\n",
            "-------------------------\n",
            "\n",
            " * LOSS - 5.220, TOP-5 ACCURACY - 62.219, BLEU-4 - 0.1503382598144659\n",
            "\n",
            "\n",
            "Epochs since last improvement: (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyKTKbP_EAq8",
        "outputId": "abcd286d-80a4-45ce-ab9f-fa8cfb6ec75a"
      },
      "source": [
        "checkpoint = load_checkpoint('/content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam_finetune.pth.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 5\n",
            "Best Bleu-4: 0.15255731663785566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOumQKMDMziE",
        "outputId": "6bab49f7-869a-4367-8729-0afeb533beca"
      },
      "source": [
        "checkpoint = load_checkpoint('/content/BEST_checkpoint_flickr8k_5_cap_per_img_2_min_word_freq_resnet101_fullvocab_fix_ds_adam.pth.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Checkpoint!!\n",
            "Last Epoch: 4\n",
            "Best Bleu-4: 0.1466164024634058\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}